<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>03_theoretical_foundations</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<h1 id="theory">Theoretical Foundations</h1>
<p><em>The mathematics of self.</em> This section reviews the formal
apparatus of the Free Energy Principle and Active Inference. We present
the core formalisms—variational free energy, Markov blankets,
hierarchical generative models, precision weighting, prediction error,
expected free energy, and multi-agent extensions—that the subsequent
synthesis will bring into structural alignment with Blake’s prophetic
phenomenology.</p>
<h2 id="fep">The Free Energy Principle</h2>
<p>Self-organizing systems persist by minimizing surprise (realism), or
at least can be viewed as if they do (instrumentalism). Friston’s Free
Energy Principle (FEP) formalizes this imperative <span class="citation"
data-cites="friston2010free friston2006free">[@friston2010free;
@friston2006free]</span>, now comprehensively synthesized in Parr,
Pezzulo, and Friston’s canonical textbook <span class="citation"
data-cites="parr2022active">[@parr2022active]</span>.</p>
<p><strong>Variational free energy</strong> provides a tractable upper
bound on surprise (negative log model evidence):</p>
<p><span class="math display">\[\begin{equation}\label{eq:free_energy}
F = \mathbb{E}_q[\ln q(\theta) - \ln p(o, \theta)]
\end{equation}\]</span></p>
<p>where <span class="math inline">\(o\)</span> denotes observations,
<span class="math inline">\(\theta\)</span> denotes hidden states
(causes), <span class="math inline">\(q(\theta)\)</span> is a
variational density encoding the agent’s beliefs, and <span
class="math inline">\(p(o, \theta)\)</span> is the generative model
specifying how hidden states produce observations.</p>
<p><strong>Decomposition</strong> reveals the relationship between free
energy, divergence, and surprise:</p>
<p><span
class="math display">\[\begin{equation}\label{eq:fe_decomposition}
F = D_{KL}[q(\theta) \| p(\theta | o)] - \ln p(o)
\end{equation}\]</span></p>
<p>Since KL-divergence is non-negative, free energy upper-bounds
surprise:</p>
<p><span
class="math display">\[\begin{equation}\label{eq:surprise_bound}
F \geq -\ln p(o)
\end{equation}\]</span></p>
<p>This bound is tight when <span class="math inline">\(q(\theta) =
p(\theta | o)\)</span>, i.e., when the agent’s beliefs equal the true
posterior. Minimizing <span class="math inline">\(F\)</span> thus serves
two functions simultaneously: it makes beliefs more accurate (reducing
the divergence term) and implicitly minimizes surprise (the model
evidence term).</p>
<h3 id="minimization-pathways">Minimization Pathways</h3>
<p>Two complementary pathways reduce free energy (Equation <span
class="math inline">\(\ref{eq:free_energy}\)</span>):</p>
<ol type="1">
<li><strong>Perceptual inference</strong> — Update beliefs <span
class="math inline">\(q(\theta)\)</span> toward the true posterior <span
class="math inline">\(p(\theta | o)\)</span>. This is changing mind to
fit world.</li>
<li><strong>Active inference</strong> — Select actions <span
class="math inline">\(a\)</span> that sample observations <span
class="math inline">\(o\)</span> consistent with predictions. This is
changing world to fit mind.</li>
</ol>
<p>Both pathways reduce the same objective. The agent that updates its
beliefs <em>and</em> acts on the world is performing complete free
energy minimization.</p>
<h3 id="expected-free-energy-and-policy-selection">Expected Free Energy
and Policy Selection</h3>
<p>Agents must also select among possible courses of action (policies
<span class="math inline">\(\pi\)</span>). The <strong>expected free
energy</strong> <span class="math inline">\(G(\pi)\)</span> evaluates
policies by their anticipated consequences:</p>
<p><span
class="math display">\[\begin{equation}\label{eq:expected_free_energy}
G(\pi) = -\mathbb{E}_{\tilde{q}}[\ln p(o_\tau | C)] +
\mathbb{E}_{\tilde{q}}[D_{KL}[q(\theta_\tau | o_\tau, \pi) \|
q(\theta_\tau | \pi)]]
\end{equation}\]</span></p>
<p>where <span class="math inline">\(C\)</span> encodes preferred
observations (prior preferences), and <span
class="math inline">\(\tilde{q}\)</span> denotes the predictive density
under the policy. The first term drives the agent toward outcomes it
prefers; the second drives it to resolve uncertainty about hidden
states. Optimal policies minimize <span
class="math inline">\(G(\pi)\)</span>, balancing exploitation (pragmatic
value) against exploration (epistemic value) <span class="citation"
data-cites="dacosta2020active parr2022active">[@dacosta2020active;
@parr2022active]</span>.</p>
<p>This decomposition is central to the synthesis that follows: it
formally separates the <em>habitual</em> from the <em>curious</em>, the
routine from the exploratory—categories that recur throughout the
humanistic tradition under different names.</p>
<h2 id="blanket">The Markov Blanket</h2>
<p>The Markov blanket defines the statistical boundary of any autonomous
system, partitioning states into internal, external, and blanket
(interface) components <span class="citation"
data-cites="friston2019markov kirchhoff2018markov">[@friston2019markov;
@kirchhoff2018markov]</span>.</p>
<p><strong>Conditional independence:</strong></p>
<p><span
class="math display">\[\begin{equation}\label{eq:conditional_independence}
p(\mu | \eta, B) = p(\mu | B)
\end{equation}\]</span></p>
<p>Internal states <span class="math inline">\(\mu\)</span> are
conditionally independent of external states <span
class="math inline">\(\eta\)</span> given blanket states <span
class="math inline">\(B\)</span>. The blanket comprises two
complementary channels:</p>
<table>
<colgroup>
<col style="width: 26%" />
<col style="width: 19%" />
<col style="width: 39%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Component</th>
<th style="text-align: left;">Symbol</th>
<th style="text-align: left;">Flow Direction</th>
<th style="text-align: left;">Role</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Sensory states</td>
<td style="text-align: left;"><span
class="math inline">\(s\)</span></td>
<td style="text-align: left;">World <span
class="math inline">\(\to\)</span> Self</td>
<td style="text-align: left;">Carry observations</td>
</tr>
<tr>
<td style="text-align: left;">Active states</td>
<td style="text-align: left;"><span
class="math inline">\(a\)</span></td>
<td style="text-align: left;">Self <span
class="math inline">\(\to\)</span> World</td>
<td style="text-align: left;">Carry interventions</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Blanket</strong></td>
<td style="text-align: left;"><span class="math inline">\(B = \{s,
a\}\)</span></td>
<td style="text-align: left;">Bidirectional</td>
<td style="text-align: left;">The statistical interface</td>
</tr>
</tbody>
</table>
<p>Every self-organizing system—from cell to organism to social
group—possesses a Markov blanket. The blanket is constitutive: without
it, there is no distinction between system and environment, hence no
inference. The topology of this partition—what is inside, what is
outside, what mediates—determines the scope and character of an agent’s
engagement with its world.</p>
<h3 id="nested-blankets-and-multi-scale-organization">Nested Blankets
and Multi-Scale Organization</h3>
<p>Markov blankets nest recursively: cells within organs, organs within
organisms, organisms within social groups. Each scale defines its own
internal/external partition and performs its own inference <span
class="citation"
data-cites="kirchhoff2018markov ramstead2018answering">[@kirchhoff2018markov;
@ramstead2018answering]</span>. This nesting is not merely a descriptive
convenience but a formal property of hierarchical self-organization.</p>
<h2 id="hierarchy">Hierarchical Generative Models</h2>
<p>Generative models are typically layered, with each level predicting
the activity of the level below <span class="citation"
data-cites="clark2016surfing hohwy2013predictive">[@clark2016surfing;
@hohwy2013predictive]</span>.</p>
<p><strong>Hierarchical factorization:</strong></p>
<p><span
class="math display">\[\begin{equation}\label{eq:hierarchical_model}
p(o, \theta) = p(o | \theta_1) \prod_{i=1}^{n-1} p(\theta_i |
\theta_{i+1}) \cdot p(\theta_n)
\end{equation}\]</span></p>
<p>At the lowest level, <span class="math inline">\(\theta_1\)</span>
generates observations through the likelihood <span
class="math inline">\(p(o | \theta_1)\)</span>. Each higher level <span
class="math inline">\(\theta_{i+1}\)</span> provides the prior context
for the level below. The deepest level <span
class="math inline">\(\theta_n\)</span> encodes the most abstract,
slowly varying regularities of the environment.</p>
<p>This architecture has several key properties:</p>
<ul>
<li><strong>Abstraction increases with depth.</strong> Low levels encode
fast sensory features; high levels encode slow contextual
structure.</li>
<li><strong>Temporal scale separation.</strong> Higher levels change
more slowly, providing a stable context for faster dynamics below <span
class="citation"
data-cites="kiebel2008hierarchy friston2017deep">[@kiebel2008hierarchy;
@friston2017deep]</span>.</li>
<li><strong>Bidirectional message passing.</strong> Top-down predictions
and bottom-up prediction errors flow through the hierarchy, settling
jointly to minimize free energy.</li>
</ul>
<p>The depth of the hierarchy determines the scope of patterns the model
can represent—from local texture to global meaning.</p>
<h3 id="model-evidence-and-complexity">Model Evidence and
Complexity</h3>
<p>The marginal likelihood (model evidence) quantifies how well a
generative model accounts for observations:</p>
<p><span
class="math display">\[\begin{equation}\label{eq:model_evidence}
\ln p(o) = \mathbb{E}_{q}[\ln p(o | \theta)] - D_{KL}[q(\theta) \|
p(\theta)]
\end{equation}\]</span></p>
<p>Good models maximize accuracy while minimizing complexity—a formal
instantiation of Occam’s razor. Overly simple models are inaccurate;
overly complex models overfit. The free energy bound (Equation <span
class="math inline">\(\ref{eq:surprise_bound}\)</span>) ensures that
minimizing <span class="math inline">\(F\)</span> implicitly maximizes
model evidence, favoring parsimonious yet accurate explanations.</p>
<p><strong>Model comparison:</strong></p>
<p><span
class="math display">\[\begin{equation}\label{eq:model_complexity}
F_{\text{simple}} \gg F_{\text{rich}}
\end{equation}\]</span></p>
<p>A model of insufficient depth incurs high free energy because it
cannot account for the hierarchical structure of observations. A richer
model, one with appropriate depth and structure, achieves lower free
energy by capturing regularities that the shallow model misses (though a
larger model may have other tradeoffs or penalization terms applied,
balancing the tendency to inflate the number of parameters).</p>
<h2 id="precision">Precision</h2>
<p>Precision is the inverse variance of a probability distribution—a
measure of confidence or reliability:</p>
<p><span class="math display">\[\begin{equation}\label{eq:precision}
\pi = \sigma^{-1}
\end{equation}\]</span></p>
<p>In hierarchical inference, precision weights determine how strongly
each level of the hierarchy influences the overall posterior. Two
sources of precision compete at every level:</p>
<ul>
<li><strong>Prior precision</strong> (<span
class="math inline">\(\pi_{\text{prior}}\)</span>): confidence in
top-down predictions</li>
<li><strong>Sensory precision</strong> (<span
class="math inline">\(\pi_{\text{sensory}}\)</span>): confidence in
bottom-up evidence</li>
</ul>
<p>Their balance determines the character of inference:</p>
<table>
<colgroup>
<col style="width: 21%" />
<col style="width: 28%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Regime</th>
<th style="text-align: left;">Condition</th>
<th style="text-align: left;">Perceptual Effect</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Prior-dominated</td>
<td style="text-align: left;"><span
class="math inline">\(\pi_{\text{prior}} \gg
\pi_{\text{sensory}}\)</span></td>
<td style="text-align: left;">Expectations override evidence;
hallucination-like states</td>
</tr>
<tr>
<td style="text-align: left;">Sensory-dominated</td>
<td style="text-align: left;"><span
class="math inline">\(\pi_{\text{sensory}} \gg
\pi_{\text{prior}}\)</span></td>
<td style="text-align: left;">Sensory flooding; loss of contextual
interpretation</td>
</tr>
<tr>
<td style="text-align: left;">Balanced</td>
<td style="text-align: left;"><span
class="math inline">\(\pi_{\text{prior}} \approx
\pi_{\text{sensory}}\)</span></td>
<td style="text-align: left;">Optimal inference; accurate and
contextually rich perception</td>
</tr>
</tbody>
</table>
<p>Attention, in this framework, is the optimization of precision—the
process by which the brain infers the reliability of its own prediction
errors and weights them accordingly <span class="citation"
data-cites="feldman2010attention parr2019attention">[@feldman2010attention;
@parr2019attention]</span>.</p>
<h3 id="precision-dynamics-and-pathology">Precision Dynamics and
Pathology</h3>
<p>When prior precision becomes extreme:</p>
<p><strong>Prior dominance:</strong></p>
<p><span
class="math display">\[\begin{equation}\label{eq:prior_dominance}
\pi_{\text{prior}} \gg \pi_{\text{sensory}}
\end{equation}\]</span></p>
<p>the agent’s beliefs become insensitive to new evidence. The
generative model ceases to update, and perception rigidifies.
Conversely, when sensory precision vastly exceeds prior precision, the
agent is overwhelmed by unstructured input, unable to extract meaning.
Pathological states—from delusions to anxiety disorders—can be
understood as failures of precision optimization <span class="citation"
data-cites="adams2013computational">[@adams2013computational]</span>.</p>
<h2 id="error">Prediction Error and Message Passing</h2>
<p>At each level of the hierarchy, the brain computes prediction
error—the discrepancy between what was expected and what was
observed:</p>
<p><span
class="math display">\[\begin{equation}\label{eq:prediction_error}
\varepsilon_i = o_i - g_i(\theta_{i+1})
\end{equation}\]</span></p>
<p>where <span class="math inline">\(g_i(\cdot)\)</span> is the
generative function mapping higher-level states to predicted
observations at level <span class="math inline">\(i\)</span>. Errors
ascend the hierarchy; predictions descend. The system settles when <span
class="math inline">\(\varepsilon \rightarrow 0\)</span> across all
levels—when predictions match observations at every scale.</p>
<p>Each error signal (Equation <span
class="math inline">\(\ref{eq:prediction_error}\)</span>) propagates
through the hierarchy defined in Equation <span
class="math inline">\(\ref{eq:hierarchical_model}\)</span>, weighted by
the precision (Equation <span
class="math inline">\(\ref{eq:precision}\)</span>) assigned to that
level. High-precision errors demand model revision; low-precision errors
are discounted. This <strong>precision-weighted prediction
error</strong> is the fundamental currency of hierarchical
inference.</p>
<p>The bidirectional cascade of predictions and errors constitutes
perception itself: a continuous, iterative process of generating
hypotheses, testing them against evidence, and revising. Action enters
when the system changes the world to reduce prediction error rather than
changing beliefs.</p>
<h2 id="temporal-depth">Temporal Depth</h2>
<p>Generative models can extend across time, encoding dependencies
between successive observations:</p>
<p><strong>Temporal hierarchy:</strong></p>
<p><span
class="math display">\[\begin{equation}\label{eq:temporal_hierarchy}
p(o_{1:T}, \theta) = \prod_{t=1}^{T} p(o_t | \theta_t) \cdot p(\theta_t
| \theta_{t-1})
\end{equation}\]</span></p>
<p>Higher levels of the hierarchy encode slower dynamics, providing a
context for the faster fluctuations below. The lowest levels track
moment-to-moment sensory input; intermediate levels integrate over
seconds to minutes; the deepest levels encode regularities persisting
across hours, years, or longer <span class="citation"
data-cites="kiebel2008hierarchy friston2017deep">[@kiebel2008hierarchy;
@friston2017deep]</span>.</p>
<p>The <strong>temporal depth</strong> of a model determines how far
into the past and future its predictions extend. A shallow model is
reactive, bound to immediate stimulus; a deep model integrates broad
temporal context into present inference. Extending temporal depth
imposes computational cost but enables the agent to detect and exploit
regularities that span long durations.</p>
<h2 id="multi-agent">Multi-Agent Inference</h2>
<p>Active Inference extends naturally to systems of coupled agents, each
bounded by its own Markov blanket but sharing statistical structure:</p>
<p><strong>Multi-agent coordination:</strong></p>
<p><span class="math display">\[\begin{equation}\label{eq:multi_agent}
p(o, \theta) = \prod_{i=1}^{N} p(o_i | \theta_i) \cdot p(\theta_i |
\theta_{\text{shared}}) \cdot p(\theta_{\text{shared}})
\end{equation}\]</span></p>
<p>Multiple agents share a common prior <span
class="math inline">\(\theta_{\text{shared}}\)</span>—the cultural,
institutional, or ecological generative model that aligns their
individual inferences. Communication between agents can be formalized as
generalized synchronization, where coupled systems entrain their
internal dynamics to infer each other’s hidden states <span
class="citation"
data-cites="friston2015duet veissiere2020thinking">[@friston2015duet;
@veissiere2020thinking]</span>.</p>
<h3 id="mean-field-factorization">Mean-Field Factorization</h3>
<p>When the joint posterior over all hidden states is intractable,
variational inference approximates it by assuming independence between
factors:</p>
<p><strong>Mean-field approximation:</strong></p>
<p><span class="math display">\[\begin{equation}\label{eq:mean_field}
q(\theta) \approx \prod_{k=1}^{K} q(\theta_k)
\end{equation}\]</span></p>
<p>This factorization makes computation tractable but introduces
coordination costs: correlations between components are lost. The
quality of inference depends on how well the factorization structure
matches the true dependencies in the generative model. Structured
variational families that preserve key correlations improve upon the
fully factorized approximation.</p>
<p>This formalized understanding of collective intelligence provides the
necessary bridge to the aesthetic domain. If culture is a shared
generative model, then art is the engineering of that model—a
“cognitive” intervention that reshapes the priors of the collective.</p>
<h2 id="cognitive-art-and-the-fourfold">Cognitive Art and the
Fourfold</h2>
<p>The integration of Active Inference with broad-scale historical and
aesthetic systems suggests a “cognitive art”—a practice of mind that is
both rigorous and generative. Friedman’s recent work on “Cognitive Art
&amp; Science” <span class="citation"
data-cites="friedman2025cognitive">[@friedman2025cognitive]</span>
proposes a fourfold schema for intelligence that maps directly onto the
Blakean/Fristonian synthesis. This framework distinguishes between the
“Low Road” (<span class="math inline">\(2 \rightarrow 3\)</span>) of
explanatory modeling—fitting data to priors—and the “High Road” (<span
class="math inline">\(4 \rightarrow 3\)</span>) of anticipatory
wisdom—shaping the niche to afford new forms of life. Blake’s rejection
of “Single Vision” (pure 2nd-ness) in favor of “Fourfold Vision”
(integrated 1st, 2nd, 3rd, and 4th-ness) prefigures the move from mere
error minimization to the active construction of a “wise” sensorimotor
niche.</p>
<h2 id="summary-of-formal-apparatus">Summary of Formal Apparatus</h2>
<p>The following table collects the core equations and their roles in
the synthesis that follows:</p>
<table>
<colgroup>
<col style="width: 17%" />
<col style="width: 20%" />
<col style="width: 61%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Equation</th>
<th style="text-align: left;">Name</th>
<th style="text-align: left;">Role in Synthesis (§4)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><span
class="math inline">\(\ref{eq:free_energy}\)</span></td>
<td style="text-align: left;">Variational Free Energy</td>
<td style="text-align: left;">Objective function for perception and
action</td>
</tr>
<tr>
<td style="text-align: left;"><span
class="math inline">\(\ref{eq:fe_decomposition}\)</span></td>
<td style="text-align: left;">FEP Decomposition</td>
<td style="text-align: left;">Relation of divergence and surprise</td>
</tr>
<tr>
<td style="text-align: left;"><span
class="math inline">\(\ref{eq:surprise_bound}\)</span></td>
<td style="text-align: left;">Surprise Bound</td>
<td style="text-align: left;">Evidence lower bound (ELBO) logic</td>
</tr>
<tr>
<td style="text-align: left;"><span
class="math inline">\(\ref{eq:expected_free_energy}\)</span></td>
<td style="text-align: left;">Expected Free Energy</td>
<td style="text-align: left;">Policy selection
(exploration/exploitation)</td>
</tr>
<tr>
<td style="text-align: left;"><span
class="math inline">\(\ref{eq:conditional_independence}\)</span></td>
<td style="text-align: left;">Conditional Independence</td>
<td style="text-align: left;">Markov blanket as statistical
boundary</td>
</tr>
<tr>
<td style="text-align: left;"><span
class="math inline">\(\ref{eq:hierarchical_model}\)</span></td>
<td style="text-align: left;">Hierarchical Factorization</td>
<td style="text-align: left;">Depth of generative model</td>
</tr>
<tr>
<td style="text-align: left;"><span
class="math inline">\(\ref{eq:model_evidence}\)</span></td>
<td style="text-align: left;">Model Evidence</td>
<td style="text-align: left;">Accuracy–Complexity trade-off</td>
</tr>
<tr>
<td style="text-align: left;"><span
class="math inline">\(\ref{eq:model_complexity}\)</span></td>
<td style="text-align: left;">Model Comparison</td>
<td style="text-align: left;">Necessity of hierarchical depth</td>
</tr>
<tr>
<td style="text-align: left;"><span
class="math inline">\(\ref{eq:precision}\)</span></td>
<td style="text-align: left;">Precision</td>
<td style="text-align: left;">Confidence weighting</td>
</tr>
<tr>
<td style="text-align: left;"><span
class="math inline">\(\ref{eq:prior_dominance}\)</span></td>
<td style="text-align: left;">Prior Dominance</td>
<td style="text-align: left;">Pathological rigidity</td>
</tr>
<tr>
<td style="text-align: left;"><span
class="math inline">\(\ref{eq:prediction_error}\)</span></td>
<td style="text-align: left;">Prediction Error</td>
<td style="text-align: left;">Bidirectional message passing</td>
</tr>
<tr>
<td style="text-align: left;"><span
class="math inline">\(\ref{eq:temporal_hierarchy}\)</span></td>
<td style="text-align: left;">Temporal Hierarchy</td>
<td style="text-align: left;">Depth of temporal prediction</td>
</tr>
<tr>
<td style="text-align: left;"><span
class="math inline">\(\ref{eq:multi_agent}\)</span></td>
<td style="text-align: left;">Multi-Agent Coordination</td>
<td style="text-align: left;">Shared priors and collective
inference</td>
</tr>
<tr>
<td style="text-align: left;"><span
class="math inline">\(\ref{eq:mean_field}\)</span></td>
<td style="text-align: left;">Mean-Field Approximation</td>
<td style="text-align: left;">Factorized variational inference</td>
</tr>
</tbody>
</table>
<p>Each of these formalisms will be brought into structural alignment
with a specific aspect of Blake’s prophetic phenomenology in the
sections that follow.</p>
</body>
</html>
