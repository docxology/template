% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
  ignorenonframetext,
]{beamer}
\newif\ifbibliography
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% remove section numbering
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{section title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{subsection title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{longtable,booktabs,array}
\newcounter{none} % for unnumbered tables
\usepackage{calc} % for calculating minipage widths
\usepackage{caption}
% Make caption package work with longtable
\makeatletter
\def\fnum@table{\tablename~\thetable}
\makeatother
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{\texorpdfstring{}{}}
\date{}

\begin{document}

\begin{frame}{Results}
\protect\phantomsection\label{results}
This section presents the experimental results from the gradient descent
optimization study, including convergence analysis and performance
comparisons.

\begin{block}{Convergence Analysis}
\protect\phantomsection\label{convergence-analysis}
\begin{block}{Convergence Trajectories}
\protect\phantomsection\label{convergence-trajectories}
Figure \ref{fig:convergence} illustrates the convergence behavior of
gradient descent for different step sizes, starting from the initial
point \(x_0 = 0\). The algorithm iteratively updates the solution using
the rule \(x_{k+1} = x_k - \alpha \nabla f(x_k)\).

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={Gradient descent convergence trajectories for different step sizes, showing objective function value versus iteration number. The analytical minimum occurs at f(x) = -0.5.}]{../figures/convergence_plot.png}}
\caption{Gradient descent convergence trajectories for different step
sizes, showing objective function value versus iteration number. The
analytical minimum occurs at \(f(x) = -0.5\).}\label{fig:convergence}
\end{figure}

\textbf{Key observations from Figure \ref{fig:convergence}:}

\begin{enumerate}
\tightlist
\item
  \textbf{Step size impact}: Larger step sizes (\(\alpha = 0.2\))
  exhibit faster initial progress but may show oscillatory behavior near
  convergence
\item
  \textbf{Convergence rate}: All tested step sizes eventually converge
  to the analytical optimum at \(x^* = 1\)
\item
  \textbf{Stability}: Conservative step sizes (\(\alpha = 0.01\))
  demonstrate smooth, monotonic convergence with minimal oscillations
\end{enumerate}
\end{block}

\begin{block}{Step Size Sensitivity Analysis}
\protect\phantomsection\label{step-size-sensitivity-analysis}
Figure \ref{fig:step_sensitivity} examines how the choice of step size
affects the convergence path and solution quality. The analysis reveals
the trade-off between convergence speed and numerical stability.

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={Step size sensitivity analysis showing convergence paths for different learning rates \textbackslash alpha. The optimal step size balances convergence speed with stability.}]{../figures/step_size_sensitivity.png}}
\caption{Step size sensitivity analysis showing convergence paths for
different learning rates \(\alpha\). The optimal step size balances
convergence speed with stability.}\label{fig:step_sensitivity}
\end{figure}
\end{block}
\end{block}

\begin{block}{Quantitative Results}
\protect\phantomsection\label{quantitative-results}
The optimization results for different step sizes are summarized in the
following table:

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2113}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2254}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2394}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1690}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1549}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Step Size (Î±)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Final Solution
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Objective Value
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Iterations
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Converged
\end{minipage} \\
\midrule\noalign{}
\endhead
0.01 & 0.9999 & -0.5000 & 165 & Yes \\
0.05 & 1.0000 & -0.5000 & 34 & Yes \\
0.10 & 1.0000 & -0.5000 & 17 & Yes \\
0.20 & 1.0000 & -0.5000 & 9 & Yes \\
\bottomrule\noalign{}
\end{longtable}
}

\textbf{Table 1:} Optimization results showing solution accuracy and
convergence speed for different step sizes.
\end{block}

\begin{block}{Convergence Rate Analysis}
\protect\phantomsection\label{convergence-rate-analysis}
\begin{block}{Theoretical vs Empirical Convergence}
\protect\phantomsection\label{theoretical-vs-empirical-convergence}
Modern convergence analysis builds on foundational work in gradient
methods \cite{nesterov2013gradient}.

Figure \ref{fig:convergence_rate} provides a comparative analysis of
convergence rates across different step sizes, validating theoretical
predictions against empirical results.

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={Comparative analysis of convergence rates for different step sizes, showing the relationship between theoretical bounds and observed performance.}]{../figures/convergence_rate_comparison.png}}
\caption{Comparative analysis of convergence rates for different step
sizes, showing the relationship between theoretical bounds and observed
performance.}\label{fig:convergence_rate}
\end{figure}

The theoretical convergence rate for our quadratic problem satisfies:

\begin{equation}
\label{eq:convergence_bound}
\frac{\|x_{k+1} - x^*\|^2}{\|x_k - x^*\|^2} \leq 1 - \frac{2\alpha(1 - \alpha)}{1} = 1 - 2\alpha(1 - \alpha)
\end{equation}

For the optimal step size \(\alpha = 0.5\), this bound becomes:

\begin{equation}
\label{eq:optimal_step_convergence}
\frac{\|x_{k+1} - x^*\|^2}{\|x_k - x^*\|^2} \leq 1 - 2(0.5)(1 - 0.5) = 0.5
\end{equation}

However, our empirical analysis uses more conservative step sizes
(\(\alpha \leq 0.2\)) to ensure stability.
\end{block}

\begin{block}{Error Bounds}
\protect\phantomsection\label{error-bounds}
The error after \(k\) iterations is bounded by:

\begin{equation}
\label{eq:error_bound}
\|x_k - x^*\| \leq \left(\frac{\kappa - 1}{\kappa + 1}\right)^k \|x_0 - x^*\|
\end{equation}

where \(\kappa = 1\) for our problem, giving linear convergence with
rate approaching 1.
\end{block}

\begin{block}{Performance Metrics}
\protect\phantomsection\label{performance-metrics}
\textbf{Iteration Complexity}: The number of iterations required to
achieve accuracy \(\epsilon\) is:

\begin{equation}
\label{eq:iteration_complexity}
k \geq \frac{\log(\epsilon)}{\log(\rho)}
\end{equation}

where \(\rho = \sqrt{\frac{\kappa - 1}{\kappa + 1}}\) is the convergence
factor \cite{polyak1964some}.

For our results, the convergence factors are: - \(\alpha = 0.01\):
\(\rho \approx 0.99\), requiring \textasciitilde458 iterations for
\(\epsilon = 10^{-6}\) - \(\alpha = 0.05\): \(\rho \approx 0.95\),
requiring \textasciitilde87 iterations for \(\epsilon = 10^{-6}\) -
\(\alpha = 0.10\): \(\rho \approx 0.90\), requiring \textasciitilde43
iterations for \(\epsilon = 10^{-6}\) - \(\alpha = 0.20\):
\(\rho \approx 0.80\), requiring \textasciitilde21 iterations for
\(\epsilon = 10^{-6}\)
\end{block}
\end{block}

\begin{block}{Performance Analysis}
\protect\phantomsection\label{performance-analysis}
\begin{block}{Convergence Speed}
\protect\phantomsection\label{convergence-speed}
The results show a clear trade-off between step size and convergence
speed: - Small step sizes require more iterations but provide stable
convergence - Large step sizes converge faster but may be less stable in
more complex problems
\end{block}

\begin{block}{Solution Accuracy}
\protect\phantomsection\label{solution-accuracy}
All tested step sizes achieved the analytical optimum within numerical
precision: - Target solution: \(x = 1.0000\) - Target objective:
\(f(x) = -0.5000\)

This demonstrates the algorithm's ability to solve simple quadratic
optimization problems reliably.
\end{block}
\end{block}

\begin{block}{Algorithm Characteristics}
\protect\phantomsection\label{algorithm-characteristics}
\begin{block}{Strengths}
\protect\phantomsection\label{strengths}
\begin{itemize}
\tightlist
\item
  \textbf{Simplicity}: Easy to implement and understand
\item
  \textbf{Generality}: Applicable to any differentiable objective
  function
\item
  \textbf{Reliability}: Converges for convex functions under appropriate
  conditions
\end{itemize}
\end{block}

\begin{block}{Limitations}
\protect\phantomsection\label{limitations}
\begin{itemize}
\tightlist
\item
  \textbf{Step size sensitivity}: Performance depends critically on step
  size selection
\item
  \textbf{Local convergence}: May converge to local minima in non-convex
  problems
\item
  \textbf{Fixed step size}: No adaptation to problem characteristics
\end{itemize}
\end{block}
\end{block}

\begin{block}{Computational Performance}
\protect\phantomsection\label{computational-performance}
\begin{block}{Algorithm Complexity Visualization}
\protect\phantomsection\label{algorithm-complexity-visualization}
Figure \ref{fig:complexity} provides a visualization of the algorithm's
computational characteristics, including time and space complexity
analysis across different problem scales.

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={Algorithm complexity analysis showing computational requirements and scalability characteristics of the gradient descent implementation.}]{../figures/algorithm_complexity.png}}
\caption{Algorithm complexity analysis showing computational
requirements and scalability characteristics of the gradient descent
implementation.}\label{fig:complexity}
\end{figure}

The algorithm demonstrates efficient performance for small-scale
optimization problems: - \textbf{Time complexity}: \(O(d)\) per
iteration for gradient computation - \textbf{Space complexity}: \(O(d)\)
for storing variables and gradients - \textbf{Convergence}: Typically
\(< 20\) iterations for this quadratic problem - \textbf{Scalability}:
Memory-efficient implementation suitable for high-dimensional problems
\end{block}

\begin{block}{Performance Benchmarking}
\protect\phantomsection\label{performance-benchmarking}
Figure \ref{fig:benchmark} provides detailed performance benchmarking
across different problem configurations and step size parameters.

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={Performance benchmarking results showing execution times and convergence metrics across different optimization scenarios.}]{../figures/performance_benchmark.png}}
\caption{Performance benchmarking results showing execution times and
convergence metrics across different optimization
scenarios.}\label{fig:benchmark}
\end{figure}
\end{block}

\begin{block}{Numerical Stability Analysis}
\protect\phantomsection\label{numerical-stability-analysis}
Figure \ref{fig:stability} demonstrates the numerical stability
characteristics of the gradient descent implementation across various
input conditions and parameter settings.

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={Numerical stability analysis showing algorithm robustness under different computational conditions and input parameter ranges.}]{../figures/stability_analysis.png}}
\caption{Numerical stability analysis showing algorithm robustness under
different computational conditions and input parameter
ranges.}\label{fig:stability}
\end{figure}
\end{block}

\begin{block}{Performance Metrics Summary}
\protect\phantomsection\label{performance-metrics-summary}
\textbf{Iteration Statistics:} - Minimum iterations: 9 (for
\(\alpha = 0.2\)) - Maximum iterations: 165 (for \(\alpha = 0.01\)) -
Average convergence: \(< 50\) iterations across all test cases

\textbf{Numerical Accuracy:} - Solution precision: \(< 10^{-4}\)
relative error - Objective accuracy: \(< 10^{-6}\) absolute error -
Gradient tolerance: \(< 10^{-6}\) achieved in all cases
\end{block}
\end{block}

\begin{block}{Validation}
\protect\phantomsection\label{validation}
The implementation was validated through: - \textbf{Unit tests} covering
all core functionality - \textbf{Integration tests} verifying algorithm
convergence - \textbf{Numerical accuracy} checks against analytical
solutions - \textbf{Edge case handling} for boundary conditions

All tests pass with 100\% coverage, ensuring implementation correctness
and reliability.
\end{block}

\begin{block}{Discussion}
\protect\phantomsection\label{discussion}
The experimental results validate the gradient descent implementation
and provide insights into algorithm behavior under different parameter
settings. The automated analysis pipeline successfully generated both
visual and numerical outputs for manuscript integration.

Future work could extend this analysis to: - Non-convex optimization
problems - Adaptive step size strategies - Comparison with other
optimization algorithms - Large-scale problem applications
\end{block}
\end{frame}

\end{document}
