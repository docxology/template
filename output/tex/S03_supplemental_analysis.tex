% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  11pt,
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{setspace}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
  \setmainfont[]{Times New Roman}
  \setmonofont[]{Courier New}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  colorlinks=true,
  linkcolor={blue},
  filecolor={blue},
  citecolor={blue},
  urlcolor={blue},
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1.5cm,top=1.5cm,bottom=1.5cm,left=1.5cm,right=1.5cm,includeheadfoot]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{3}
% Essential packages for academic documents
\usepackage{amsmath,amssymb}          % Mathematical symbols and environments
\usepackage{amsfonts}                 % Additional math fonts
\usepackage{amsthm}                   % Theorem environments
\usepackage{graphicx}                 % Include graphics
\usepackage{float}                    % Better float placement
\usepackage{booktabs}                 % Professional tables
\usepackage{longtable}                % Long tables spanning pages
\usepackage{array}                    % Advanced table formatting
\usepackage{multirow}                 % Multi-row table cells
\usepackage{caption}                  % Enhanced caption formatting
\usepackage{subcaption}               % Sub-figures and sub-tables
\usepackage{bm}                       % Bold math symbols
\usepackage{url}                      % URL formatting
\usepackage{hyperref}                 % Hyperlinks and cross-references
\usepackage{cleveref}                 % Intelligent cross-referencing
\usepackage[capitalise]{cleveref}     % Capitalize cross-reference labels
\usepackage{natbib}                   % Bibliography support
\usepackage{doi}                      % DOI links

% Configure figure numbering and captions
\renewcommand{\figurename}{Figure}
\captionsetup{
    justification=centering,
    font=small,
    labelfont=bf,
    labelsep=period
}

% Configure table numbering and captions
\renewcommand{\tablename}{Table}
\captionsetup[table]{
    justification=centering,
    font=small,
    labelfont=bf,
    labelsep=period
}

% Configure section numbering
\setcounter{secnumdepth}{3}
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thesubsection}{\arabic{section}.\arabic{subsection}}
\renewcommand{\thesubsubsection}{\arabic{section}.\arabic{subsection}.\arabic{subsubsection}}

% Configure equation numbering
\numberwithin{equation}{section}

% Configure hyperref for proper linking
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue,
    filecolor=blue,
    pdfborder={0 0 0},
    bookmarks=true,
    bookmarksnumbered=true,
    bookmarkstype=toc,
    pdftitle={Research Project Template},
    pdfauthor={Template Author},
    pdfsubject={Academic Research},
    pdfkeywords={research, template, academic, LaTeX},
    pdfcreator={render_pdf.sh},
    pdfproducer={XeLaTeX}
}

% Configure cleveref for intelligent cross-references
\crefname{section}{Section}{Sections}
\crefname{subsection}{Subsection}{Subsections}
\crefname{subsubsection}{Subsubsection}{Subsubsections}
\crefname{equation}{Equation}{Equations}
\crefname{figure}{Figure}{Figures}
\crefname{table}{Table}{Tables}
\crefname{appendix}{Appendix}{Appendices}

% Configure fonts for Unicode support with fallbacks
\usepackage{newunicodechar}
\newunicodechar{⁴}{\textsuperscript{4}}
\newunicodechar{₄}{\textsubscript{4}}
\newunicodechar{²}{\textsuperscript{2}}
\newunicodechar{₀}{\textsubscript{0}}
\newunicodechar{₁}{\textsubscript{1}}
\newunicodechar{₂}{\textsubscript{2}}
\newunicodechar{₃}{\textsubscript{3}}

% Use standard fonts for better compatibility
\usepackage{lmodern}
\usepackage[T1]{fontenc}

% Enhanced code block styling for better contrast and readability
\usepackage{fancyvrb}
\usepackage{xcolor}
\usepackage{listings}

% Define custom colors for code blocks
\definecolor{codebg}{RGB}{248, 248, 248}      % Very light gray background
\definecolor{codeborder}{RGB}{200, 200, 200}  % Medium gray border
\definecolor{codefg}{RGB}{34, 34, 34}         % Dark gray text
\definecolor{commentcolor}{RGB}{102, 102, 102} % Comment color
\definecolor{keywordcolor}{RGB}{0, 0, 0}       % Keyword color
\definecolor{stringcolor}{RGB}{0, 102, 0}      % String color

% Configure Verbatim environment for inline code
\DefineVerbatimEnvironment{Verbatim}{Verbatim}{%
    fontsize=\small,
    frame=single,
    framerule=0.5pt,
    framesep=3pt,
    rulecolor=\color{codeborder},
    bgcolor=\color{codebg},
    fgcolor=\color{codefg}
}

% Configure code block styling
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{%
    fontsize=\footnotesize,
    frame=single,
    framerule=0.5pt,
    framesep=5pt,
    rulecolor=\color{codeborder},
    bgcolor=\color{codebg},
    fgcolor=\color{codefg}
}

% Style inline code with \texttt
\renewcommand{\texttt}[1]{%
    \colorbox{codebg}{\color{codefg}\ttfamily #1}%
}

% Configure listings package for code blocks
\lstset{
    backgroundcolor=\color{codebg},
    basicstyle=\footnotesize\ttfamily\color{codefg},
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    commentstyle=\color{commentcolor},
    deletekeywords={...},
    escapeinside={\%*}{*)},
    extendedchars=true,
    frame=single,
    framerule=0.5pt,
    framesep=5pt,
    keepspaces=true,
    keywordstyle=\color{keywordcolor}\bfseries,
    language=Python,
    morekeywords={*,...},
    numbers=left,
    numbersep=5pt,
    numberstyle=\tiny\color{codefg},
    rulecolor=\color{codeborder},
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    stepnumber=1,
    stringstyle=\color{stringcolor},
    tabsize=4,
    title=\lstname
}

% Override any Pandoc default lstset configurations
\AtBeginDocument{
    \lstset{
        backgroundcolor=\color{codebg},
        basicstyle=\footnotesize\ttfamily\color{codefg},
        frame=single,
        framerule=0.5pt,
        framesep=5pt,
        rulecolor=\color{codeborder},
        numbers=left,
        numbersep=5pt,
        numberstyle=\tiny\color{codefg}
    }
}

% Configure bibliography
\bibliographystyle{unsrt}  % Unsorted bibliography style
% Bibliography is handled in 07_references.md

% Simple page break support for document structure
% Note: Page breaks are handled in the markdown generation, not here

% Ensure proper spacing and formatting
\frenchspacing  % Single space after periods
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{S03 supplemental analysis}
\author{ORCID: 0000-0000-0000-0000\\ Email: author@example.com}
\date{November 13, 2025}

\begin{document}
\maketitle

{
\hypersetup{linkcolor=black}
\setcounter{tocdepth}{3}
\tableofcontents
}
\setstretch{1.2}
\hypertarget{sec:supplemental_analysis}{%
\section{Supplemental Analysis}\label{sec:supplemental_analysis}}

This section provides detailed analytical results and theoretical
extensions that complement the main findings presented in Sections
\ref{sec:methodology} and \ref{sec:experimental_results}.

\hypertarget{s3.1-theoretical-extensions}{%
\subsection{S3.1 Theoretical
Extensions}\label{s3.1-theoretical-extensions}}

\hypertarget{s3.1.1-non-convex-optimization-extensions}{%
\subsubsection{S3.1.1 Non-Convex Optimization
Extensions}\label{s3.1.1-non-convex-optimization-extensions}}

While our main theoretical results focus on convex optimization
problems, we have extended the framework to handle certain classes of
non-convex problems. Following the approach outlined in
\cite{nesterov2018}, we consider objectives that satisfy the
Polyak-Łojasiewicz condition:

\begin{equation}\label{eq:polyak_lojasiewicz}
\|\nabla f(x)\|^2 \geq 2\mu (f(x) - f^*)
\end{equation}

where \(f^*\) is the global minimum value. Under this condition, our
algorithm achieves linear convergence even for non-convex problems, as
demonstrated in \cite{beck2009}.

\hypertarget{s3.1.2-stochastic-variants-and-convergence-guarantees}{%
\subsubsection{S3.1.2 Stochastic Variants and Convergence
Guarantees}\label{s3.1.2-stochastic-variants-and-convergence-guarantees}}

For the stochastic variant introduced in Section
\ref{sec:supplemental_methods}, we establish convergence guarantees
following the analysis framework of \cite{kingma2014}. The key result
is:

\begin{equation}\label{eq:stochastic_guarantee}
\mathbb{E}[f(x_k) - f^*] \leq \frac{C_1}{k} + \frac{C_2 \sigma^2}{\sqrt{k}}
\end{equation}

where \(C_1\) and \(C_2\) are constants depending on problem parameters,
and \(\sigma^2\) is the variance of stochastic gradient estimates. This
result improves upon standard stochastic gradient descent
\cite{ruder2016} by incorporating adaptive step sizes and momentum.

\hypertarget{s3.2-computational-complexity-analysis}{%
\subsection{S3.2 Computational Complexity
Analysis}\label{s3.2-computational-complexity-analysis}}

\hypertarget{s3.2.1-per-iteration-cost-breakdown}{%
\subsubsection{S3.2.1 Per-Iteration Cost
Breakdown}\label{s3.2.1-per-iteration-cost-breakdown}}

Detailed analysis of computational costs per iteration:

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Operation} & \textbf{Cost} & \textbf{Notes} \\
\hline
Gradient computation & $O(n)$ & Dense problems \\
Gradient computation & $O(k)$ & Sparse with $k$ non-zeros \\
Update rule & $O(n)$ & Vector operations \\
Adaptive step size & $O(1)$ & Scalar operations \\
Momentum term & $O(n)$ & Vector addition \\
\hline
\textbf{Total (dense)} & $O(n)$ & Per iteration \\
\textbf{Total (sparse)} & $O(k)$ & Per iteration \\
\hline
\end{tabular}
\caption{Detailed computational cost breakdown per iteration}
\label{tab:complexity_breakdown}
\end{table}

\hypertarget{s3.2.2-memory-complexity-analysis}{%
\subsubsection{S3.2.2 Memory Complexity
Analysis}\label{s3.2.2-memory-complexity-analysis}}

Memory requirements scale linearly with problem dimension, as
established in \cite{boyd2004}:

\begin{equation}\label{eq:memory_detailed}
M(n) = O(n) + O(\log n) \cdot K
\end{equation}

where \(K\) is the number of iterations. This compares favorably to
quasi-Newton methods \cite{schmidt2017} which require \(O(n^2)\) memory.

\hypertarget{s3.3-convergence-rate-analysis}{%
\subsection{S3.3 Convergence Rate
Analysis}\label{s3.3-convergence-rate-analysis}}

\hypertarget{s3.3.1-rate-of-convergence-for-different-problem-classes}{%
\subsubsection{S3.3.1 Rate of Convergence for Different Problem
Classes}\label{s3.3.1-rate-of-convergence-for-different-problem-classes}}

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Problem Class} & \textbf{Rate} & \textbf{Iterations} & \textbf{Reference} \\
\hline
Strongly convex & $O(\rho^k)$ & $O(\kappa \log(1/\epsilon))$ & \cite{nesterov2018} \\
Convex & $O(1/k)$ & $O(1/\epsilon)$ & \cite{beck2009} \\
Non-convex (PL) & $O(\rho^k)$ & $O(\log(1/\epsilon))$ & This work \\
Stochastic & $O(1/k)$ & $O(1/\epsilon^2)$ & \cite{kingma2014} \\
\hline
\end{tabular}
\caption{Convergence rates for different problem classes}
\label{tab:convergence_rates}
\end{table}

\hypertarget{s3.3.2-comparison-with-existing-methods}{%
\subsubsection{S3.3.2 Comparison with Existing
Methods}\label{s3.3.2-comparison-with-existing-methods}}

Our method achieves convergence rates competitive with state-of-the-art
approaches:

\begin{itemize}
\tightlist
\item
  \textbf{vs.~Gradient Descent} \cite{ruder2016}: Faster convergence
  through adaptive step sizes
\item
  \textbf{vs.~Adam} \cite{kingma2014}: Better theoretical guarantees for
  convex problems
\item
  \textbf{vs.~L-BFGS} \cite{schmidt2017}: Lower memory requirements with
  similar convergence
\item
  \textbf{vs.~Proximal Methods} \cite{beck2009}: More general
  applicability beyond sparse problems
\end{itemize}

\hypertarget{s3.4-sensitivity-and-robustness-analysis}{%
\subsection{S3.4 Sensitivity and Robustness
Analysis}\label{s3.4-sensitivity-and-robustness-analysis}}

\hypertarget{s3.4.1-hyperparameter-sensitivity}{%
\subsubsection{S3.4.1 Hyperparameter
Sensitivity}\label{s3.4.1-hyperparameter-sensitivity}}

Detailed sensitivity analysis reveals that our method is robust to
hyperparameter choices:

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Parameter} & \textbf{Baseline} & \textbf{Range Tested} & \textbf{Performance Impact} \\
\hline
$\alpha_0$ & 0.01 & [0.001, 0.1] & ±15\% \\
$\beta$ & 0.9 & [0.5, 0.99] & ±8\% \\
$\lambda$ & 0.001 & [0, 0.01] & ±3\% \\
$\gamma$ (adaptive) & 0.1 & [0.01, 1.0] & ±5\% \\
\hline
\end{tabular}
\caption{Hyperparameter sensitivity analysis}
\label{tab:hyperparameter_sensitivity_detailed}
\end{table}

The adaptive nature of our step size selection, inspired by
\cite{duchi2011}, reduces sensitivity to initial learning rate choices
compared to fixed-step methods.

\hypertarget{s3.4.2-numerical-stability-analysis}{%
\subsubsection{S3.4.2 Numerical Stability
Analysis}\label{s3.4.2-numerical-stability-analysis}}

We analyze numerical stability following the framework in
\cite{bertsekas2015}:

\begin{equation}\label{eq:numerical_stability}
\text{Condition Number} = \frac{\lambda_{\max}(\nabla^2 f)}{\lambda_{\min}(\nabla^2 f)} = \kappa
\end{equation}

Our method maintains stability for problems with condition numbers up to
\(\kappa = 10^6\), outperforming standard gradient descent which becomes
unstable for \(\kappa > 10^4\).

\hypertarget{s3.5-extended-experimental-validation}{%
\subsection{S3.5 Extended Experimental
Validation}\label{s3.5-extended-experimental-validation}}

\hypertarget{s3.5.1-additional-benchmark-problems}{%
\subsubsection{S3.5.1 Additional Benchmark
Problems}\label{s3.5.1-additional-benchmark-problems}}

We evaluated our method on 25 additional benchmark problems from the
optimization literature \cite{polak1997}:

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Problem Class} & \textbf{Count} & \textbf{Success Rate} & \textbf{Avg. Iterations} \\
\hline
Quadratic Programming & 8 & 100\% & 156 \\
Non-linear Programming & 7 & 94.3\% & 287 \\
Constrained Optimization & 6 & 91.7\% & 342 \\
Non-convex (PL) & 4 & 87.5\% & 412 \\
\hline
\textbf{Overall} & 25 & 94.0\% & 274 \\
\hline
\end{tabular}
\caption{Performance on extended benchmark suite}
\label{tab:extended_benchmarks}
\end{table}

\hypertarget{s3.5.2-statistical-significance-testing}{%
\subsubsection{S3.5.2 Statistical Significance
Testing}\label{s3.5.2-statistical-significance-testing}}

All performance improvements were validated using rigorous statistical
testing:

\begin{itemize}
\tightlist
\item
  \textbf{Paired t-tests}: \(p < 0.001\) for all comparisons
\item
  \textbf{Effect sizes}: Cohen's \(d > 0.8\) (large effect) for
  convergence speed
\item
  \textbf{Confidence intervals}: 95\% CI for improvement: {[}21.3\%,
  26.1\%{]}
\end{itemize}

\hypertarget{s3.6-implementation-optimizations}{%
\subsection{S3.6 Implementation
Optimizations}\label{s3.6-implementation-optimizations}}

\hypertarget{s3.6.1-vectorization-and-parallelization}{%
\subsubsection{S3.6.1 Vectorization and
Parallelization}\label{s3.6.1-vectorization-and-parallelization}}

Following best practices from \cite{reddi2018}, we implemented several
optimizations:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Vectorized operations}: Using NumPy for efficient
  matrix-vector operations
\item
  \textbf{Parallel gradient computation}: For separable objectives,
  gradients computed in parallel
\item
  \textbf{Memory-efficient storage}: Sparse matrix representations when
  applicable
\item
  \textbf{JIT compilation}: Using Numba for critical loops
\end{enumerate}

These optimizations provide 2-3x speedup over naive implementations.

\hypertarget{s3.6.2-code-quality-and-reproducibility}{%
\subsubsection{S3.6.2 Code Quality and
Reproducibility}\label{s3.6.2-code-quality-and-reproducibility}}

Our implementation follows scientific computing best practices
\cite{bertsekas2015}:

\begin{itemize}
\tightlist
\item
  \textbf{Deterministic seeds}: All random operations use fixed seeds
\item
  \textbf{Comprehensive logging}: All experiments log hyperparameters
  and results
\item
  \textbf{Version control}: Full git history for reproducibility
\item
  \textbf{Documentation}: Complete API documentation with examples
\end{itemize}

\hypertarget{s3.7-limitations-and-future-directions}{%
\subsection{S3.7 Limitations and Future
Directions}\label{s3.7-limitations-and-future-directions}}

\hypertarget{s3.7.1-current-limitations}{%
\subsubsection{S3.7.1 Current
Limitations}\label{s3.7.1-current-limitations}}

While our method shows strong performance, several limitations remain:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Convexity requirement}: Theoretical guarantees require
  convexity or PL condition
\item
  \textbf{Hyperparameter tuning}: Some parameters still require domain
  knowledge
\item
  \textbf{Problem structure}: Optimal performance requires certain
  problem structures
\end{enumerate}

\hypertarget{s3.7.2-future-research-directions}{%
\subsubsection{S3.7.2 Future Research
Directions}\label{s3.7.2-future-research-directions}}

Building on our results and related work \cite{nesterov2018, beck2009},
future directions include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Non-convex extensions}: Developing guarantees for broader
  non-convex classes
\item
  \textbf{Distributed optimization}: Scaling to multi-machine settings
\item
  \textbf{Online learning}: Adapting to streaming data scenarios
\item
  \textbf{Multi-objective optimization}: Handling conflicting objectives
  simultaneously
\end{enumerate}

These extensions will further broaden the applicability of our
framework.

\end{document}
