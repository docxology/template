\begin{thebibliography}{10}

\bibitem{example2023}
John Smith and Jane Johnson.
\newblock Example research paper.
\newblock {\em Journal of Example Research}, 42(3):123--145, 2023.

\bibitem{template2024}
Template Team.
\newblock {\em Research Project Template: A Comprehensive Guide}.
\newblock Academic Press, New York, NY, 2024.

\bibitem{optimization2022}
Alice Brown and Robert Wilson.
\newblock Advanced optimization techniques for machine learning.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning}, pages 456--467. ICML, 2022.

\bibitem{nesterov2018}
Yurii Nesterov.
\newblock Lectures on convex optimization.
\newblock {\em Springer Optimization and Its Applications}, 137, 2018.

\bibitem{kingma2014}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In {\em Proceedings of the 3rd International Conference on Learning
  Representations}, 2015.

\bibitem{beck2009}
Amir Beck and Marc Teboulle.
\newblock A fast iterative shrinkage-thresholding algorithm for linear inverse
  problems.
\newblock {\em SIAM Journal on Imaging Sciences}, 2(1):183--202, 2009.

\bibitem{boyd2004}
Stephen Boyd and Lieven Vandenberghe.
\newblock {\em Convex Optimization}.
\newblock Cambridge University Press, Cambridge, UK, 2004.

\bibitem{reddi2018}
Sashank~J. Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of adam and beyond.
\newblock In {\em Proceedings of the 6th International Conference on Learning
  Representations}, 2018.

\bibitem{parikh2014}
Neal Parikh and Stephen Boyd.
\newblock Proximal algorithms.
\newblock Technical Report~3, Foundations and Trends in Optimization, 2014.

\bibitem{wright2010}
Stephen~J. Wright.
\newblock {\em Optimization Algorithms for Large-Scale Machine Learning}.
\newblock Phd thesis, University of Wisconsin-Madison, 2010.

\bibitem{ruder2016}
Sebastian Ruder.
\newblock An overview of gradient descent optimization algorithms.
\newblock {\em arXiv preprint arXiv:1609.04747}, 2016.

\bibitem{duchi2011}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock In {\em Proceedings of the 24th Annual Conference on Learning
  Theory}, pages 257--269. COLT, 2011.

\bibitem{schmidt2017}
Mark Schmidt, Nicolas Le~Roux, and Francis Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock {\em Mathematical Programming}, 162(1):83--112, 2017.

\bibitem{bertsekas2015}
Dimitri~P. Bertsekas.
\newblock {\em Convex Optimization Algorithms}.
\newblock Athena Scientific, Belmont, MA, 2015.

\bibitem{pandoc2024}
{Pandoc Development Team}.
\newblock Pandoc: A universal document converter, 2024.
\newblock Accessed: 2024-10-09.

\bibitem{polak1997}
Elijah Polak.
\newblock Optimization: Algorithms and consistent approximations.
\newblock {\em Applied Mathematical Sciences}, 124, 1997.

\end{thebibliography}
