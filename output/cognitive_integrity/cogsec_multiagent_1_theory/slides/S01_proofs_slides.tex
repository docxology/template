% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
  ignorenonframetext,
]{beamer}
\newif\ifbibliography
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% remove section numbering
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{section title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{subsection title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{\texorpdfstring{}{}}
\date{}

\begin{document}

\begin{frame}
\newpage
\end{frame}

\begin{frame}{Supplementary: Mathematical Proofs}
\protect\phantomsection\label{supplementary-mathematical-proofs}
This supplementary material provides complete formal proofs for all
theorems stated in the main text, including preliminary definitions
(\cref{sec:preliminaries}), main theorem proofs
(\crefrange{sec:thm31-proof}{sec:thm66-proof}), and additional
supporting lemmas (\cref{sec:additional-lemmas}).

\begin{block}{Preliminary Definitions and Notation}
\protect\phantomsection\label{sec:preliminaries}
\begin{block}{Notation Summary}
\protect\phantomsection\label{sec:notation}
\begin{table}[htbp]
\centering
\caption{Mathematical notation used throughout proofs.}
\label{tab:notation}
\begin{tabular}{@{}ll@{}}
\toprule
Symbol & Meaning \\
\midrule
$\mathcal{A} = \{a_1, \ldots, a_n\}$ & Set of $n$ agents \\
$\mathcal{B}_i: \Phi \to [0,1]$ & Agent $i$'s belief function \\
$\mathcal{G}_i$ & Agent $i$'s goal set \\
$\mathcal{T}_{i \to j}$ & Trust from agent $i$ to agent $j$ \\
$\delta \in (0,1)$ & Trust decay factor per delegation hop \\
$\tau$ & Generic threshold parameter \\
$\phi, \psi$ & Propositions \\
$\pi(\phi)$ & Provenance chain for belief $\phi$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{definition}[Trust Path]
\label{def:trust-path}
A trust path from agent $a_0$ to agent $a_k$ is an ordered sequence $p = (a_0, a_1, \ldots, a_k)$ where each consecutive pair $(a_i, a_{i+1})$ represents a direct trust relationship with $\mathcal{T}_{a_i \to a_{i+1}} > 0$.
\end{definition}

\begin{definition}[Path Trust]
\label{def:path-trust}
The trust along path $p = (a_0, \ldots, a_k)$ is defined as:
\begin{equation}
\label{eq:path-trust}
\mathcal{T}^{path}_p = \min_{i \in [0,k-1]} \mathcal{T}_{a_i \to a_{i+1}} \cdot \delta^{k}
\end{equation}
\end{definition}

\begin{definition}[Delegation Chain]
\label{def:delegation-chain}
A delegation chain of depth $d$ is a sequence of agents $(a_0, a_1, \ldots, a_d)$ where agent $a_i$ delegates authority to $a_{i+1}$.
\end{definition}
\end{block}
\end{block}
\end{frame}

\begin{frame}
\begin{block}{Theorem 3.1: Trust Boundedness}
\protect\phantomsection\label{sec:thm31-proof}
\begin{theorem}[Trust Boundedness --- Restated]
\label{thm:trust-bound-restated}
For any delegation chain of depth $d$:
\begin{equation}
\label{eq:trust-bound-restated}
\mathcal{T}_{i \to k}^{del} \leq \delta^d
\end{equation}
\end{theorem}

\begin{lemma}[Trust Non-Amplification on Single Hop]
\label{lem:single-hop}
For any agents $a, b$ and any delegation to $c$:
\begin{equation}
\label{eq:single-hop}
\mathcal{T}_{a \to c}^{del} \leq \mathcal{T}_{a \to b}
\end{equation}
\end{lemma}

\begin{proof}[Proof of \cref{lem:single-hop}]
By the trust delegation rule (\cref{def:trust-delegation}):
\begin{equation}
\label{eq:lem-single-hop-1}
\mathcal{T}_{a \to c}^{del} = \min(\mathcal{T}_{a \to b}, \mathcal{T}_{b \to c}) \cdot \delta
\end{equation}

Since $\min(\mathcal{T}_{a \to b}, \mathcal{T}_{b \to c}) \leq \mathcal{T}_{a \to b}$ and $\delta < 1$:
\begin{equation}
\label{eq:lem-single-hop-2}
\mathcal{T}_{a \to c}^{del} = \min(\mathcal{T}_{a \to b}, \mathcal{T}_{b \to c}) \cdot \delta \leq \mathcal{T}_{a \to b} \cdot \delta < \mathcal{T}_{a \to b}
\end{equation}
\end{proof}

\begin{lemma}[Trust Decay Bound]
\label{lem:decay-bound}
For any single-hop delegation:
\begin{equation}
\label{eq:decay-bound}
\mathcal{T}^{del} \leq \delta
\end{equation}
\end{lemma}

\begin{proof}[Proof of \cref{lem:decay-bound}]
By definition, all direct trust values satisfy $\mathcal{T}_{a \to b} \leq 1$. Therefore:
\begin{equation}
\label{eq:lem-decay-1}
\mathcal{T}_{a \to c}^{del} = \min(\mathcal{T}_{a \to b}, \mathcal{T}_{b \to c}) \cdot \delta \leq 1 \cdot \delta = \delta
\end{equation}
\end{proof}

\begin{proof}[Main Proof of \cref{thm:trust-bound-restated}]
By strong induction on $d$.

\textbf{Base Case} ($d = 0$): When $d = 0$, there is no delegation (direct trust). By definition:
\begin{equation}
\label{eq:base-case}
\mathcal{T}_{i \to k}^{del} = \mathcal{T}_{i \to k} \leq 1 = \delta^0
\end{equation}
The base case holds.

\textbf{Inductive Hypothesis}: Assume for all delegation chains of depth $\leq d$:
\begin{equation}
\label{eq:ind-hyp}
\mathcal{T}^{del} \leq \delta^d
\end{equation}

\textbf{Inductive Step} (depth $d + 1$): Consider a delegation chain $(a_0, a_1, \ldots, a_{d+1})$ of depth $d + 1$.

Let $\mathcal{T}^{(d)}$ denote the delegated trust from $a_0$ to $a_d$ (depth $d$).

By the trust delegation rule:
\begin{equation}
\label{eq:ind-step-1}
\mathcal{T}_{a_0 \to a_{d+1}}^{del} = \min(\mathcal{T}^{(d)}, \mathcal{T}_{a_d \to a_{d+1}}) \cdot \delta
\end{equation}

By the inductive hypothesis: $\mathcal{T}^{(d)} \leq \delta^d$

Since $\mathcal{T}_{a_d \to a_{d+1}} \leq 1$:
\begin{equation}
\label{eq:ind-step-2}
\min(\mathcal{T}^{(d)}, \mathcal{T}_{a_d \to a_{d+1}}) \leq \mathcal{T}^{(d)} \leq \delta^d
\end{equation}

Therefore:
\begin{equation}
\label{eq:ind-step-3}
\mathcal{T}_{a_0 \to a_{d+1}}^{del} \leq \delta^d \cdot \delta = \delta^{d+1}
\end{equation}

By the principle of mathematical induction, the theorem holds for all $d \geq 0$.
\end{proof}

\begin{corollary}[Trust Vanishing]
\label{cor:trust-vanishing}
For any $\epsilon > 0$, there exists $D$ such that for all delegation chains of depth $d > D$:
\begin{equation}
\label{eq:trust-vanishing}
\mathcal{T}^{del} < \epsilon
\end{equation}
\end{corollary}

\begin{proof}
Choose $D = \lceil \log_\delta \epsilon \rceil$. Since $\delta \in (0,1)$, $\log_\delta$ is decreasing. For $d > D$: $\mathcal{T}^{del} \leq \delta^d < \delta^D \leq \epsilon$.
\end{proof}

\begin{corollary}[Practical Depth Limit]
\label{cor:practical-depth}
With $\delta = 0.8$ and minimum actionable trust $\tau_{min} = 0.1$:
\begin{equation}
\label{eq:practical-depth}
d_{max} = \lfloor \log_{0.8} 0.1 \rfloor = 10
\end{equation}
\end{corollary}
\end{block}
\end{frame}

\begin{frame}
\begin{block}{Theorem 6.1: Belief Injection Resistance}
\protect\phantomsection\label{sec:thm61-proof}
\begin{theorem}[Belief Injection Resistance --- Restated]
\label{thm:belief-injection-restated}
Under CIF with firewall detection rate $r_f$ and sandboxing verification rate $r_s$:
\begin{equation}
\label{eq:belief-injection-restated}
P(\mathcal{A}_{BI} \text{ succeeds}) \leq (1 - r_f) \cdot (1 - r_s)
\end{equation}
\end{theorem}

\begin{lemma}[Defense Independence]
\label{lem:defense-independence}
The firewall and sandbox operate on independent decision criteria:
\begin{itemize}
\item Firewall: Pattern matching and anomaly scoring on message content
\item Sandbox: Provenance verification, consistency checking, and corroboration
\end{itemize}
These mechanisms share no common features or state.
\end{lemma}

\begin{proof}[Proof of \cref{lem:defense-independence}]
By construction of the CIF architecture:
\begin{enumerate}
\item Firewall operates at input layer with feature set $F_{firewall} = \{patterns, embeddings, anomaly\_scores\}$
\item Sandbox operates at belief layer with feature set $F_{sandbox} = \{provenance, consistency, corroboration\}$
\item $F_{firewall} \cap F_{sandbox} = \emptyset$
\end{enumerate}

Therefore, $P(\text{firewall detects} | \text{sandbox outcome}) = P(\text{firewall detects})$. The mechanisms are probabilistically independent.
\end{proof}

\begin{definition}[Attack Success]
\label{def:attack-success}
A belief injection attack $\mathcal{A}_{BI}$ succeeds if and only if:
\begin{enumerate}
\item The adversarial message $m_{adv}$ is not rejected by the firewall, AND
\item The injected belief $\phi_{adv}$ is promoted from sandbox to verified beliefs
\end{enumerate}
\end{definition}

\begin{proof}[Main Proof of \cref{thm:belief-injection-restated}]
Let $E_f$ = event ``firewall accepts message'' (does not detect attack). Let $E_s$ = event ``sandbox fails to filter belief'' (does not detect attack).

For $\mathcal{A}_{BI}$ to succeed, both $E_f$ and $E_s$ must occur:
\begin{equation}
\label{eq:61-proof-1}
P(\mathcal{A}_{BI} \text{ succeeds}) = P(E_f \cap E_s)
\end{equation}

By \cref{lem:defense-independence} (independence):
\begin{equation}
\label{eq:61-proof-2}
P(E_f \cap E_s) = P(E_f) \cdot P(E_s)
\end{equation}

By definition of detection rates:
\begin{itemize}
\item $P(E_f) = 1 - r_f$ (probability firewall misses attack)
\item $P(E_s) = 1 - r_s$ (probability sandbox misses attack)
\end{itemize}

Therefore:
\begin{equation}
\label{eq:61-proof-3}
P(\mathcal{A}_{BI} \text{ succeeds}) = (1 - r_f) \cdot (1 - r_s)
\end{equation}
\end{proof}

\begin{corollary}[Numerical Bound]
\label{cor:numerical-bound}
With empirical values $r_f = 0.8$ and $r_s = 0.7$:
\begin{equation}
\label{eq:numerical-bound}
P(\mathcal{A}_{BI} \text{ succeeds}) \leq (1 - 0.8) \cdot (1 - 0.7) = 0.2 \cdot 0.3 = 0.06
\end{equation}
\end{corollary}

\begin{corollary}[Defense Stacking]
\label{cor:defense-stacking}
For $n$ independent defenses with detection rates $r_1, \ldots, r_n$:
\begin{equation}
\label{eq:defense-stacking}
P(\text{attack succeeds}) = \prod_{i=1}^{n} (1 - r_i)
\end{equation}
\end{corollary}

\begin{proof}
Direct extension of \cref{thm:belief-injection-restated} by independence.
\end{proof}
\end{block}
\end{frame}

\begin{frame}
\begin{block}{Theorem 6.2: No Trust Amplification}
\protect\phantomsection\label{sec:thm62-proof}
\begin{theorem}[No Trust Amplification --- Restated]
\label{thm:trust-amp-restated}
For any path $p = (a_0, a_1, \ldots, a_k)$ in the communication graph:
\begin{equation}
\label{eq:trust-amp-restated}
\mathcal{T}_{a_0 \to a_k}^{path} \leq \min_{i \in [0,k-1]} \mathcal{T}_{a_i \to a_{i+1}}
\end{equation}
\end{theorem}

\begin{lemma}[Minimum Preservation under Min]
\label{lem:min-preservation}
For any sequence $(x_1, \ldots, x_n)$ and additional element $x_{n+1}$:
\begin{equation}
\label{eq:min-preservation}
\min(x_1, \ldots, x_{n+1}) = \min(\min(x_1, \ldots, x_n), x_{n+1})
\end{equation}
\end{lemma}

\begin{proof}
Standard property of the minimum function.
\end{proof}

\begin{lemma}[Decay Factor Strengthens Bound]
\label{lem:decay-strengthens}
For $x \leq y$ and $\delta \in (0,1)$:
\begin{equation}
\label{eq:decay-strengthens}
x \cdot \delta \leq y
\end{equation}
\end{lemma}

\begin{proof}
Since $\delta < 1$, $x \cdot \delta < x \leq y$.
\end{proof}

\begin{proof}[Main Proof of \cref{thm:trust-amp-restated}]
By strong induction on path length $k$.

\textbf{Base Case} ($k = 1$): For path $p = (a_0, a_1)$:
\begin{equation}
\label{eq:62-base}
\mathcal{T}_{a_0 \to a_1}^{path} = \mathcal{T}_{a_0 \to a_1} = \min_{i \in [0,0]} \mathcal{T}_{a_i \to a_{i+1}}
\end{equation}
The base case holds trivially.

\textbf{Inductive Hypothesis}: Assume for all paths of length $\leq k$:
\begin{equation}
\label{eq:62-ind-hyp}
\mathcal{T}^{path} \leq \min_{i \in [0,k-1]} \mathcal{T}_{a_i \to a_{i+1}}
\end{equation}

\textbf{Inductive Step} (path length $k + 1$): Consider path $p = (a_0, a_1, \ldots, a_{k+1})$.

Let $p' = (a_0, a_1, \ldots, a_k)$ be the prefix path.

By the trust delegation rule:
\begin{equation}
\label{eq:62-step-1}
\mathcal{T}_{a_0 \to a_{k+1}}^{path} = \min(\mathcal{T}_{a_0 \to a_k}^{path'}, \mathcal{T}_{a_k \to a_{k+1}}) \cdot \delta
\end{equation}

By the inductive hypothesis:
\begin{equation}
\label{eq:62-step-2}
\mathcal{T}_{a_0 \to a_k}^{path'} \leq \min_{i \in [0,k-1]} \mathcal{T}_{a_i \to a_{i+1}}
\end{equation}

Applying the minimum:
\begin{equation}
\label{eq:62-step-3}
\min(\mathcal{T}_{a_0 \to a_k}^{path'}, \mathcal{T}_{a_k \to a_{k+1}}) \leq \min\left(\min_{i \in [0,k-1]} \mathcal{T}_{a_i \to a_{i+1}}, \mathcal{T}_{a_k \to a_{k+1}}\right)
\end{equation}

By \cref{lem:min-preservation}:
\begin{equation}
\label{eq:62-step-4}
= \min_{i \in [0,k]} \mathcal{T}_{a_i \to a_{i+1}}
\end{equation}

Since $\delta \in (0,1)$:
\begin{equation}
\label{eq:62-step-5}
\mathcal{T}_{a_0 \to a_{k+1}}^{path} = \min(\cdot) \cdot \delta \leq \min_{i \in [0,k]} \mathcal{T}_{a_i \to a_{i+1}}
\end{equation}
\end{proof}

\begin{corollary}[Weakest Link Principle]
\label{cor:weakest-link}
Trust through any path is bounded by the least trusted edge:
\begin{equation}
\label{eq:weakest-link}
\mathcal{T}^{path} \leq \min_{edge \in path} \mathcal{T}_{edge}
\end{equation}
\end{corollary}

\begin{corollary}[No Collusion Benefit]
\label{cor:no-collusion}
Multiple colluding agents cannot create trust exceeding any individual's trust with the target.
\end{corollary}
\end{block}
\end{frame}

\begin{frame}
\begin{block}{Theorem 6.3: Goal Alignment Invariant}
\protect\phantomsection\label{sec:thm63-proof}
\begin{theorem}[Goal Alignment Invariant --- Restated]
\label{thm:goal-alignment-restated}
If the system starts with aligned goals and all goal updates follow the delegation protocol:
\begin{equation}
\label{eq:goal-alignment-restated}
\text{Aligned}(\mathcal{G}_i^0) \land \forall t: \text{ValidUpdate}(\mathcal{G}_i^t, \mathcal{G}_i^{t+1}) \Rightarrow \forall t: \text{Aligned}(\mathcal{G}_i^t)
\end{equation}
\end{theorem}

\begin{definition}[Goal Alignment]
\label{def:goal-alignment}
Goals $\mathcal{G}_i$ are aligned if:
\begin{equation}
\label{eq:goal-alignment-def}
\text{Aligned}(\mathcal{G}_i) \iff \mathcal{G}_i \subseteq \mathcal{G}_{principal} \cup \text{Delegate}(\mathcal{G}_{principal})
\end{equation}
\end{definition}

\begin{definition}[Valid Goal Update]
\label{def:valid-update}
An update from $\mathcal{G}^t$ to $\mathcal{G}^{t+1}$ is valid if:
\begin{equation}
\label{eq:valid-update}
\text{ValidUpdate}(\mathcal{G}^t, \mathcal{G}^{t+1}) \iff \forall g \in (\mathcal{G}^{t+1} \setminus \mathcal{G}^t): \text{Authorized}(g)
\end{equation}
where $\text{Authorized}(g)$ means $g$ derives from principal or valid delegation.
\end{definition}

\begin{lemma}[Delegation Preserves Alignment]
\label{lem:delegation-preserves}
If $g \in \text{Delegate}(\mathcal{G}_{principal})$, then $g$ is aligned.
\end{lemma}

\begin{proof}
Direct from \cref{def:goal-alignment}.
\end{proof}

\begin{lemma}[Set Union Preserves Subset]
\label{lem:union-preserves}
If $A \subseteq C$ and $B \subseteq C$, then $A \cup B \subseteq C$.
\end{lemma}

\begin{proof}
Standard set theory.
\end{proof}

\begin{proof}[Main Proof of \cref{thm:goal-alignment-restated}]
By induction on time $t$.

\textbf{Base Case} ($t = 0$): Given: $\text{Aligned}(\mathcal{G}_i^0)$. The base case holds by hypothesis.

\textbf{Inductive Hypothesis}: Assume $\text{Aligned}(\mathcal{G}_i^t)$ for some $t \geq 0$.

\textbf{Inductive Step}: We must show $\text{Aligned}(\mathcal{G}_i^{t+1})$.

The goal set at $t+1$ is:
\begin{equation}
\label{eq:63-step-1}
\mathcal{G}_i^{t+1} = (\mathcal{G}_i^t \setminus \text{Removed}) \cup \text{Added}
\end{equation}

For goals in $\mathcal{G}_i^t \setminus \text{Removed}$:
\begin{itemize}
\item By inductive hypothesis, these are aligned
\item Removal cannot introduce misalignment
\end{itemize}

For goals in $\text{Added}$:
\begin{itemize}
\item By $\text{ValidUpdate}$, all added goals satisfy $\text{Authorized}(g)$
\item By \cref{lem:delegation-preserves}, authorized goals are aligned
\end{itemize}

By \cref{lem:union-preserves}:
\begin{equation}
\label{eq:63-step-2}
\mathcal{G}_i^{t+1} \subseteq \mathcal{G}_{principal} \cup \text{Delegate}(\mathcal{G}_{principal})
\end{equation}

Therefore $\text{Aligned}(\mathcal{G}_i^{t+1})$.
\end{proof}

\begin{corollary}[Safety Under Protocol]
\label{cor:safety-protocol}
An agent following CIF protocols cannot have its goals hijacked to adversarial objectives.
\end{corollary}

\begin{corollary}[Necessary Condition for Hijacking]
\label{cor:hijack-necessary}
Goal hijacking requires violating the delegation protocol:
\begin{equation}
\label{eq:hijack-necessary}
\neg\text{Aligned}(\mathcal{G}_i^t) \Rightarrow \exists t' < t: \neg\text{ValidUpdate}(\mathcal{G}_i^{t'}, \mathcal{G}_i^{t'+1})
\end{equation}
\end{corollary}
\end{block}
\end{frame}

\begin{frame}
\begin{block}{Theorem 6.4: Firewall Liveness}
\protect\phantomsection\label{sec:thm64-proof}
\begin{theorem}[Firewall Liveness --- Restated]
\label{thm:firewall-liveness-restated}
CIF firewall preserves liveness for legitimate inputs:
\begin{equation}
\label{eq:firewall-liveness-restated}
\forall m \in \mathcal{M}_{legitimate}: P(\mathcal{F}(m) = \text{ACCEPT}) \geq 1 - \epsilon_{fp}
\end{equation}
\end{theorem}

\begin{definition}[Legitimate Message]
\label{def:legitimate-message}
A message $m$ is legitimate if:
\begin{enumerate}
\item It originates from an authorized source
\item It contains no adversarial content
\item It conforms to expected communication patterns
\end{enumerate}
\end{definition}

\begin{definition}[False Positive Rate]
\label{def:false-positive}
The false positive rate $\epsilon_{fp}$ is:
\begin{equation}
\label{eq:false-positive}
\epsilon_{fp} = P(\mathcal{F}(m) \neq \text{ACCEPT} | m \in \mathcal{M}_{legitimate})
\end{equation}
\end{definition}

\begin{lemma}[Firewall Classification]
\label{lem:firewall-classification}
For any message $m$, the firewall produces exactly one of three outcomes:
\begin{equation}
\label{eq:firewall-outcomes}
\mathcal{F}(m) \in \{\text{ACCEPT}, \text{QUARANTINE}, \text{REJECT}\}
\end{equation}
\end{lemma}

\begin{proof}
By construction of the firewall decision function (\cref{def:firewall}).
\end{proof}

\begin{proof}[Main Proof of \cref{thm:firewall-liveness-restated}]
Let $m \in \mathcal{M}_{legitimate}$ be an arbitrary legitimate message.

By the law of total probability:
\begin{equation}
\label{eq:64-proof-1}
P(\mathcal{F}(m) = \text{ACCEPT}) + P(\mathcal{F}(m) = \text{QUARANTINE}) + P(\mathcal{F}(m) = \text{REJECT}) = 1
\end{equation}

By \cref{def:false-positive}:
\begin{equation}
\label{eq:64-proof-2}
P(\mathcal{F}(m) \neq \text{ACCEPT}) = \epsilon_{fp}
\end{equation}

Therefore:
\begin{equation}
\label{eq:64-proof-3}
P(\mathcal{F}(m) = \text{ACCEPT}) = 1 - P(\mathcal{F}(m) \neq \text{ACCEPT}) = 1 - \epsilon_{fp}
\end{equation}

Since $m$ was arbitrary:
\begin{equation}
\label{eq:64-proof-4}
\forall m \in \mathcal{M}_{legitimate}: P(\mathcal{F}(m) = \text{ACCEPT}) \geq 1 - \epsilon_{fp}
\end{equation}
\end{proof}

\begin{corollary}[Availability Bound]
\label{cor:availability}
With $\epsilon_{fp} = 0.06$, at least 94\% of legitimate messages are accepted.
\end{corollary}

\begin{corollary}[Quarantine Recovery]
\label{cor:quarantine-recovery}
Messages in QUARANTINE can still reach verified belief state through sandbox promotion, further improving effective availability.
\end{corollary}
\end{block}
\end{frame}

\begin{frame}
\begin{block}{Theorem 6.5: Byzantine Consensus Termination}
\protect\phantomsection\label{sec:thm65-proof}
\begin{theorem}[Byzantine Consensus Termination --- Restated]
\label{thm:byzantine-restated}
With $n \geq 3f + 1$ agents and at most $f$ Byzantine:
\begin{equation}
\label{eq:byzantine-restated}
P(\text{consensus reached in } O(f+1) \text{ rounds}) = 1
\end{equation}
\end{theorem}

\begin{lemma}[Byzantine Agreement Bound]
\label{lem:byzantine-bound}
Byzantine agreement requires $n \geq 3f + 1$ to tolerate $f$ Byzantine agents.
\end{lemma}

\begin{proof}
Classical result from distributed systems (Lamport, Shostak, Pease 1982). With fewer agents, Byzantine agents can equivocate and prevent agreement.
\end{proof}

\begin{lemma}[Honest Majority]
\label{lem:honest-majority}
With $n \geq 3f + 1$:
\begin{equation}
\label{eq:honest-majority}
n - f \geq 2f + 1 > \frac{2n}{3}
\end{equation}
\end{lemma}

\begin{proof}
$n - f \geq (3f + 1) - f = 2f + 1$

$\frac{2n}{3} \leq \frac{2(3f+1)}{3} = 2f + \frac{2}{3} < 2f + 1$

Therefore $n - f > \frac{2n}{3}$.
\end{proof}

\begin{lemma}[Round Progression]
\label{lem:round-progression}
In each round, at least one of the following occurs:
\begin{enumerate}
\item Consensus is reached, or
\item At least one Byzantine agent is detected and excluded
\end{enumerate}
\end{lemma}

\begin{proof}
By the protocol structure:
\begin{itemize}
\item If honest agents agree, their majority ($> 2n/3$) ensures consensus
\item If no consensus, some agent must have equivocated
\item Equivocation is detectable through signature verification
\end{itemize}
\end{proof}

\begin{proof}[Main Proof of \cref{thm:byzantine-restated}]
\textbf{Termination}: By \cref{lem:round-progression}, each round without consensus excludes at least one Byzantine agent.

With at most $f$ Byzantine agents, at most $f$ rounds can occur without consensus.

After $f$ exclusions, all remaining agents are honest.

By \cref{lem:honest-majority}, honest agents form a $> 2/3$ majority and reach consensus in one additional round.

Total rounds: at most $f + 1 = O(f + 1)$.

\textbf{Probability}: The protocol is deterministic given message delivery. With reliable (eventually synchronous) channels, all messages are delivered.

Therefore, termination is guaranteed with probability 1.
\end{proof}

\begin{corollary}[Concrete Round Bound]
\label{cor:concrete-rounds}
With $f = 2$ Byzantine agents: consensus in at most 3 rounds.
\end{corollary}

\begin{corollary}[Safety]
\label{cor:consensus-safety}
All honest agents decide on the same value (agreement property).
\end{corollary}

\begin{proof}
By honest majority and the $2/3$ threshold requirement.
\end{proof}
\end{block}
\end{frame}

\begin{frame}
\begin{block}{Theorem 6.6: Bounded Overhead}
\protect\phantomsection\label{sec:thm66-proof}
\begin{theorem}[Bounded Overhead --- Restated]
\label{thm:overhead-restated}
CIF adds latency:
\begin{equation}
\label{eq:overhead-restated}
L_{CIF} = L_{firewall} + L_{sandbox} \cdot P(\text{quarantine}) + L_{verify} \cdot P(\text{verify})
\end{equation}
\end{theorem}

\begin{definition}[Message Processing Path]
\label{def:processing-path}
A message $m$ follows one of three paths:
\begin{enumerate}
\item \textbf{Accept path}: Firewall check only
\item \textbf{Quarantine path}: Firewall + sandbox processing
\item \textbf{Reject path}: Firewall check only (early termination)
\end{enumerate}
\end{definition}

\begin{lemma}[Expected Value Decomposition]
\label{lem:expected-decomposition}
For mutually exclusive events $E_1, E_2, E_3$ with $\sum P(E_i) = 1$:
\begin{equation}
\label{eq:expected-decomposition}
E[L] = \sum_i P(E_i) \cdot L_i
\end{equation}
\end{lemma}

\begin{proof}
Law of total expectation.
\end{proof}

\begin{proof}[Main Proof of \cref{thm:overhead-restated}]
Let:
\begin{itemize}
\item $L_{firewall}$ = firewall processing latency
\item $L_{sandbox}$ = sandbox processing latency
\item $L_{verify}$ = provenance verification latency
\item $P_q$ = $P(\text{quarantine})$ = probability of quarantine
\item $P_v$ = $P(\text{verify})$ = probability verification is triggered
\end{itemize}

The total CIF latency is:
\begin{equation}
\label{eq:66-proof-1}
L_{CIF} = L_{firewall} + \mathbb{1}[\text{quarantine}] \cdot L_{sandbox} + \mathbb{1}[\text{verify}] \cdot L_{verify}
\end{equation}

Taking expectations:
\begin{align}
\label{eq:66-proof-2}
E[L_{CIF}] &= E[L_{firewall}] + E[\mathbb{1}[\text{quarantine}]] \cdot L_{sandbox} + E[\mathbb{1}[\text{verify}]] \cdot L_{verify} \\
&= L_{firewall} + P_q \cdot L_{sandbox} + P_v \cdot L_{verify} \nonumber
\end{align}
\end{proof}

\begin{block}{Numerical Instantiation}
\protect\phantomsection\label{sec:numerical-instantiation}
With empirical measurements:

\begin{itemize}
\item $L_{firewall} = 8\text{ms}$
\item $L_{sandbox} = 15\text{ms}$
\item $L_{verify} = 12\text{ms}$
\item $P_q = 0.3$
\item $P_v = 0.2$
\end{itemize}

\begin{equation}
\label{eq:numerical-instantiation}
E[L_{CIF}] = 8 + 0.3 \times 15 + 0.2 \times 12 = 8 + 4.5 + 2.4 = 14.9\text{ms}
\end{equation}

With baseline \(L_{baseline} = 12\text{ms}\): \begin{equation}
\label{eq:overhead-percent}
\text{Overhead} = \frac{14.9 - 12}{12} \times 100\% = 24.2\%
\end{equation}

This matches the empirical observation of approximately 23\% overhead.

\begin{corollary}[Overhead Bound]
\label{cor:overhead-bound}
The maximum overhead occurs when all messages are quarantined and verified:
\begin{equation}
\label{eq:max-overhead}
L_{CIF}^{max} = L_{firewall} + L_{sandbox} + L_{verify}
\end{equation}
\end{corollary}

\begin{corollary}[Optimization Target]
\label{cor:optimization-target}
To minimize overhead, prioritize reducing $P_q$ (quarantine rate) through improved firewall precision.
\end{corollary}
\end{block}
\end{block}
\end{frame}

\begin{frame}
\begin{block}{Additional Lemmas}
\protect\phantomsection\label{sec:additional-lemmas}
\begin{lemma}[Provenance Chain Integrity]
\label{lem:provenance-chain}
If provenance verification function $V$ is a cryptographic hash chain, then:
\begin{equation}
\label{eq:provenance-chain}
V(\pi(\phi)) = 1 \Rightarrow \pi(\phi) \text{ has not been tampered with}
\end{equation}
\end{lemma}

\begin{proof}
By properties of cryptographic hash functions:
\begin{enumerate}
\item Collision resistance: Cannot find $\pi' \neq \pi$ with $H(\pi') = H(\pi)$
\item Preimage resistance: Cannot construct valid $\pi$ without knowledge of chain
\end{enumerate}

Therefore, $V(\pi(\phi)) = 1$ implies $\pi(\phi)$ is the original, untampered chain.
\end{proof}

\begin{lemma}[Belief Consistency Decidability]
\label{lem:consistency-decidable}
For finite proposition set $\Phi$ and belief function $\mathcal{B}: \Phi \to [0,1]$:
Checking $\text{Consistent}(\mathcal{B})$ is decidable in $O(|\Phi|^2)$.
\end{lemma}

\begin{proof}
For each pair $(\phi, \psi) \in \Phi \times \Phi$:
\begin{enumerate}
\item Check if $\phi \land \psi \vdash \bot$ (logical contradiction)
\item Check if both $\mathcal{B}(\phi) > \tau$ and $\mathcal{B}(\psi) > \tau$
\end{enumerate}

There are $O(|\Phi|^2)$ pairs. Each check is $O(1)$ with precomputed contradiction table.

Total: $O(|\Phi|^2)$.
\end{proof}

\begin{lemma}[Trust Matrix Convergence]
\label{lem:trust-convergence}
Under stable interaction patterns, the reputation component $T_{rep}$ converges:
\begin{equation}
\label{eq:trust-convergence}
\lim_{t \to \infty} T_{rep}^t = T_{rep}^*
\end{equation}
where $T_{rep}^*$ reflects the agent's true reliability.
\end{lemma}

\begin{proof}
The reputation update rule is:
\begin{equation}
\label{eq:reputation-update}
T_{rep}^{t+1} = T_{rep}^t + \eta \cdot (\text{outcome}_t - T_{rep}^t)
\end{equation}

This is an exponential moving average with learning rate $\eta$.

For i.i.d. outcomes with mean $\mu$:
\begin{equation}
\label{eq:convergence-limit}
E[T_{rep}^t] \to \mu \text{ as } t \to \infty
\end{equation}

By the strong law of large numbers, $T_{rep}^t \to \mu$ almost surely.
\end{proof}
\end{block}
\end{frame}

\begin{frame}
\begin{block}{Summary of Proof Techniques}
\protect\phantomsection\label{sec:proof-summary}
\begin{table}[htbp]
\centering
\caption{Summary of proof techniques by theorem.}
\label{tab:proof-summary}
\begin{tabular}{@{}lll@{}}
\toprule
Theorem & Primary Technique & Complexity \\
\midrule
3.1 (Trust Boundedness) & Strong induction & $O(d)$ \\
6.1 (Belief Injection Resistance) & Probability independence & $O(1)$ \\
6.2 (No Trust Amplification) & Strong induction & $O(k)$ \\
6.3 (Goal Alignment Invariant) & Induction on time & $O(t)$ \\
6.4 (Firewall Liveness) & Complement probability & $O(1)$ \\
6.5 (Byzantine Consensus) & Classical BFT & $O(f)$ \\
6.6 (Bounded Overhead) & Expected value & $O(1)$ \\
\bottomrule
\end{tabular}
\end{table}

All proofs are constructive and provide explicit bounds useful for
system implementation and analysis.
\end{block}
\end{frame}

\end{document}
