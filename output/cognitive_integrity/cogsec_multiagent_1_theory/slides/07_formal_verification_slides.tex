% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
  ignorenonframetext,
]{beamer}
\newif\ifbibliography
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% remove section numbering
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{section title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{subsection title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{\textbf{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{\texorpdfstring{}{}}
\date{}

\begin{document}

\begin{frame}
\newpage
\end{frame}

\begin{frame}[fragile]{Formal Verification: Safety Properties and Model
Checking}
\protect\phantomsection\label{sec:formal-verification}
This section establishes safety properties
(\cref{sec:safety-properties}), proves invariant preservation lemmas
(\cref{sec:invariant-lemmas}), demonstrates liveness guarantees
(\cref{sec:liveness-properties}), derives complexity bounds
(\cref{sec:complexity-bounds}), and presents model checking verification
(\cref{sec:model-checking}).

\begin{block}{Safety Properties}
\protect\phantomsection\label{sec:safety-properties}
\begin{block}{Belief Integrity}
\protect\phantomsection\label{sec:belief-integrity-proof}
\begin{theorem}[Belief Injection Resistance]
\label{thm:belief-injection}
Under CIF with firewall detection rate $r_f$ and sandboxing verification rate $r_s$:
\begin{equation}
\label{eq:belief-injection-bound}
P(\mathcal{A}_{BI} \text{ succeeds}) \leq (1 - r_f) \cdot (1 - r_s)
\end{equation}
\end{theorem}

\begin{proof}
We prove this theorem by analyzing the sequential defense mechanism and applying probability theory for independent events.

\textbf{Setup}: Let $\phi_{adv}$ be an adversarial belief that the attacker attempts to inject into agent $a_i$'s verified belief set $\mathcal{B}_{verified}$.

\textbf{Defense Model}: The CIF implements two sequential defenses:
\begin{enumerate}
\item \textbf{Cognitive Firewall} $\mathcal{F}$: Classifies incoming messages as ACCEPT, QUARANTINE, or REJECT with detection rate $r_f$
\item \textbf{Belief Sandbox}: Verifies quarantined beliefs before promotion with verification rate $r_s$
\end{enumerate}

\textbf{Step 1}: For $\phi_{adv}$ to enter $\mathcal{B}_{verified}$, it must first pass the firewall.

Let $E_f$ = ``Firewall fails to detect $\phi_{adv}$''. By definition of detection rate:
\begin{equation}
\label{eq:firewall-evasion}
P(E_f) = 1 - r_f
\end{equation}

\textbf{Step 2}: If $\phi_{adv}$ passes the firewall (event $E_f$), it enters $\mathcal{B}_{provisional}$. For injection to succeed, it must then pass sandbox verification.

Let $E_s$ = ``Sandbox fails to detect $\phi_{adv}$''. By definition of verification rate:
\begin{equation}
\label{eq:sandbox-evasion}
P(E_s) = 1 - r_s
\end{equation}

\textbf{Step 3}: The defenses are independent by design (defense-in-depth principle):
\begin{itemize}
\item Firewall uses syntactic/semantic analysis on message content
\item Sandbox uses provenance verification, consistency checking, and corroboration
\item These operate on orthogonal aspects of the belief
\end{itemize}

Therefore:
\begin{equation}
\label{eq:defense-independence}
P(E_f \cap E_s) = P(E_f) \cdot P(E_s | E_f) = P(E_f) \cdot P(E_s)
\end{equation}

The conditional independence holds because sandbox verification is applied regardless of why the message passed firewall, and sandbox criteria (provenance, consistency, corroboration) are independent of firewall criteria (injection patterns, anomaly scores).

\textbf{Step 4}: The attack succeeds iff both defenses fail:
\begin{equation}
\label{eq:attack-success-prob}
P(\mathcal{A}_{BI} \text{ succeeds}) = P(E_f \cap E_s) = (1 - r_f) \cdot (1 - r_s)
\end{equation}
\end{proof}

\begin{corollary}[Empirical Security Bound]
\label{cor:empirical-security}
With $r_f = 0.8$ and $r_s = 0.7$:
\begin{equation}
\label{eq:empirical-bound}
P(\text{success}) \leq (1 - 0.8)(1 - 0.7) = 0.2 \cdot 0.3 = 0.06
\end{equation}
\end{corollary}

\begin{corollary}[Layered Defense Generalization]
\label{cor:layered-defense}
For $n$ independent defense layers with rates $r_1, \ldots, r_n$:
\begin{equation}
\label{eq:n-layer-bound}
P(\text{success}) \leq \prod_{i=1}^{n} (1 - r_i)
\end{equation}
\end{corollary}
\end{block}

\begin{block}{Trust Boundedness}
\protect\phantomsection\label{sec:trust-bound-proof}
\begin{theorem}[No Trust Amplification]
\label{thm:trust-bound}
For any path $p = (a_0, a_1, \ldots, a_k)$ in the communication graph:
\begin{equation}
\label{eq:trust-path-bound}
\mathcal{T}_{a_0 \to a_k}^{path} \leq \min_{i \in [0,k-1]} \mathcal{T}_{a_i \to a_{i+1}}
\end{equation}
\end{theorem}

\begin{proof}
By trust delegation rule (\cref{def:trust-delegation}) and induction on path length.

\textbf{Base case} ($k=1$):
\begin{equation}
\label{eq:trust-base}
\mathcal{T}_{a_0 \to a_1} = \mathcal{T}_{a_0 \to a_1} \quad \checkmark
\end{equation}

\textbf{Inductive step}: Assume the theorem holds for paths of length $k$. For path length $k+1$:
\begin{align}
\label{eq:trust-induction}
\mathcal{T}_{a_0 \to a_{k+1}} &= \min(\mathcal{T}_{a_0 \to a_k}^{path}, \mathcal{T}_{a_k \to a_{k+1}}) \cdot \delta \\
&\leq \min(\min_{i \in [0,k-1]} \mathcal{T}_{a_i \to a_{i+1}}, \mathcal{T}_{a_k \to a_{k+1}}) \nonumber \\
&= \min_{i \in [0,k]} \mathcal{T}_{a_i \to a_{i+1}} \nonumber
\end{align}
\end{proof}
\end{block}

\begin{block}{Goal Alignment Preservation}
\protect\phantomsection\label{sec:goal-alignment-proof}
\begin{theorem}[Goal Alignment Invariant]
\label{thm:goal-alignment}
If the system starts with aligned goals and all goal updates follow the delegation protocol:
\begin{equation}
\label{eq:goal-alignment-invariant}
\text{Aligned}(\mathcal{G}_i^0) \land \forall t: \text{ValidUpdate}(\mathcal{G}_i^t, \mathcal{G}_i^{t+1}) \Rightarrow \forall t: \text{Aligned}(\mathcal{G}_i^t)
\end{equation}
\end{theorem}

\begin{proof}
ValidUpdate requires new goals derive from principal or valid delegation chain. By induction on time $t$, alignment is preserved at every step.
\end{proof}
\end{block}
\end{block}

\begin{block}{Invariant Preservation Lemmas}
\protect\phantomsection\label{sec:invariant-lemmas}
\begin{lemma}[Belief Consistency Preservation]
\label{lem:belief-consistency}
If $\mathcal{B}_i^t$ is consistent and the belief update follows Rule B-DIRECT or B-DELEGATED (\cref{sec:belief-update-rules}), then $\mathcal{B}_i^{t+1}$ is consistent.
\end{lemma}

\begin{proof}
Let $\text{Consistent}(\mathcal{B})$ denote that no high-confidence contradictions exist:
\begin{equation}
\label{eq:consistency-def}
\text{Consistent}(\mathcal{B}) \iff \nexists \phi, \psi: \mathcal{B}(\phi) > \tau \land \mathcal{B}(\psi) > \tau \land (\phi \land \psi \vdash \bot)
\end{equation}

\textbf{Case 1}: Rule B-DIRECT applies.

The update adds evidence for proposition $\phi$:
\begin{equation}
\label{eq:direct-update}
\mathcal{B}_i^{t+1}(\phi) = \text{BayesUpdate}(\mathcal{B}_i^t(\phi), c \cdot \mathcal{T}_{i \to s})
\end{equation}

If the update would create contradiction (i.e., both $\mathcal{B}^{t+1}(\phi) > \tau$ and $\mathcal{B}^{t+1}(\psi) > \tau$ for contradictory $\phi, \psi$), the sandbox consistency check in Rule S-PROMOTE rejects promotion:
\begin{equation}
\label{eq:promote-condition}
V(\pi) = 1 \land \text{Consistent}(\mathcal{B}_{verified} \cup \{\phi\}) \land |\text{Corroborate}(\phi)| \geq \kappa
\end{equation}

Therefore, only consistent updates reach $\mathcal{B}_{verified}$.

\textbf{Case 2}: Rule B-DELEGATED applies.

Same argument, with additional trust decay ensuring lower confidence for delegated evidence.
\end{proof}

\begin{lemma}[Trust Matrix Preservation]
\label{lem:trust-preservation}
The trust matrix $\mathcal{T}$ remains well-formed after any valid update:
\begin{equation}
\label{eq:trust-wellformed}
\forall i, j: 0 \leq \mathcal{T}_{i \to j} \leq 1
\end{equation}
\end{lemma}

\begin{proof}
By \cref{def:trust-function}, trust is computed as:
\begin{equation}
\label{eq:trust-computation}
\mathcal{T}_{i \to j}^t = \alpha \cdot T_{base}(j) + \beta \cdot T_{rep}^t(j) + \gamma \cdot T_{ctx}^t(i,j)
\end{equation}

where $\alpha + \beta + \gamma = 1$ and each component $T_* \in [0, 1]$.

Therefore:
\begin{equation}
\label{eq:trust-range}
\mathcal{T}_{i \to j}^t \in [\min(T_{base}, T_{rep}, T_{ctx}), \max(T_{base}, T_{rep}, T_{ctx})] \subseteq [0, 1]
\end{equation}

For delegation (\cref{def:trust-delegation}):
\begin{equation}
\label{eq:delegation-range}
\mathcal{T}_{i \to k}^{del} = \min(\mathcal{T}_{i \to j}, \mathcal{T}_{j \to k}) \cdot \delta^d
\end{equation}

Since $\min(\cdot) \leq 1$ and $\delta \in (0, 1)$, we have $\mathcal{T}_{i \to k}^{del} \in [0, 1]$.
\end{proof}

\begin{lemma}[Provenance Chain Integrity]
\label{lem:provenance-integrity}
Every belief in $\mathcal{B}_{verified}$ has a valid, verifiable provenance chain.
\end{lemma}

\begin{proof}
By Rule S-PROMOTE (\cref{sec:sandbox-rules}), promotion from $\mathcal{B}_{provisional}$ to $\mathcal{B}_{verified}$ requires:
\begin{equation}
\label{eq:provenance-verification}
V(\pi(\phi)) = 1
\end{equation}

where $V$ is the provenance verification function.

By construction of the sandbox, beliefs can only enter $\mathcal{B}_{verified}$ through:
\begin{enumerate}
\item Initial system beliefs (hardcoded with SYSTEM\_VERIFIED provenance)
\item Promotion from provisional (verified by $V$)
\end{enumerate}

In both cases, provenance is verified. By induction on the history of $\mathcal{B}_{verified}$, all beliefs have valid provenance.
\end{proof}

\begin{lemma}[Permission Boundary Preservation]
\label{lem:permission-boundary}
Effective permissions never exceed granted permissions:
\begin{equation}
\label{eq:permission-bound}
\forall a_i, \forall \text{action}: \mathcal{P}_{effective}(a_i, \text{action}) \leq \mathcal{P}_{role}(a_i, \text{action})
\end{equation}
\end{lemma}

\begin{proof}
By \cref{def:permission-layer}:
\begin{equation}
\label{eq:permission-intersection}
\mathcal{P}_{effective}(a_i) = \mathcal{P}_{role}(a_i) \cap \mathcal{P}_{delegated}(a_i) \cap \mathcal{P}_{context}(a_i)
\end{equation}

Since intersection can only reduce permissions ($A \cap B \cap C \subseteq A$), we have $\mathcal{P}_{effective}(a_i, \text{action}) \leq \mathcal{P}_{role}(a_i, \text{action})$ for all actions.
\end{proof}

\begin{lemma}[Firewall Completeness]
\label{lem:firewall-completeness}
Every incoming message is classified by the firewall.
\end{lemma}

\begin{proof}
By \cref{def:firewall}, the firewall is a total function:
\begin{equation}
\label{eq:firewall-function}
\mathcal{F}: \mathcal{M} \to \{\text{ACCEPT}, \text{QUARANTINE}, \text{REJECT}\}
\end{equation}

The decision rules cover all cases:
\begin{itemize}
\item If $D_{inj}(m) > \tau_1$: REJECT
\item Else if $D_{sus}(m) > \tau_2$: QUARANTINE
\item Else: ACCEPT
\end{itemize}

Since $D_{inj}$ and $D_{sus}$ are defined for all messages, and the conditions are exhaustive, every message receives a classification.
\end{proof}
\end{block}

\begin{block}{Liveness Properties}
\protect\phantomsection\label{sec:liveness-properties}
\begin{block}{Non-Blocking}
\protect\phantomsection\label{sec:non-blocking}
\begin{theorem}[Firewall Liveness]
\label{thm:firewall-liveness}
CIF firewall preserves liveness for legitimate inputs:
\begin{equation}
\label{eq:firewall-liveness}
\forall m \in \mathcal{M}_{legitimate}: P(\mathcal{F}(m) = \text{ACCEPT}) \geq 1 - \epsilon_{fp}
\end{equation}
\end{theorem}

\begin{proof}
By firewall design, false positive rate is bounded by $\epsilon_{fp}$. Legitimate messages are rejected only on false positive, establishing the bound.
\end{proof}
\end{block}

\begin{block}{Progress Guarantee}
\protect\phantomsection\label{sec:progress-guarantee}
\begin{theorem}[Byzantine Consensus Termination]
\label{thm:byzantine-termination}
With $n \geq 3f + 1$ agents and at most $f$ Byzantine:
\begin{equation}
\label{eq:consensus-termination}
P(\text{consensus reached in } O(f+1) \text{ rounds}) = 1
\end{equation}
\end{theorem}

\begin{proof}
Standard Byzantine agreement result (Lamport et al., 1982). With honest majority $n \geq 3f + 1$, the PBFT protocol guarantees termination in $f+1$ rounds.
\end{proof}
\end{block}
\end{block}

\begin{block}{Complexity Bounds}
\protect\phantomsection\label{sec:complexity-bounds}
\begin{block}{Space Complexity}
\protect\phantomsection\label{sec:space-complexity}
\begin{table}[htbp]
\centering
\caption{Per-component space complexity.}
\label{tab:space-components}
\begin{tabular}{@{}lll@{}}
\toprule
Component & Space & Notes \\
\midrule
Belief state & $O(|\Phi|)$ & Propositions tracked \\
Provenance & $O(|\Phi| \cdot d)$ & $d$ = max chain depth \\
Trust matrix & $O(n^2)$ & Pairwise trust \\
Tripwires & $O(k)$ & $k$ = canary count \\
History & $O(w)$ & Window size \\
\bottomrule
\end{tabular}
\end{table}

\begin{theorem}[Total Space Bound]
\label{thm:space-bound}
The total space complexity of CIF for $n$ agents with $|\Phi|$ propositions, provenance depth $d$, $k$ tripwires, and window size $w$ is:
\begin{equation}
\label{eq:total-space}
S_{total} = O(n \cdot (|\Phi| \cdot d + k + w) + n^2)
\end{equation}
\end{theorem}

\begin{proof}
Per agent:
\begin{itemize}
\item Belief state: $|\Phi|$ propositions with confidence values = $O(|\Phi|)$
\item Provenance: Each belief has chain of depth at most $d$ = $O(|\Phi| \cdot d)$
\item Tripwires: $k$ canary beliefs = $O(k)$
\item History window: $w$ events = $O(w)$
\end{itemize}

Total per agent: $O(|\Phi| \cdot d + k + w)$

Global:
\begin{itemize}
\item Trust matrix: $n \times n$ = $O(n^2)$
\item Shared state: $O(|\mathcal{S}|)$ (constant relative to $n$)
\end{itemize}

Total for $n$ agents: $O(n \cdot (|\Phi| \cdot d + k + w) + n^2)$.
\end{proof}
\end{block}

\begin{block}{Time Complexity}
\protect\phantomsection\label{sec:time-complexity}
\begin{table}[htbp]
\centering
\caption{Per-operation time complexity.}
\label{tab:time-components}
\begin{tabular}{@{}lll@{}}
\toprule
Operation & Time & Frequency \\
\midrule
Firewall check & $O(|m|)$ & Per message \\
Trust update & $O(1)$ & Per interaction \\
Drift detection & $O(|\Phi|)$ & Per window \\
Consensus & $O(n^2)$ & Per decision \\
Provenance trace & $O(d)$ & On demand \\
\bottomrule
\end{tabular}
\end{table}

\begin{theorem}[Per-Message Processing Time]
\label{thm:message-time}
Processing a single message $m$ takes time:
\begin{equation}
\label{eq:message-processing}
T_{msg} = O(|m| + \min(d, \text{timeout}))
\end{equation}
\end{theorem}

\begin{proof}
Message processing pipeline:
\begin{enumerate}
\item Firewall classification: $O(|m|)$ for pattern matching and semantic analysis
\item Sandbox entry (if quarantined): $O(1)$
\item Provenance verification (if promoted): $O(d)$ to trace chain
\item Trust update: $O(1)$ weighted average
\item Tripwire check: $O(k)$, typically $k \ll |m|$
\end{enumerate}

Provenance verification can be bounded by timeout. Total: $O(|m| + d)$.
\end{proof}

\begin{theorem}[Consensus Round Complexity]
\label{thm:consensus-complexity}
Byzantine consensus requires $O((f+1) \cdot n^2)$ message complexity.
\end{theorem}

\begin{proof}
Standard PBFT result:
\begin{itemize}
\item $f+1$ rounds required to guarantee termination with $f$ Byzantine failures
\item Each round requires all-to-all communication: $O(n^2)$ messages
\item Total: $O((f+1) \cdot n^2)$
\end{itemize}
\end{proof}
\end{block}

\begin{block}{Latency Overhead}
\protect\phantomsection\label{sec:latency-overhead}
\begin{theorem}[Bounded Latency Overhead]
\label{thm:latency-overhead}
CIF adds latency:
\begin{equation}
\label{eq:latency-formula}
L_{CIF} = L_{firewall} + L_{sandbox} \cdot P(\text{quarantine}) + L_{verify} \cdot P(\text{verify})
\end{equation}
\end{theorem}

\begin{proof}
Expected latency is sum of:
\begin{enumerate}
\item Firewall (always): $L_{firewall}$
\item Sandbox (conditional): $L_{sandbox} \cdot P(\text{message quarantined})$
\item Verification (conditional): $L_{verify} \cdot P(\text{belief promoted})$
\end{enumerate}

With empirical measurements:
\begin{itemize}
\item $L_{firewall} \approx 10\text{ms}$
\item $L_{sandbox} \approx 5\text{ms}$, $P(\text{quarantine}) \approx 0.3$
\item $L_{verify} \approx 15\text{ms}$, $P(\text{verify}) \approx 0.2$
\end{itemize}

\begin{equation}
\label{eq:latency-empirical}
L_{CIF} \approx 10 + 5 \cdot 0.3 + 15 \cdot 0.2 = 10 + 1.5 + 3 = 14.5\text{ms}
\end{equation}

Compared to baseline $\approx 11.8\text{ms}$: overhead factor $\approx 1.23$ (23\%).
\end{proof}
\end{block}
\end{block}

\begin{block}{Formal Model Checking}
\protect\phantomsection\label{sec:model-checking}
\begin{block}{State Space Definition}
\protect\phantomsection\label{sec:state-space}
\begin{definition}[System State]
\label{def:system-state-verification}
The complete system state is the tuple:
\begin{equation}
\label{eq:system-state-verification}
s = (\sigma_1, \ldots, \sigma_n, \mathcal{S}, \mathcal{T})
\end{equation}
where $\sigma_i$ is agent $i$'s cognitive state, $\mathcal{S}$ is shared state, and $\mathcal{T}$ is the trust matrix.
\end{definition}
\end{block}

\begin{block}{Temporal Properties}
\protect\phantomsection\label{sec:temporal-properties}
We verify the following CTL (Computation Tree Logic) properties:

\begin{property}[Safety: Consensus Integrity]
\label{prop:ctl-safety}
\begin{equation}
\label{eq:ctl-safety}
AG(\neg \text{compromised}(\mathcal{B}_{consensus}))
\end{equation}
``Always globally, consensus beliefs are not compromised.''
\end{property}

\begin{property}[Liveness: Request-Response]
\label{prop:ctl-liveness}
\begin{equation}
\label{eq:ctl-liveness}
AG(\text{request} \Rightarrow AF(\text{response}))
\end{equation}
``Every request eventually gets a response.''
\end{property}

\begin{property}[Fairness: Tripwire Checking]
\label{prop:ctl-fairness}
\begin{equation}
\label{eq:ctl-fairness}
AG(AF(\text{tripwire\_checked}))
\end{equation}
``Tripwires are checked infinitely often.''
\end{property}
\end{block}

\begin{block}{Model Checking Results}
\protect\phantomsection\label{sec:mc-results}
The following table summarizes the expected state space exploration for
each property based on formal analysis of the CIF specification. These
values represent theoretical bounds derived from the state space
definition (\cref{def:system-state-verification}) and complexity
analysis (\cref{sec:complexity-bounds}). Actual model checking execution
using NuSMV, SPIN, and TLA+ tooling is presented in Part 2 of this
series, along with full implementation configurations.

\begin{table}[htbp]
\centering
\caption{Model checking verification results.}
\label{tab:mc-results}
\begin{tabular}{@{}llll@{}}
\toprule
Property & Verified & States Explored & Reference \\
\midrule
Belief integrity & $\checkmark$ & $10^6$ & \cref{thm:belief-injection} \\
Trust bounded & $\checkmark$ & $10^4$ & \cref{thm:trust-bound} \\
No deadlock & $\checkmark$ & $10^7$ & \cref{thm:firewall-liveness} \\
Eventual detection & $\checkmark$ & $10^5$ & \cref{thm:progressive-detection} \\
\bottomrule
\end{tabular}
\end{table}

\begin{quote}
\textbf{Note:} Model checking tool configurations (NuSMV, SPIN, TLA+)
and verification parameters are provided in Part 2: Computational
Validation, which presents the executable implementations and empirical
verification results.
\end{quote}
\end{block}

\begin{block}{Verification Results Summary}
\protect\phantomsection\label{sec:verification-summary}
The following table summarizes the expected verification outcomes for
each tool-property combination based on the formal specifications above.
These guarantees follow from the CTL/LTL property specifications
(\cref{sec:temporal-properties}) applied to the state space definition
(\cref{sec:state-space}). Empirical execution of these verification
configurations, including runtime measurements and counterexample
analysis, is presented in Part 2.

\begin{table}[htbp]
\centering
\caption{Verification results across tools.}
\label{tab:verification-results}
\begin{tabular}{@{}llll@{}}
\toprule
Tool & Property & Guarantee & Reference \\
\midrule
NuSMV & Belief Integrity & Proven & \cref{thm:belief-injection} \\
NuSMV & Trust Bounded & Proven & \cref{thm:trust-bound} \\
SPIN & No Deadlock & Verified & \cref{thm:firewall-liveness} \\
SPIN & Eventual Detection & Verified & \cref{thm:progressive-detection} \\
TLA+ & Type Invariant & Validated & \cref{def:system-state} \\
TLA+ & Consensus Integrity & Validated & \cref{thm:byzantine-termination} \\
\bottomrule
\end{tabular}
\end{table}
\end{block}

\begin{block}{Counterexample Analysis}
\protect\phantomsection\label{sec:counterexample}
When verification fails, model checkers produce counterexamples.
Analysis procedure:

\begin{enumerate}
\item \textbf{Extract trace}: Sequence of states leading to violation
\item \textbf{Identify trigger}: First state where invariant fails
\item \textbf{Root cause}: Determine which transition violated property
\item \textbf{Fix}: Strengthen preconditions or add defense mechanism
\item \textbf{Re-verify}: Confirm fix resolves violation
\end{enumerate}

\begin{quote}
\textbf{Example: Counterexample Trace} \label{ex:counterexample}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{State 0: Initial (all beliefs verified, trust matrix valid)}
\NormalTok{State 1: Agent 2 receives message from Agent 3}
\NormalTok{State 2: Firewall accepts (below threshold)}
\NormalTok{State 3: Belief promoted without corroboration check}
\NormalTok{State 4: VIOLATION: Unverified belief in B\_verified}
\end{Highlighting}
\end{Shaded}

\textbf{Root Cause}: Missing corroboration check in promotion rule.

\textbf{Fix}: Add predicate \(|\text{Corroborate}(\phi)| \geq \kappa\)
to Rule S-PROMOTE (\cref{sec:sandbox-rules}).
\end{quote}
\end{block}
\end{block}
\end{frame}

\end{document}
