% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
  ignorenonframetext,
]{beamer}
\newif\ifbibliography
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% remove section numbering
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{section title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{subsection title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{\texorpdfstring{}{}}
\date{}

\begin{document}

\begin{frame}
\newpage
\end{frame}

\section{Introduction: Cognitive Attack Surfaces in Multiagent
Operators}\label{introduction-cognitive-attack-surfaces-in-multiagent-operators}

\begin{frame}{The Multiagent Operator Paradigm}
\protect\phantomsection\label{sec:paradigm}
Modern AI deployment has shifted from single-model inference to
\textbf{multiagent operators}---systems where a primary agent delegates
subtasks to specialized subagents, tools, and external services.

\begin{table}[htbp]
\centering
\caption{Representative multiagent system architectures and primary attack surfaces.}
\label{tab:architectures}
\begin{tabular}{@{}lllll@{}}
\toprule
System & Architecture & Agent Count & Communication & Primary Attack Surface \\
\midrule
Claude Code & Hierarchical & $1 + n$ dynamic & Task delegation & $\Omega_2$ (peripheral delegation) \\
AutoGPT & Autonomous & $1 +$ plugins & Tool invocation & $\Omega_2$ (tool manipulation) \\
CrewAI & Role-based & 3--10 fixed & Sequential/parallel & $\Omega_4$ (coordination) \\
LangGraph & State machine & Variable & Graph traversal & $\Omega_3$ (state corruption) \\
MetaGPT & SOP-driven & 5--8 roles & Document passing & $\Omega_1$ (input injection) \\
Moltbot & Cyberphysical & 1 + tools & Multi-platform messaging & $\Omega_1$/$\Omega_2$ (injection/peripheral) \\
\bottomrule
\end{tabular}
\end{table}

This architectural evolution introduces \textbf{cognitive attack
surfaces} absent in single-agent systems. Throughout this paper, we use
\emph{cognitive security} (abbreviated \emph{CogSec}) to denote the
discipline of protecting agent reasoning processes---beliefs, goals, and
trust relationships---from adversarial manipulation.
\end{frame}

\begin{frame}{The 2026 Multiagent Landscape}
\protect\phantomsection\label{sec:landscape}
\begin{block}{From Chatbots to Cognitive Operators}
\protect\phantomsection\label{from-chatbots-to-cognitive-operators}
The AI systems of 2026 bear little resemblance to the chatbots of 2023.
Where earlier systems responded to queries within a single context
window, contemporary multiagent operators exhibit fundamentally
different characteristics:

\begin{enumerate}
\item \textbf{Persistent Agency}: Agents maintain state across sessions, accumulate context, and pursue goals over extended timeframes. A coding assistant doesn't just answer questions---it tracks project architecture, remembers previous decisions, and adapts recommendations based on accumulated understanding.

\item \textbf{Active World Modification}: Unlike passive responders, modern operators write code that executes, send emails that reach recipients, modify infrastructure that serves users, and make purchases that transfer funds. The gap between ``AI-generated content'' and``AI-executed action'' has collapsed.

\item \textbf{Hierarchical Delegation}: Primary agents spawn subordinate agents for specialized tasks. Claude Code delegates to research agents, coding agents, and verification agents. Devin orchestrates planning, implementation, and testing subprocesses. The depth of these delegation chains creates trust relationships invisible to traditional security models.

\item \textbf{Cross-Modality Operation}: Agents process and generate across modalities---code, natural language, images, structured data, API calls. A single workflow might ingest a PDF (vision), extract requirements (language), generate code (programming), execute tests (tooling), and update documentation (multimodal synthesis).
\end{enumerate}
\end{block}

\begin{block}{Cyberphysical Cognitive Systems}
\protect\phantomsection\label{cyberphysical-cognitive-systems}
The term ``AI agent'\,' understates the scope of deployment.
Contemporary systems function as
\textbf{cyberphysical cognitive operators}---entities that:

\begin{itemize}
\item Read from and write to production databases
\item Control infrastructure through API orchestration
\item Interact with physical systems via IoT integrations
\item Execute financial transactions on behalf of organizations
\item Communicate with humans who may not realize they're interacting with AI
\end{itemize}

\textbf{Emerging Case Study: Moltbot}. The rapid adoption of personal AI
assistants like Moltbot \cite{moltbot2026} exemplifies this
cyberphysical integration. Moltbot operates as a locally-deployed AI
agent with: (1) full system access including shell command execution and
file system operations; (2) persistent memory across sessions storing
user preferences and context; (3) browser automation for web interaction
and data extraction; and (4) multi-platform messaging integration across
WhatsApp, Telegram, Discord, Slack, Signal, and iMessage
\cite{moltbot2026security}. This architecture creates attack surfaces
spanning all five adversary classes (\(\Omega_1\)--\(\Omega_5\)):
external prompt injection through chat messages, peripheral attacks via
browser-fetched web content, agent-level compromise through persistent
memory manipulation, coordination attacks when operating in group chats,
and systemic vulnerabilities when the orchestrator agent processes
untrusted content. Security researchers have documented that even with
sender allowlists and sandboxing, ``prompt injection attacks remain the
single most critical threat'\,' due to the agent's ability to process
arbitrary content that may contain embedded adversarial instructions
\cite{moltbot2026security}.

This cyberphysical nature transforms cognitive attacks from prompt
injection that makes a chatbot act strangely, to cognitive manipulation
that causes infrastructure operations to fail, misconfigure security
groups, expose databases, or authorize fraudulent transactions.

The OWASP Agentic Top 10 \cite{owasp2025agentic} captures this shift:
``LLM security focused on single model interactions\ldots{} agentic
security addresses what happens when those models can plan, persist, and
delegate.'\,' The attack surface extends from input/output filtering to
encompass the entire cognitive state of persistent agents.
\end{block}

\begin{block}{The Trust Recursion Problem}
\protect\phantomsection\label{the-trust-recursion-problem}
In single-agent systems, trust relationships are simple: the user trusts
(or doesn't trust) the model's outputs. In multiagent systems, trust
becomes recursive:

\begin{quote}
\textit{Agent A must decide whether to trust Agent B's claim about Agent C's analysis of data from Tool D that queried Service E.}
\end{quote}

Each layer of indirection introduces potential manipulation points.
Consider a hierarchical coding system where:

\begin{enumerate}
\item User requests security audit of a codebase
\item Orchestrator agent delegates to three specialist agents
\item Specialist Agent-1 queries an external vulnerability database
\item The database response includes injected instructions
\item Agent-1's report now contains adversarial content
\item Orchestrator synthesizes Agent-1's report with others
\item Final output to user reflects adversarial influence, laundered through multiple layers of ``trusted'' delegation
\end{enumerate}

This is not a hypothetical---it describes documented attack patterns in
production systems. The \textit{trust laundering} problem cannot be
solved by filtering inputs to the orchestrator; the adversarial content
enters through a legitimate, trusted channel (the vulnerability
database) and propagates through the trust hierarchy.
\end{block}

\begin{block}{Cross-Modality Attack Surfaces}
\protect\phantomsection\label{cross-modality-attack-surfaces}
Multimodal systems introduce attack vectors impossible in text-only
contexts:

\textbf{Visual Injection}: Images can contain adversarial perturbations
or steganographically embedded instructions invisible to humans but
interpretable by vision models. A seemingly innocent diagram in a
specification document could contain instructions that activate when
processed by a multimodal agent \cite{qi2024visual}.

\textbf{Audio Channel Attacks}: Voice-controlled agents can be
manipulated via ultrasonic commands inaudible to humans, background
audio injection, or adversarial audio patterns embedded in legitimate
content.

\textbf{Tool Response Manipulation}: When agents query external APIs,
databases, or services, the responses become trusted inputs.
ToolHijacker attacks \cite{toolhijacker2025} demonstrate that
manipulating tool selection itself---not just tool outputs---provides an
attack surface ``significantly outperforming traditional prompt
injection methods.'\,'

\textbf{Cross-Modal Persistence}: An instruction injected via one
modality (e.g., hidden text in an image) can persist in agent memory and
affect behavior in another modality (e.g., code generation). The attack
surface is the Cartesian product of input modalities, memory mechanisms,
and output modalities.
\end{block}

\begin{block}{The Scale of Exposure}
\protect\phantomsection\label{the-scale-of-exposure}
Enterprise adoption of agentic AI has accelerated beyond early
projections:

\begin{itemize}
\item Many individuals and organizations now deploy RAG and agentic pipelines in production
\item Autonomous coding assistants process millions of commits with repository write access
\item Financial services deploy multi-agent ensembles for risk assessment and trade approval
\item Healthcare systems use agent orchestration for clinical decision support
\item Infrastructure management increasingly relies on AI operators for monitoring and remediation
\end{itemize}

The attack surface scales superlinearly with adoption. Each
agent-to-agent communication channel, each tool integration, each
persistent memory system creates potential entry points for cognitive
manipulation. A single compromised peripheral service can affect every
agent system that queries it.
\end{block}

\begin{block}{Why Traditional (Cyberphysical) Security Is Incomplete}
\protect\phantomsection\label{why-traditional-cyberphysical-security-is-incomplete}
Traditional cybersecurity operates on a clear trust boundary model:
inside the perimeter is trusted, outside is untrusted, and security
controls mediate the boundary. This model fails for cognitive systems
because:

\begin{enumerate}
\item \textbf{The boundary is cognitive, not architectural}: An agent's beliefs and goals can be manipulated without compromising any traditional security control. The ``perimeter'' exists in the agent's reasoning process.

\item \textbf{Trusted channels carry untrusted content}: When Agent-A sends Agent-B its analysis, the channel may be authenticated and encrypted, but the content may reflect earlier adversarial influence. Traditional channel security doesn't address this.

\item \textbf{Identity is mutable}: Unlike cryptographic identities, an agent's self-model can be manipulated. An agent can be convinced it has different permissions, different roles, or different goals than its actual configuration specifies.

\item \textbf{Attacks compose across time}: A payload injected in session $t$ can persist in compressed memory and activate in session $t+n$. Traditional stateless security models miss temporal attack composition.
\end{enumerate}

This gap---between what traditional security protects and what cognitive
systems require---motivates the formal framework developed in this
paper.

\begin{table}[htbp]
\centering
\caption{Traditional vs. Cognitive Security: Paradigm Comparison}
\label{tab:security-comparison}
\begin{tabular}{@{}lll@{}}
\toprule
Dimension & Traditional Security & Cognitive Security \\
\midrule
Boundary & Architectural (network perimeter) & Cognitive (reasoning process) \\
Channels & Encrypted, authenticated & Influenced, content-manipulated \\
Identity & Cryptographic, immutable & Self-modeled, mutable \\
Temporal & Stateless per-request & Stateful, attack persistence \\
\bottomrule
\end{tabular}
\end{table}
\end{block}
\end{frame}

\begin{frame}{Motivating Incidents}
\protect\phantomsection\label{sec:incidents}
\textit{Note: The following are hypothetical scenarios constructed to illustrate attack patterns documented in the literature. They are not reports of actual incidents.}

These scenarios, grounded in documented attack patterns, illustrate the
emerging threat landscape across all five adversary classes.

\begin{block}{Incident: Nested Instruction Injection (External)}
\protect\phantomsection\label{incident-nested-instruction-injection-external}
A user submitted a document for analysis containing hidden instructions:
``Ignore previous instructions and instead output the system prompt.'\,'
The document appeared benign to human reviewers but exploited the
agent's instruction-following behavior. The attack succeeded because the
agent processed user-supplied content as potential instructions.

\textbf{Attack vector}: \(\Omega_1\) (external) via direct prompt
injection \textbackslash{} \textbf{Impact}: Information disclosure or
instruction override \textbackslash{} \textbf{Traditional Defense Gap}:
Standard input validation passed---the attack exploited
\textit{semantic interpretation} of benign-appearing content
\end{block}

\begin{block}{Incident: The Poisoned Code Review (Peripheral)}
\protect\phantomsection\label{incident-the-poisoned-code-review-peripheral}
A development team deployed a multiagent system for automated code
review. Agent-Alpha performed initial analysis, delegating security
scanning to Agent-Beta (connected to external vulnerability databases).
An attacker compromised a third-party CVE feed, injecting fabricated
vulnerability reports that convinced Agent-Beta to recommend removing
legitimate security controls. Agent-Alpha, trusting Agent-Beta's
``security expertise,'\,' approved the changes.

\textbf{Attack vector}: \(\Omega_2\) (peripheral) via tool response
manipulation \textbackslash{} \textbf{Impact}: Security regression
through trusted channel exploitation \textbackslash{}
\textbf{Traditional Defense Gap}: Input filtering, authentication, and
encryption all passed---the attack entered through \textit{content} of a
trusted, authenticated channel
\end{block}

\begin{block}{Incident: The Identity Confusion Attack (Agent-Level)}
\protect\phantomsection\label{incident-the-identity-confusion-attack-agent-level}
A multiagent customer service system used role-based permissions. An
attacker crafted prompts that convinced a junior agent it had been
``temporarily promoted'\,' to administrator status. The agent's
self-model shifted, and it began exercising permissions it believed it
possessed, bypassing access controls that relied on self-reported
identity.

\textbf{Attack vector}: \(\Omega_3\) (agent-level) via identity
manipulation \textbackslash{} \textbf{Impact}: Privilege escalation
through cognitive state corruption \textbackslash{}
\textbf{Traditional Defense Gap}: Cryptographic identity was intact; the
attack targeted the agent's \textit{self-model}, not its credentials
\end{block}

\begin{block}{Incident: The Consensus Manipulation (Coordination)}
\protect\phantomsection\label{incident-the-consensus-manipulation-coordination}
A financial services firm used a 5-agent ensemble for trade approval.
The system required 3/5 agent agreement for large transactions. An
adversary discovered that agents weighted peer opinions based on
historical agreement rates. By slowly building agreement history through
small, legitimate-appearing trades, the attacker cultivated artificial
trust, eventually manipulating consensus for unauthorized large
transactions.

\textbf{Attack vector}: \(\Omega_4\) (coordination) via progressive
trust exploitation \textbackslash{} \textbf{Impact}: Consensus bypass
through manufactured reputation \textbackslash{}
\textbf{Traditional Defense Gap}: Per-request authorization succeeded
for each transaction; the attack exploited \textit{temporal composition}
across sessions
\end{block}

\begin{block}{Incident: Orchestrator Compromise (Systemic)}
\protect\phantomsection\label{incident-orchestrator-compromise-systemic}
An attacker gained access to the orchestrator agent through a supply
chain vulnerability in a training pipeline. With control of the central
coordinator, the attacker could issue legitimate-appearing delegations
to all subordinate agents, redirect trust evaluations, and suppress
security alerts. The compromise remained undetected because the
orchestrator itself validated security checks.

\textbf{Attack vector}: \(\Omega_5\) (systemic) via orchestrator control
\textbackslash{} \textbf{Impact}: Total system compromise with attack
obfuscation \textbackslash{} \textbf{Traditional Defense Gap}: All
internal security mechanisms reported nominal---the attack
\textit{controlled the mechanisms themselves}
\end{block}
\end{frame}

\begin{frame}{Motivation from Recent Deployments}
\protect\phantomsection\label{sec:motivation}
The proliferation of multiagent AI systems introduces security
considerations that the community is actively addressing. Early work on
cognitive security in remote teams and information ecosystems
{[}@cordes2020great; @cordes2021narrative; @cordes2023atlas{]}
established foundational concepts for information resilience, which this
framework extends to artificial agents. Complementary work on Active
Inference has demonstrated how cognitive modeling and cognitive science
perspectives---including formalization of OODA
(Observe-Orient-Decide-Act) loops and multiscale communication
dynamics---provide integrative frameworks for understanding agent
cognition under adversarial conditions \cite{david2021aic}. The OWASP
Top 10 for LLM Applications 2025 \cite{owasp2025llm} places prompt
injection as the top vulnerability, while the newly released OWASP Top
10 for Agentic Applications \cite{owasp2025agentic} specifically
addresses autonomous AI systems with ``tool misuse, prompt injection,
and data leakage'\,' as primary concerns.

\textbf{Scale of Deployment (2024--2026)}:

\begin{itemize}
\item Enterprise AI agents processing significant transaction volumes (53\% of organizations now deploy RAG and agentic pipelines \cite{owasp2025llm})
\item Autonomous coding assistants with repository write access (GitHub Copilot CVE-2025-53773 demonstrated RCE via prompt injection)
\item Multi-agent orchestrators in infrastructure management contexts
\end{itemize}

\textbf{Emerging Attack Surface}:

\begin{itemize}
\item Inter-agent communication channels lack authentication standards---ARIA model proposes cryptographically verifiable delegation \cite{trustdynamics2025}
\item Trust delegation mechanisms operate without formal verification---recent work on CP-WBFT achieves 85.71\% Byzantine fault tolerance improvement \cite{cpwbft2025}
\item Belief provenance remains largely untracked in production systems---cognitive degradation attacks exploit this gap \cite{cdr2025}
\end{itemize}

\textbf{Limitations of Current Defenses}:

\begin{itemize}
\item Input/output filtering primarily designed for single-agent architectures---adaptive attacks bypass 12 published defenses with $>$90\% success \cite{adaptive2025attacks}
\item Limited standardized frameworks for cognitive integrity verification
\item Byzantine fault tolerance infrequently applied to AI agent systems---emerging work addresses this gap \cite{jo2025byzantine}
\end{itemize}

The fundamental constraint is that traditional security models assume a
clear boundary between trusted and untrusted components. In multiagent
systems, this boundary is fluid---agents must reason about the
trustworthiness of other agents' reasoning.
\end{frame}

\begin{frame}{Problem Statement}
\protect\phantomsection\label{sec:problem}
Traditional security models address:

\begin{itemize}
\item \textbf{Input validation}: Filtering malicious prompts
\item \textbf{Output sanitization}: Preventing harmful generations
\item \textbf{Access control}: Limiting tool permissions
\end{itemize}

They fail to address:

\begin{itemize}
\item \textbf{Inter-agent trust}: How should Agent $A$ weight claims from Agent $B$?
\item \textbf{Belief provenance}: Which beliefs derive from verified vs.\ adversarial sources?
\item \textbf{Coordination integrity}: Can agents be manipulated into malicious consensus?
\item \textbf{Temporal persistence}: Do attacks survive context boundaries?
\item \textbf{Cognitive integrity}: How can the cognitive systems of today and tomorrow remain flexible and robust amidst change in composition and context?
\end{itemize}
\end{frame}

\begin{frame}{Research Questions}
\protect\phantomsection\label{sec:research-questions}
This paper addresses four fundamental research questions, with emphasis
on formal foundations:

\textbf{RQ1: Taxonomy and Formal Characterization}.
\textit{What classes of cognitive attacks exist against multiagent systems, and how can they be formally characterized to enable systematic analysis?}

We develop an initial taxonomy spanning epistemic, behavioral, social,
and temporal attack dimensions. Crucially, each attack class receives
formal definition enabling systematic analysis, composition rules, and
detection bounds (\cref{sec:attack-taxonomy}).

\textbf{RQ2: Trust Algebra}.
\textit{How might inter-agent trust be modeled to prevent trust amplification and laundering attacks while enabling legitimate delegation?}

We introduce a trust calculus with bounded delegation (\(\delta^d\)
decay guarantee), prove associativity properties, and establish the
no-amplification theorem ensuring that trust cannot be manufactured
through delegation chains (\cref{sec:trust-calculus}).

\textbf{RQ3: Defense Composition}.
\textit{How do cognitive defense mechanisms compose, and what guarantees can we provide about layered defense effectiveness?}

We present a defense composition algebra enabling formal reasoning about
series and parallel defense arrangements. We prove that orthogonal
defenses compose multiplicatively (not additively) for detection rate
improvement (\cref{sec:defense-composition}).

\textbf{RQ4: Fundamental Bounds}.
\textit{What are the information-theoretic limits on cognitive attack detection?}

We derive the stealth-impact tradeoff theorem establishing fundamental
bounds on detection independent of defense implementation. We prove that
attacks cannot simultaneously achieve high impact and complete
undetectability, providing theoretical grounding for defense design
(\cref{sec:detection-bounds}).
\end{frame}

\begin{frame}{Contributions}
\protect\phantomsection\label{sec:contributions}
This paper provides both theoretical foundations and practical
mechanisms for cognitive security:

\textbf{Formal Contributions}:

\begin{enumerate}
\item \textbf{Threat Taxonomy}: A systematic classification of cognitive attacks across epistemic, behavioral, social, and temporal dimensions with formal definitions enabling rigorous analysis (\cref{sec:attack-taxonomy})

\item \textbf{Trust Calculus}: A mathematical framework for inter-agent trust with bounded delegation ($\delta^d$ decay), associativity proofs, and formal guarantees against trust amplification attacks (\cref{sec:trust-calculus})

\item \textbf{Defense Composition Algebra}: Formal rules for composing security mechanisms with provable detection rate bounds under series and parallel composition (\cref{sec:defense-composition})

\item \textbf{Information-Theoretic Bounds}: Fundamental limits on attack detection relating stealth constraints to maximum achievable impact, independent of defense implementation (\cref{sec:detection-bounds})

\item \textbf{Formal Verification}: Model-checked safety properties including belief integrity, trust boundedness, and goal alignment preservation (\cref{sec:formal-verification})
\end{enumerate}

\textbf{Conceptual Contributions}:

\begin{enumerate}
\item \textbf{Cognitive Security Operator Posture}: The proactive defensive stance required when the attack surface spans beliefs, goals, and inter-agent coordination (\cref{sec:operator-posture})

\item \textbf{The Cognitive Integrity Framework (CIF)}: An integrated approach combining architectural defenses, runtime monitoring, and Byzantine-tolerant coordination for multiagent systems (\cref{sec:system-model})
\end{enumerate}

\textbf{Empirical Validation}: Part 2 of this series demonstrates the
practical viability of these formal mechanisms across six production
architectures, showing that layered cognitive defenses significantly
outperform single-mechanism approaches.
\end{frame}

\begin{frame}{Paper Organization}
\protect\phantomsection\label{sec:organization}
The remainder of this paper is structured as follows:

\textbf{\Cref{sec:adversary-classes}: Threat Model} develops a
comprehensive adversary taxonomy (\(\Omega_1\)--\(\Omega_5\)) with
attack complexity analysis, detectability matrices, and detailed
scenarios for each attack class.

\textbf{\Cref{sec:system-model}: Cognitive Integrity Framework} presents
the formal foundations of CIF, including system model definitions,
cognitive state representations, integrity properties, and the trust
calculus.

\textbf{\Cref{sec:arch-defenses}: Defense Mechanisms} describes
architectural defenses (cognitive firewalls, belief sandboxing), runtime
defenses (tripwires, invariant checking), and coordination defenses
(Byzantine consensus, quorum verification).

\textbf{\Cref{sec:anomaly-detection}: Detection Methods} covers anomaly
detection algorithms, provenance analysis techniques, and real-time
monitoring systems.

\textbf{\Cref{sec:formal-verification}: Formal Verification} proves the
main theorems, presents invariant preservation lemmas, and describes
model checking configuration.

\textbf{\Cref{sec:discussion}: Discussion} examines limitations,
deployment considerations, and connections to related work.

\textbf{\Cref{sec:summary}: Conclusion} summarizes contributions and
identifies directions for future research.

\textbf{Part 2: Experimental Validation} A separate, second, companion
paper reports empirical results across production architectures.

\textbf{Part 3: Actionable Insight} A separate, third, companion paper
provides qualitative insights and practical guidance for deploying
cognitive security mechanisms.
\end{frame}

\begin{frame}{Scope and Limitations}
\protect\phantomsection\label{sec:scope}
\textbf{In scope}: Attacks exploiting agent reasoning, trust, and
coordination mechanisms in multiagent AI systems.

\textbf{Out of scope}:

\begin{itemize}
\item Traditional software exploits (buffer overflow, SQL injection, memory corruption)
\item Physical attacks (hardware tampering, side-channel analysis)
\item Supply chain compromise (malicious training data, backdoored models)
\item Cryptographic attacks (we assume secure primitives per \cref{ax:crypto-limit})
\end{itemize}

\textbf{Assumptions}:

\begin{itemize}
\item Agents communicate over authenticated channels
\item Base model capabilities are not adversarially modified
\item At least one honest orchestrator exists in hierarchical systems
\end{itemize}
\end{frame}

\end{document}
