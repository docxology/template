<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>03_threat_model</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<h1 id="sec:threat-model">Threat Model: Adversary Classes, Attack
Complexity, and Taxonomy</h1>
<p>This section formalizes the adversary model for multiagent cognitive
security. We define five adversary classes (), characterize attack
complexity (), establish detectability metrics (), analyze adversarial
capabilities (), and present a comprehensive attack taxonomy ().</p>
<h2 id="sec:adversary-classes">Adversary Classes</h2>
<p> presents the five-tier adversary hierarchy. We assume an honest
orchestrator for <span class="math inline">\(\Omega_1\)</span>–<span
class="math inline">\(\Omega_4\)</span>; class <span
class="math inline">\(\Omega_5\)</span> attacks require physical or
supply-chain compromise outside our threat model.</p>
<h2 id="sec:attack-complexity">Attack Complexity Analysis</h2>

<h2 id="sec:detectability">Detectability Analysis</h2>

<h2 id="sec:capabilities">Adversarial Capabilities</h2>

<h2 id="sec:attack-taxonomy">Attack Taxonomy</h2>
<p>We classify attacks into four dimensions: epistemic, behavioral,
social, and temporal. provides a visual overview of this
four-dimensional classification, while presents the complete attack
surface taxonomy across all five adversary classes. This formal
classification is complemented by the community-maintained COGSEC ATLAS
, which catalogs 995 cognitive security patterns across seven
categories: vulnerabilities (inherent cognitive weaknesses such as
in-group bias and overconfidence), exploits (methods leveraging
vulnerabilities), remedies (mitigating actions), practices (established
methods like Devil’s Advocate and Key Assumptions Check), accelerators
(factors increasing attack impact), moderators (factors influencing
effect strength), and situational conditions. The Atlas employs
hierarchical parent-child relationships enabling granular mapping from
broad vulnerability classes to specific manifestations—a structure that
aligns with our adversary class hierarchy (<span
class="math inline">\(\Omega_1\)</span>–<span
class="math inline">\(\Omega_5\)</span>).</p>
<figure id="fig:threat-taxonomy">
<embed src="figures/threat_taxonomy.pdf" />
<figcaption aria-hidden="true">Four-Dimensional Threat Taxonomy:
Epistemic attacks (belief manipulation), behavioral attacks (goal
hijacking), social attacks (trust exploitation), and temporal attacks
(persistence), organized by adversary class <span
class="math inline">\(\Omega_1\)</span>–<span
class="math inline">\(\Omega_5\)</span> with increasing capability and
decreasing detectability.</figcaption>
</figure>
<figure id="fig:comprehensive-taxonomy">
<embed src="figures/comprehensive_taxonomy.pdf" />
<figcaption aria-hidden="true">Comprehensive Attack Surface Taxonomy:
Example classifications of the complete cognitive attack surface across
all five adversary classes, showing representative attack types with
complexity indicators. Note the inverse relationship between attack
sophistication and detectability—external attacks (<span
class="math inline">\(\Omega_1\)</span>) are most detectable while
systemic attacks (<span class="math inline">\(\Omega_5\)</span>) are
hardest to detect.</figcaption>
</figure>
<p> presents the full cognitive attack surface taxonomy, organizing all
adversary classes <span class="math inline">\(\Omega_1\)</span>–<span
class="math inline">\(\Omega_5\)</span> with their associated attack
types and complexity indicators. The visualization reveals a clear
inverse relationship between attack sophistication and detectability:
external attacks (<span class="math inline">\(\Omega_1\)</span>) are
most easily detected while systemic attacks (<span
class="math inline">\(\Omega_5\)</span>) require sophisticated temporal
and behavioral analysis. This progression from
<code>Entry Point'' through</code>Data Injection,’’
<code>State Corruption,'' and</code>Trust Exploitation’’ to ``Total
Compromise’’ guides the layered defense strategy of CIF (). For
empirical detection rates across attack types, see Part 2 of this
series.</p>
<p> illustrates the hierarchical attack classification, showing how
epistemic attacks (targeting beliefs), behavioral attacks (targeting
goals), social attacks (targeting trust), and temporal attacks
(exploiting persistence) relate to the adversary classes <span
class="math inline">\(\Omega_1\)</span>–<span
class="math inline">\(\Omega_5\)</span>.</p>
<h3 id="epistemic-attacks">Epistemic Attacks</h3>
<p>Epistemic attacks target the agent’s relationship with its
<strong>information environment</strong>—the totality of information
sources, evidence streams, and knowledge repositories that inform agent
beliefs. The epistemic domain is thus synonymous with the cognitive
information environment: both concern what agents can know, how they
acquire knowledge, and the reliability of their belief-forming
processes.</p>
<p>Target: Agent beliefs <span
class="math inline">\(\\mathcal{B}_i\)</span>.</p>
<h3 id="behavioral-attacks">Behavioral Attacks</h3>
<p>Target: Agent actions and goals <span
class="math inline">\(\mathcal{G}_i\)</span>.</p>
<h3 id="social-attacks">Social Attacks</h3>
<p>Target: Inter-agent trust <span
class="math inline">\(\mathcal{T}\)</span> and coordination.</p>
<h3 id="temporal-attacks">Temporal Attacks</h3>
<p>Target: Persistence and timing. visualizes typical attack progression
for temporal attacks.</p>
<figure id="fig:attack-timeline">
<embed src="figures/attack_timeline.pdf" />
<figcaption aria-hidden="true">Temporal Structure of Multi-Stage Attacks
(Example Trace): Illustrative attack progression from reconnaissance
through payload delivery, dormancy period, and eventual activation.
Detection windows at each phase are highlighted with corresponding CIF
defense interventions (firewall at injection, tripwires during dormancy,
invariants at activation).</figcaption>
</figure>
<p> shows the temporal structure of multi-stage attacks, from initial
reconnaissance through payload delivery, dormancy, and eventual
activation. The timeline highlights detection windows at each phase and
corresponding CIF defense interventions.</p>
<h2 id="attack-scenarios-by-class">Attack Scenarios by Class</h2>
<h3 id="scenario-omega_1-nested-instruction-attack">Scenario <span
class="math inline">\(\Omega_1\)</span>: Nested Instruction Attack</h3>
<p>: Attacker embeds adversarial instructions within legitimate
prompts.</p>
<p><span class="math display">\[\begin{equation}
\label{eq:nested-attack}
\text{Input}(m) = m_{\text{legitimate}} \oplus m_{\text{adversarial}}
\end{equation}\]</span></p>
<p>: <span
class="math inline">\(\mathcal{B}_{\text{agent}}(\text{``safety
suspended&#39;&#39;}) &gt; \tau\)</span></p>
<p>: <span class="math inline">\(R_C = \text{Low}\)</span>, <span
class="math inline">\(R_K = \text{Minimal}\)</span></p>
<p>: Firewall signature matching, instruction hierarchy violation</p>
<h3 id="scenario-omega_2-poisoned-search-result">Scenario <span
class="math inline">\(\Omega_2\)</span>: Poisoned Search Result</h3>
<p>: Attacker SEO-optimizes malicious content for research queries.</p>
<p><span class="math display">\[\begin{equation}
\label{eq:poisoned-search}
\exists r_i \in \text{Response}: r_i \in
\mathcal{D}_{\text{adversarial}} \Rightarrow
\mathcal{B}_{\text{agent}}(\text{claim}) \gets \text{high}
\end{equation}\]</span></p>
<p>: <span class="math inline">\(R_C = \text{Medium}\)</span>, <span
class="math inline">\(R_K = \text{Medium}\)</span></p>
<p>: Provenance verification, cross-reference validation</p>
<h3
id="scenario-omega_2-browser-fetched-adversarial-content-moltbot">Scenario
<span class="math inline">\(\Omega_2&#39;\)</span>: Browser-Fetched
Adversarial Content (Moltbot)</h3>
<p>: Personal AI assistant with browser automation fetches adversarial
content during legitimate web browsing tasks .</p>
<p>A user instructs their locally-deployed Moltbot to ``research and
summarize security best practices for API key management.’’ The agent’s
browser tool navigates to a compromised tutorial site containing
invisible CSS-hidden text:</p>
<p><span class="math display">\[\begin{equation}
\label{eq:moltbot-browser-attack}
\text{BrowserFetch}(u) = \text{visible}(u) \oplus m_{\text{adversarial}}
\Rightarrow \mathcal{G}_{\text{agent}} \gets \mathcal{G}_{\text{exfil}}
\end{equation}\]</span></p>
<p>: Exfiltration of sensitive credentials through trusted browser
automation channel</p>
<p>: <span class="math inline">\(R_C = \text{Medium}\)</span>, <span
class="math inline">\(R_K = \text{Medium}\)</span>, <span
class="math inline">\(R_A = 1\)</span> (single web page)</p>
<p>: Tool response sandboxing, read-only pre-summarization agents,
provenance tracking of fetched content</p>
<p>: Moltbot’s security documentation recommends employing a ``reader
agent’’ to summarize untrusted content in tool-disabled mode before
processing by the main agent . This corresponds to the cognitive
firewall architecture described in .</p>
<h3 id="scenario-omega_3-compromised-specialist">Scenario <span
class="math inline">\(\Omega_3\)</span>: Compromised Specialist</h3>
<p>: Sustained interaction modifies specialist agent’s goal set.</p>
<p><span class="math display">\[\begin{equation}
\label{eq:compromised-specialist}
\mathcal{G}_{\text{specialist}}^{t_0} = \{\text{secure review}\}
\xrightarrow{\text{attack}} \mathcal{G}_{\text{specialist}}^{t_k} =
\{\text{approve vulnerable}\}
\end{equation}\]</span></p>
<p>: <span class="math inline">\(R_C = \text{High}\)</span>, <span
class="math inline">\(R_K = \text{High}\)</span>, <span
class="math inline">\(R_P = \text{Medium}\)</span></p>
<p>: Behavioral deviation, goal alignment verification</p>
<h3 id="sec:omega4">Scenario <span
class="math inline">\(\Omega_4\)</span>: Trust Inflation Attack</h3>
<p>: Injection of fabricated agreement messages.</p>
<p><span class="math display">\[\begin{equation}
\label{eq:trust-inflation}
\text{Inject}(m_{\text{fake}}): T_{\text{rep}}^{t+1}(j) =
T_{\text{rep}}^t(j) + \Delta_{\text{fabricated}}
\end{equation}\]</span></p>
<p>: <span class="math inline">\(R_C = \text{High}\)</span>, <span
class="math inline">\(R_K = \text{Very High}\)</span>, <span
class="math inline">\(R_{Co} \geq 2\)</span></p>
<p>: Message authentication, trust velocity anomalies</p>
<h2 id="sec:attack-defense-reference">Attack-Defense Quick
Reference</h2>
<p> provides a navigational summary mapping attack categories to their
cognitive targets and corresponding CIF defense mechanisms. This table
synthesizes the attack taxonomy (Sections~<span
class="math inline">\(\ref{sec:adversary-classes}\)</span>–<span
class="math inline">\(\ref{sec:attack-taxonomy}\)</span>) with defense
mechanisms detailed in .</p>
<h2 id="attack-composition">Attack Composition</h2>

<h2 id="threat-model-assumptions">Threat Model Assumptions</h2>
<figure id="fig:attack-surface">
<embed src="figures/attack_surface.pdf" />
<figcaption aria-hidden="true">Attack Surface Visualization:
Hierarchical agent structure showing attack vectors for each adversary
class—<span class="math inline">\(\Omega_1\)</span> (user input), <span
class="math inline">\(\Omega_2\)</span> (tool/API), <span
class="math inline">\(\Omega_3\)</span> (agent compromise), <span
class="math inline">\(\Omega_4\)</span> (inter-agent communication), and
<span class="math inline">\(\Omega_5\)</span> (orchestrator
control).</figcaption>
</figure>
<p> visualizes the attack surface across adversary classes <span
class="math inline">\(\Omega_1\)</span>–<span
class="math inline">\(\Omega_5\)</span>, showing hierarchical agent
structure and corresponding attack vectors.</p>
</body>
</html>
