% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
  ignorenonframetext,
]{beamer}
\newif\ifbibliography
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% remove section numbering
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{section title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{subsection title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{longtable,booktabs,array}
\newcounter{none} % for unnumbered tables
\usepackage{calc} % for calculating minipage widths
\usepackage{caption}
% Make caption package work with longtable
\makeatletter
\def\fnum@table{\tablename~\thetable}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{\texorpdfstring{}{}}
\date{}

\begin{document}

\begin{frame}{Hypothesis Evidence Landscape and Temporal Dynamics
\label{sec:hypothesis_results}}
\protect\phantomsection\label{hypothesis-evidence-landscape-and-temporal-dynamics}
The LLM-based extraction pipeline produced a total of 3\{,\}684
assertions across the eight tracked hypotheses, drawn from the full
corpus of \(N = 1208\) papers. The distribution of assertion types and
the resulting citation-weighted scores reveal a differentiated evidence
landscape:

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1429}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Hypothesis
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Score
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Supports
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Neutral
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Contradicts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Total
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Character
\end{minipage} \\
\midrule\noalign{}
\endhead
H4: Predictive Coding & \(+0.59\) & 837 & 417 & 0 & 1\{,\}254 & Strong
consensus \\
H5: Scalability & \(+0.62\) & 142 & 110 & 0 & 252 & Strong consensus \\
H6: Clinical Utility & \(+0.41\) & 16 & 29 & 0 & 45 & Moderate,
growing \\
H8: Language AIF & \(+0.39\) & 54 & 96 & 0 & 150 & Moderate, emerging \\
H7: Morphogenesis & \(+0.35\) & 23 & 61 & 1 & 85 & Moderate, emerging \\
H2: AIF Optimality & \(+0.22\) & 166 & 569 & 19 & 754 & Weakly
contested \\
H1: FEP Universality & \(+0.16\) & 297 & 1\{,\}071 & 2 & 1\{,\}370 &
Broad but diffuse \\
H3: Markov Blanket Realism & \(+0.02\) & 14 & 181 & 6 & 201 & Heavily
contested \\
\bottomrule\noalign{}
\end{longtable}
}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{../figures/hypothesis_dashboard.png}
\caption{Hypothesis scoring dashboard showing LLM-extracted evidence scores for the eight tracked hypotheses, sorted descending by consensus. Scores range from $-1$ (strong contradicting evidence) to $+1$ (strong supporting evidence).}
\label{fig:hypothesis_dashboard}
\end{figure}

\begin{block}{Interpretation of Evidence Profiles}
\protect\phantomsection\label{interpretation-of-evidence-profiles}
The eight hypotheses cluster into three distinct tiers. The
\textbf{consensus tier} (H4, H5) comprises hypotheses with strong
positive scores (\(> 0.5\)) and no contradicting assertions. Predictive
coding (H4), the most extensively assessed hypothesis with 1,254
assertions, has accumulated uniformly supportive evidence since the
1970s, reflecting the deep empirical grounding of hierarchical
prediction error models in neuroscience. Scalability (H5), while
assessed by fewer papers, shows a similarly strong positive trajectory
that accelerated after 2017 as deep active inference architectures
emerged.

The \textbf{moderate tier} (H6, H7, H8) comprises hypotheses with
positive but lower scores (\(0.3\)--\(0.4\)). Clinical utility (H6) has
the smallest evidence base (45 assertions) but shows a temporally
increasing trend, consistent with the recent growth of computational
psychiatry applications. Language AIF (H8) and morphogenesis (H7) both
show moderate support with small contradicting evidence, reflecting
their status as active research frontiers where theoretical proposals
outpace empirical validation.

The \textbf{diffuse or contested tier} (H1, H2, H3) is the most
diagnostically informative for understanding the field's intellectual
maturation. FEP universality (H1), despite generating the largest raw
evidence base (1,370 assertions), achieves a score of only
\(+0.16\)---the vast majority of assessments are strictly neutral,
indicating that researchers frequently \emph{invoke} the FEP
colloquially without explicitly testing its universality claim. AIF
optimality (H2) exhibits the largest volume of contradicting evidence
(19 assertions); crucially, its temporal trend reveals a persistent
decline from an early peak of \(+0.38\) (2012) to its current \(+0.22\).
This downward trajectory suggests that as the field has transitioned
from theory to empirical application, absolute optimality claims have
undergone increasingly stringent critical scrutiny. Markov blanket
realism (H3) remains the most heavily contested hypothesis, exhibiting a
near-zero aggregated score (\(+0.02\)) with six contradicting assertions
effectively neutralizing 14 supporting ones---empirically capturing the
intense, ongoing philosophical debate over whether Markov blankets
denote real thermodynamic boundaries or merely represent instrumental
statistical constructs.
\end{block}

\begin{block}{Temporal Dynamics of Evidence Accumulation}
\protect\phantomsection\label{temporal-dynamics-of-evidence-accumulation}
The cumulative evidence timeline (Figure \ref{fig:evidence_timeline})
reveals three temporal patterns. First, \textbf{early convergence}: H4
(predictive coding) reached positive territory in the late 1970s and has
maintained a stable, high score since, reflecting the mature empirical
base in cognitive neuroscience. Second, \textbf{recent acceleration}: H5
(scalability) and H6 (clinical utility) show steep upward trends after
2017, tracking the emergence of deep active inference tools and
computational psychiatry applications. Third, \textbf{persistent
contestation}: H3 (Markov blanket realism) has oscillated near zero
since 2018, with gains from supporting papers offset by targeted
critiques.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{../figures/evidence_timeline.png}
\caption{Temporal evolution of cumulative evidence scores by hypothesis. Divergent trajectories around the shaded neutral boundary reveal which hypotheses are gaining or losing support over time.}
\label{fig:evidence_timeline}
\end{figure}
\end{block}

\begin{block}{Assertion Composition and Distribution}
\protect\phantomsection\label{assertion-composition-and-distribution}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{../figures/assertion_breakdown.png}
\caption{Per-hypothesis stacked bar chart decomposing assertions into supports, contradicts, and neutral categories. The composition of evidence varies markedly across hypotheses.}
\label{fig:assertion_breakdown}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{../figures/assertion_summary.png}
\caption{Multi-panel assertion summary: total count, type distribution, and per-hypothesis totals. Provides a single-glance overview of the knowledge graph extraction results.}
\label{fig:assertion_summary}
\end{figure}
\end{block}

\begin{block}{Limitations of the Current Scoring Approach}
\protect\phantomsection\label{limitations-of-the-current-scoring-approach}
As noted in Section 2, these results reflect a \textbf{tally-based
aggregation} of independent LLM-extracted assertions, weighted by
citation count and confidence. This approach does not account for
evidential dependencies (e.g., papers from the same group testing the
same model), does not distinguish between empirical and theoretical
evidence, and treats the LLM's confidence scores as calibrated
probabilities. The assertion counts are also sensitive to corpus
composition: H1's large neutral tally (1,071) partially reflects the
keyword classifier's tendency to assign papers to the broad A2
(philosophy) category, where FEP universality is implicitly invoked but
rarely explicitly tested. More sophisticated approaches---including
hierarchical Bayesian models, causal evidence graphs, and evidential
diversity weighting---are discussed as future directions in Section 5.
\end{block}
\end{frame}

\end{document}
