% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
  ignorenonframetext,
]{beamer}
\newif\ifbibliography
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% remove section numbering
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{section title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{subsection title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{longtable,booktabs,array}
\newcounter{none} % for unnumbered tables
\usepackage{calc} % for calculating minipage widths
\usepackage{caption}
% Make caption package work with longtable
\makeatletter
\def\fnum@table{\tablename~\thetable}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{\texorpdfstring{}{}}
\date{}

\begin{document}

\begin{frame}{Text Analytics: Topic Modeling, Vocabulary Structure, and
Document Embeddings \label{sec:text_analytics}}
\protect\phantomsection\label{text-analytics-topic-modeling-vocabulary-structure-and-document-embeddings}
This section examines the latent semantic structure of the Active
Inference corpus through complementary text-analytic methods:
non-negative matrix factorization for topic discovery, TF-IDF vocabulary
analysis, document embedding projections, and term co-occurrence
patterns. Together, these analyses reveal thematic structure that cuts
across the keyword-based domain taxonomy presented in Section 3.

\begin{block}{Topic Modeling: Latent Structure}
\protect\phantomsection\label{topic-modeling-latent-structure}
Non-negative matrix factorization (NMF) applied to the TF-IDF matrix
identifies five latent topics:

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Topic
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Top Terms
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Interpretation
\end{minipage} \\
\midrule\noalign{}
\endhead
0 & learning, agent, model, agents, active, environments, aif,
inference, environment, based & Agent-environment modeling and robotic
applications \\
1 & inference, active, energy, free, variational, control, bayesian,
expected, optimal, principle & Active inference agents and
decision-making \\
2 & states, internal, external, systems, markov, system, dynamics,
information, beliefs, self & Markov blankets and internal/external
states \\
3 & fep, systems, ai, principle, energy, free, theory, networks,
modeling, language & Free energy principle and AI systems \\
4 & predictive, brain, cognitive, prediction, perception, processing,
sensory, models, coding, model & Predictive coding and cognitive
neuroscience \\
\bottomrule\noalign{}
\end{longtable}
}

\begin{block}{Topic--Domain Overlap}
\protect\phantomsection\label{topicdomain-overlap}
These topics are partially orthogonal to the domain taxonomy. Topic 0
(agent-environment modeling) spans tools (B), robotics (C2), and core
theory (A1)---a cross-cutting theme that the keyword classifier cannot
capture. Topic 4 (predictive coding and cognitive neuroscience) aligns
closely with neuroscience (C1) but also draws from core theory. Topic 2
(Markov blankets and states) captures the mathematical core shared
across domains. Topic 3 (FEP and AI systems) reveals the growing
intersection of active inference with mainstream artificial intelligence
research. The absence of retrieval noise (no spurious physics topics)
confirms that the phrase-matched arXiv query effectively filters
irrelevant content.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{../figures/topic_term_bars.png}
\caption{Top 10 terms per NMF topic, revealing the vocabulary structure of each thematic cluster.}
\label{fig:topic_term_bars}
\end{figure}
\end{block}
\end{block}

\begin{block}{Vocabulary Analysis}
\protect\phantomsection\label{vocabulary-analysis}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{../figures/word_cloud.png}
\caption{Word cloud of corpus vocabulary sized by mean TF-IDF weight. Prominent terms—"inference," "active," "free energy," "model"—reflect the field's core theoretical commitments.}
\label{fig:word_cloud}
\end{figure}

The word cloud reveals the conceptual core of the Active Inference
literature: terms related to the Free Energy Principle (``inference,''
``active,'' ``free energy,'' ``model,'' ``bayesian'') dominate, while
application-specific terms appear at smaller scales, reflecting the
domain distribution's heavy A2 concentration.
\end{block}

\begin{block}{Document Embedding Projections}
\protect\phantomsection\label{document-embedding-projections}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{../figures/pca_embeddings.png}
\caption{PCA projection of TF-IDF document embeddings, colored by domain. Loading arrows indicate vocabulary terms contributing most to each principal component.}
\label{fig:pca_embeddings}
\end{figure}

Principal Component Analysis of the TF-IDF document-term matrix projects
each paper into a two-dimensional space that preserves the directions of
maximum variance. The scatter plot, colored by domain assignment,
reveals the degree of semantic separation between domains. Loading
arrows overlay the top-variance terms, showing which vocabulary drives
the principal components and highlighting the partial overlap between
theoretically similar domains.
\end{block}

\begin{block}{Domain Semantic Similarity}
\protect\phantomsection\label{domain-semantic-similarity}
To further interrogate the latent semantic structure of the subfields,
we extract the top characterizing terms for each domain and compute a
hierarchical clustering of domain centroids. The heatmap reveals
distinctive vocabulary patterns beyond mere keyword-level
classification, while the dendrogram confirms the tight semantic
proximity between Core Theory subfields (A1, A2) and the methodological
alignment of Tooling (B) with Robotics (C2).

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{../figures/term_heatmap.png}
\caption{Mean TF-IDF weight for the top 20 terms across domains. Darker cells indicate higher usage, revealing distinctive vocabulary patterns beyond keyword-level classification.}
\label{fig:term_heatmap}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{../figures/dendrogram.png}
\caption{Hierarchical clustering of domain centroids (Ward linkage on mean TF-IDF vectors). A1 (formal theory) and A2 (philosophy) cluster closely, as do C2 (robotics) and B (tools).}
\label{fig:dendrogram}
\end{figure}
\end{block}

\begin{block}{Term Co-occurrence Patterns}
\protect\phantomsection\label{term-co-occurrence-patterns}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{../figures/cooccurrence_matrix.png}
\caption{Term co-occurrence matrix for the 30 most frequent terms. Cell intensity reflects normalized document co-occurrence counts.}
\label{fig:cooccurrence_matrix}
\end{figure}

The co-occurrence matrix for the 30 most frequent corpus terms reveals
tightly coupled term clusters corresponding to the NMF topics. The
strong co-occurrence between ``free,'' ``energy,'' ``principle,'' and
``bayesian'' anchors the theoretical core, while application-specific
term clusters (e.g.,
``brain''--``cognitive''--``predictive''--``coding'') form distinct
off-diagonal blocks. The relative isolation of robotics-specific terms
from neuroscience terms confirms the semantic separation between these
application domains despite their shared theoretical foundation.
\end{block}
\end{frame}

\end{document}
