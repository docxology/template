% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
  ignorenonframetext,
]{beamer}
\newif\ifbibliography
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% remove section numbering
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{section title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{subsection title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{\textbf{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\newcounter{none} % for unnumbered tables
\usepackage{calc} % for calculating minipage widths
\usepackage{caption}
% Make caption package work with longtable
\makeatletter
\def\fnum@table{\tablename~\thetable}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{\texorpdfstring{}{}}
\date{}

\begin{document}

\begin{frame}[fragile]{LLM-Based Assertion Extraction: Prompt Design,
Error Taxonomy, and Validation \label{sec:extraction_pipeline}}
\protect\phantomsection\label{llm-based-assertion-extraction-prompt-design-error-taxonomy-and-validation}
\emph{This supplementary section documents the implementation specifics
of the LLM-based assertion extraction pipeline.}

\begin{block}{Relationship to Prior Approaches}
\protect\phantomsection\label{relationship-to-prior-approaches}
The closest prior effort is the systematic literature analysis of
Knight, Cordes, and Friedman \citep{knight2022fep}, which used human
annotators to manually code structural, visual, and mathematical
features of FEP and Active Inference publications. Their work operated
at the scale of hundreds of annotated papers and employed terms from the
Active Inference Institute's Active Inference Ontology for automated
text analysis. Our pipeline replaces the manual coding step with
LLM-based assertion extraction, enabling scalable processing of the full
corpus (\(N = 1208\) papers) at the cost of exchanging human-verified
precision for machine-generated assessments that require post-hoc
validation.

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2558}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4884}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2558}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Dimension
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Knight et al.~(2022)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
This work
\end{minipage} \\
\midrule\noalign{}
\endhead
\textbf{Scale} & Hundreds of papers & 1208 papers \\
\textbf{Annotation} & Manual (structural/visual/math features) &
Automated (LLM hypothesis assessment) \\
\textbf{Ontology} & Active Inference Ontology terms & 8 standard
hypotheses \\
\textbf{Output} & Annotated features + term frequencies &
Nanopublications + knowledge graph \\
\textbf{Reproducibility} & Annotator-dependent & Deterministic (given
model + seed) \\
\textbf{Precision} & High (human-verified) & Medium (requires
validation) \\
\bottomrule\noalign{}
\end{longtable}
}
\end{block}

\begin{block}{Prompt Engineering and Schema Design}
\protect\phantomsection\label{prompt-engineering-and-schema-design}
The structured prompt is designed to minimize parsing failures and
maximize assessment quality:

\begin{enumerate}
\item
  \textbf{Explicit JSON schema.} The prompt specifies the exact output
  schema---field names, allowed direction values, and the numeric
  confidence range---reducing the LLM's tendency to generate free-form
  text or ad hoc structures.
\item
  \textbf{Hypothesis definitions in-context.} All eight definitions are
  included verbatim, ensuring the LLM assesses relevance from the
  provided context rather than relying on parametric knowledge that may
  be stale.
\item
  \textbf{Reasoning field.} Each assessment includes a natural-language
  reasoning string, providing an audit trail for human reviewers and
  enabling systematic analysis of error patterns.
\item
  \textbf{Irrelevant filtering.} An explicit ``irrelevant'' direction
  allows the LLM to mark hypotheses that a paper does not address,
  avoiding forced spurious assessments.
\end{enumerate}

\begin{block}{Prompt Template}
\protect\phantomsection\label{prompt-template}
The extraction prompt follows a two-part structure (system + user):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SYSTEM: You are a scientific literature analyst specializing in the}
\NormalTok{Free Energy Principle and Active Inference. Assess the relevance of}
\NormalTok{the given paper to each hypothesis. Return a JSON array.}

\NormalTok{USER:}
\NormalTok{Paper: \{title\}}
\NormalTok{Abstract: \{abstract\}}

\NormalTok{Hypotheses:}
\NormalTok{H1: FEP Universality — \{description\}}
\NormalTok{H2: AIF Optimality — \{description\}}
\NormalTok{...}
\NormalTok{H8: Language AIF — \{description\}}

\NormalTok{For each hypothesis, return:}
\NormalTok{\{}
\NormalTok{  "hypothesis\_id": "H1",}
\NormalTok{  "direction": "supports|contradicts|neutral|irrelevant",}
\NormalTok{  "confidence": 0.0{-}1.0,}
\NormalTok{  "reasoning": "..."}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The extraction module (\texttt{src/knowledge\_graph/llm\_extraction.py})
includes configurable retry logic with exponential backoff, JSON parsing
with handling of markdown code fences and extraneous text, confidence
clamping, and validation against the hypothesis ID set. The default
model is \texttt{gemma3:4b} on a local Ollama instance, configurable via
\texttt{-\/-llm-model} and \texttt{-\/-llm-url} flags.
\end{block}
\end{block}

\begin{block}{Failure Modes and Error Recovery}
\protect\phantomsection\label{failure-modes-and-error-recovery}
The primary failure modes are documented below.

\begin{block}{Over-Extraction Bias}
\protect\phantomsection\label{over-extraction-bias}
Approximately 15--20\% of assessments in preliminary experiments exhibit
over-extraction: the LLM attributes claims to a paper that merely
mentions a hypothesis without taking a position. This is the most common
error mode and produces false supporting evidence.
\end{block}

\begin{block}{Direction Misclassification}
\protect\phantomsection\label{direction-misclassification}
The LLM misclassifies a contradicting claim as supporting, or vice
versa. Rarer but more consequential, as it directly inverts the evidence
signal. Most common for papers that discuss limitations while ultimately
endorsing a hypothesis.
\end{block}

\begin{block}{Confidence Calibration Constraints}
\protect\phantomsection\label{confidence-calibration-constraints}
The model occasionally assigns high confidence to assessments where the
underlying semantic evidence is demonstrably weak or ambiguous. Reliable
confidence calibration remains an open research problem across nearly
all zero-shot LLM applications, necessitating the multi-tiered
validation protocols described below.
\end{block}

\begin{block}{Progressive JSON Parsing Recovery}
\protect\phantomsection\label{progressive-json-parsing-recovery}
To mitigate formatting inconsistencies, the module implements a
progressive parsing pipeline to recover malformed LLM outputs:

\begin{enumerate}
\tightlist
\item
  \textbf{Direct parse}: Attempt \texttt{json.loads()} on the raw
  response.
\item
  \textbf{Strip code fences}: Remove Markdown
  \texttt{\textasciigrave{}\textasciigrave{}\textasciigrave{}json\ ...\ \textasciigrave{}\textasciigrave{}\textasciigrave{}}
  wrappers and retry.
\item
  \textbf{Extract JSON array}: Scan for the first \texttt{{[}...{]}}
  substring in the response text.
\item
  \textbf{Individual recovery}: If a valid array contains malformed
  elements, parse each element independently.
\end{enumerate}

Papers that fail all parsing stages are logged and skipped; their count
is reported at pipeline completion.
\end{block}
\end{block}

\begin{block}{Validation Methodology}
\protect\phantomsection\label{validation-methodology}
Validation of LLM-extracted assertions follows a three-tier protocol:

\begin{enumerate}
\item
  \textbf{Spot-check validation.} A random sample of 50 papers is
  reviewed by a domain expert, comparing LLM assessments against human
  judgments for direction accuracy and confidence appropriateness.
\item
  \textbf{Boundary-case audit.} Papers known to make contested claims
  (e.g., critiques of FEP universality, Markov blanket realism debates)
  are specifically checked for correct direction assignment.
\item
  \textbf{Aggregate consistency.} Hypothesis scores are compared against
  qualitative expectations from the literature: hypotheses known to be
  well-supported (e.g., H4 Predictive Coding) should score positively;
  those known to be contested (e.g., H3 Markov Blanket Realism) should
  show lower or mixed scores.
\end{enumerate}

Preliminary experiments on a sampled subset of Active Inference
papers---evaluated across GPT-4 and Claude-family models---suggest that
this automated approach reduces human annotation time by approximately
60--70\% compared to purely manual extraction. Both over-extraction
biases and direction inversion errors are consistently intercepted by
human review at acceptable rates. Structurally, the pipeline is designed
for seamless proprietary or open-weight model upgrades: swapping the
underlying reasoning engine requires only adjusting the
\texttt{-\/-llm-model} flag.
\end{block}
\end{frame}

\end{document}
