<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>03b_text_analytics</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
</head>
<body>
<h1
id="text-analytics-topic-modeling-vocabulary-structure-and-document-embeddings">Text
Analytics: Topic Modeling, Vocabulary Structure, and Document Embeddings
</h1>
<p>This section examines the latent semantic structure of the Active
Inference corpus through complementary text-analytic methods:
non-negative matrix factorization for topic discovery, TF-IDF vocabulary
analysis, document embedding projections, and term co-occurrence
patterns. Together, these analyses reveal thematic structure that cuts
across the keyword-based domain taxonomy presented in Section 3.</p>
<h2 id="topic-modeling-latent-structure">Topic Modeling: Latent
Structure</h2>
<p>Non-negative matrix factorization (NMF) applied to the TF-IDF matrix
identifies five latent topics:</p>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th>Topic</th>
<th>Top Terms</th>
<th>Interpretation</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>learning, agent, model, agents, active, environments, aif,
inference, environment, based</td>
<td>Agent-environment modeling and robotic applications</td>
</tr>
<tr>
<td>1</td>
<td>inference, active, energy, free, variational, control, bayesian,
expected, optimal, principle</td>
<td>Active inference agents and decision-making</td>
</tr>
<tr>
<td>2</td>
<td>states, internal, external, systems, markov, system, dynamics,
information, beliefs, self</td>
<td>Markov blankets and internal/external states</td>
</tr>
<tr>
<td>3</td>
<td>fep, systems, ai, principle, energy, free, theory, networks,
modeling, language</td>
<td>Free energy principle and AI systems</td>
</tr>
<tr>
<td>4</td>
<td>predictive, brain, cognitive, prediction, perception, processing,
sensory, models, coding, model</td>
<td>Predictive coding and cognitive neuroscience</td>
</tr>
</tbody>
</table>
<h3 id="topicdomain-overlap">Topic–Domain Overlap</h3>
<p>These topics are partially orthogonal to the domain taxonomy. Topic 0
(agent-environment modeling) spans tools (B), robotics (C2), and core
theory (A1)—a cross-cutting theme that the keyword classifier cannot
capture. Topic 4 (predictive coding and cognitive neuroscience) aligns
closely with neuroscience (C1) but also draws from core theory. Topic 2
(Markov blankets and states) captures the mathematical core shared
across domains. Topic 3 (FEP and AI systems) reveals the growing
intersection of active inference with mainstream artificial intelligence
research. The absence of retrieval noise (no spurious physics topics)
confirms that the phrase-matched arXiv query effectively filters
irrelevant content.</p>
<h2 id="vocabulary-analysis">Vocabulary Analysis</h2>
<p>The word cloud reveals the conceptual core of the Active Inference
literature: terms related to the Free Energy Principle (“inference,”
“active,” “free energy,” “model,” “bayesian”) dominate, while
application-specific terms appear at smaller scales, reflecting the
domain distribution’s heavy A2 concentration.</p>
<h2 id="document-embedding-projections">Document Embedding
Projections</h2>
<p>Principal Component Analysis of the TF-IDF document-term matrix
projects each paper into a two-dimensional space that preserves the
directions of maximum variance. The scatter plot, colored by domain
assignment, reveals the degree of semantic separation between domains.
Loading arrows overlay the top-variance terms, showing which vocabulary
drives the principal components and highlighting the partial overlap
between theoretically similar domains.</p>
<h2 id="domain-semantic-similarity">Domain Semantic Similarity</h2>
<p>To further interrogate the latent semantic structure of the
subfields, we extract the top characterizing terms for each domain and
compute a hierarchical clustering of domain centroids. The heatmap
reveals distinctive vocabulary patterns beyond mere keyword-level
classification, while the dendrogram confirms the tight semantic
proximity between Core Theory subfields (A1, A2) and the methodological
alignment of Tooling (B) with Robotics (C2).</p>
<h2 id="term-co-occurrence-patterns">Term Co-occurrence Patterns</h2>
<p>The co-occurrence matrix for the 30 most frequent corpus terms
reveals tightly coupled term clusters corresponding to the NMF topics.
The strong co-occurrence between “free,” “energy,” “principle,” and
“bayesian” anchors the theoretical core, while application-specific term
clusters (e.g., “brain”–“cognitive”–“predictive”–“coding”) form distinct
off-diagonal blocks. The relative isolation of robotics-specific terms
from neuroscience terms confirms the semantic separation between these
application domains despite their shared theoretical foundation.</p>
</body>
</html>
