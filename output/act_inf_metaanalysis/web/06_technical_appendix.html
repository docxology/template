<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>06_technical_appendix</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<h1
id="technical-appendix-mathematical-and-algorithmic-details">Technical
Appendix: Mathematical and Algorithmic Details </h1>
<p><em>This appendix collects the formal mathematical definitions,
derivations, and algorithmic specifications referenced from the main
methodology section.</em></p>
<h2 id="a.1-citation-weighted-hypothesis-scoring-formula">A.1
Citation-Weighted Hypothesis Scoring Formula</h2>
<p>For each hypothesis <span class="math inline">\(H\)</span>, we
compute a citation-weighted evidence score aggregating all assertions
relevant to <span class="math inline">\(H\)</span>:</p>
<p><span class="math display">\[
\text{score}(H) = \frac{\sum_{a \in S(H)} w(a) - \sum_{a \in C(H)}
w(a)}{\sum_{a \in A(H)} w(a)}
\]</span></p>
<p>where <span class="math inline">\(S(H)\)</span> is the set of
supporting assertions, <span class="math inline">\(C(H)\)</span> is the
set of contradicting assertions, <span
class="math inline">\(A(H)\)</span> is all assertions for <span
class="math inline">\(H\)</span> (including neutral), and the weight
function is:</p>
<p><span class="math display">\[
w(a) = \log(1 + \text{citations}(a)) \cdot \text{confidence}(a)
\]</span></p>
<p>The logarithmic citation weighting ensures that highly cited papers
carry more influence while preventing any single blockbuster paper from
dominating the score. The score lies in <span class="math inline">\([-1,
1]\)</span>: values near <span class="math inline">\(+1\)</span>
indicate strong supporting evidence, values near <span
class="math inline">\(-1\)</span> indicate strong contradicting
evidence, and values near <span class="math inline">\(0\)</span>
indicate balanced or insufficient evidence.</p>
<p><strong>Temporal aggregation.</strong> We additionally compute
temporal trends by evaluating the cumulative score at each year <span
class="math inline">\(t\)</span>, using only assertions from papers
published in year <span class="math inline">\(\leq t\)</span>:</p>
<p><span class="math display">\[
\text{score}(H, t) = \frac{\sum_{a \in S(H,t)} w(a) - \sum_{a \in
C(H,t)} w(a)}{\sum_{a \in A(H,t)} w(a)}
\]</span></p>
<p>This reveals whether support for a hypothesis is growing, declining,
or plateauing over time.</p>
<h2
id="a.2-non-negative-matrix-factorization-nmf-for-topic-modeling">A.2
Non-negative Matrix Factorization (NMF) for Topic Modeling</h2>
<p>We apply NMF to the TF-IDF matrix of the corpus to discover latent
topics. Given the document-term matrix <span class="math inline">\(V \in
\mathbb{R}^{n \times m}_{\geq 0}\)</span>, NMF finds factor matrices
<span class="math inline">\(W \in \mathbb{R}^{n \times k}_{\geq
0}\)</span> and <span class="math inline">\(H \in \mathbb{R}^{k \times
m}_{\geq 0}\)</span> such that <span class="math inline">\(V \approx
WH\)</span>, where <span class="math inline">\(k\)</span> is the number
of topics.</p>
<p>We use multiplicative update rules :</p>
<p><span class="math display">\[H \leftarrow H \odot \frac{W^T V}{W^T W
H + \epsilon}, \quad W \leftarrow W \odot \frac{V H^T}{W H H^T +
\epsilon}\]</span></p>
<p>with <span class="math inline">\(\epsilon = 10^{-10}\)</span> for
numerical stability and a fixed random seed of 42 for
reproducibility.</p>
<p><strong>Term-Frequency Inverse Document Frequency (TF-IDF).</strong>
The document-term matrix is constructed using TF-IDF weighting . For
term <span class="math inline">\(t\)</span> in document <span
class="math inline">\(d\)</span>:</p>
<p><span class="math display">\[
\text{TF-IDF}(t, d) = \text{tf}(t, d) \cdot
\log\!\left(\frac{N}{\text{df}(t)}\right)
\]</span></p>
<p>where <span class="math inline">\(\text{tf}(t, d)\)</span> is the
term frequency, <span class="math inline">\(N\)</span> is the total
number of documents, and <span
class="math inline">\(\text{df}(t)\)</span> is the document frequency of
term <span class="math inline">\(t\)</span>.</p>
<h2 id="a.3-field-growth-rate-estimation">A.3 Field Growth-Rate
Estimation</h2>
<p>The <strong>mean year-over-year growth rate</strong> <span
class="math inline">\(\bar{g}\)</span> is the arithmetic mean of annual
growth rates computed only for years where the prior year had non-zero
publications:</p>
<p><span class="math display">\[
\bar{g} = \frac{1}{|Y|} \sum_{y \in Y} \frac{n_y - n_{y-1}}{n_{y-1}}
\]</span></p>
<p>where <span class="math inline">\(Y = \{y : n_{y-1} &gt; 0\}\)</span>
and <span class="math inline">\(n_y\)</span> is the number of
publications in year <span class="math inline">\(y\)</span>.</p>
<p>The <strong>doubling time</strong> <span
class="math inline">\(t_d\)</span> is derived from the mean annual
growth rate:</p>
<p><span class="math display">\[
t_d = \frac{\ln 2}{\ln(1 + \bar{g})}
\]</span></p>
<p>The <strong>compound annual growth rate</strong> (CAGR) over the full
span <span class="math inline">\([y_0, y_T]\)</span> is:</p>
<p><span class="math display">\[
\text{CAGR} =
\left(\frac{n_{\text{cumulative}}(y_T)}{n_{\text{cumulative}}(y_0)}\right)^{1/(y_T
- y_0)} - 1
\]</span></p>
<p>For the current corpus, CAGR <span class="math inline">\(=
6.63\%\)</span>. The more recent growth phase (2010â€“2025) exhibits
substantially higher annualized growth.</p>
<h2 id="a.4-advanced-visualization-methods">A.4 Advanced Visualization
Methods</h2>
<h3 id="pca-of-tf-idf-embeddings">PCA of TF-IDF Embeddings</h3>
<p>Principal Component Analysis (PCA) is applied to the TF-IDF matrix
<span class="math inline">\(V\)</span> to project each document into a
2-D space. The projection preserves the directions of maximum variance,
enabling visual inspection of document clustering by domain. Loading
arrows overlay the top-variance terms onto the scatter plot, showing
which vocabulary drives the principal components.</p>
<h3 id="hierarchical-clustering-dendrogram">Hierarchical Clustering
Dendrogram</h3>
<p>For each domain <span class="math inline">\(s\)</span>, we compute
the centroid <span class="math inline">\(\bar{v}_s = \frac{1}{|D_s|}
\sum_{d \in D_s} v_d\)</span> where <span
class="math inline">\(D_s\)</span> is the set of documents in domain
<span class="math inline">\(s\)</span> and <span
class="math inline">\(v_d\)</span> is the TF-IDF vector of document
<span class="math inline">\(d\)</span>. Ward linkage is applied to the
centroid matrix to produce a hierarchical clustering dendrogram showing
semantic proximity between domains.</p>
<h3 id="term-heatmap">Term Heatmap</h3>
<p>For each domain <span class="math inline">\(s\)</span> and term <span
class="math inline">\(t\)</span>, we compute the mean TF-IDF weight
<span class="math inline">\(\bar{w}_{s,t} = \frac{1}{|D_s|} \sum_{d \in
D_s} \text{TF-IDF}(t, d)\)</span>. The heatmap displays <span
class="math inline">\(\bar{w}_{s,t}\)</span> for the top-<span
class="math inline">\(k\)</span> terms (by global document frequency)
across all domains, with cell intensity proportional to mean weight.
This reveals distinctive vocabulary patterns that differentiate domains
beyond the keyword-level classification used for subfield
assignment.</p>
<h3 id="term-co-occurrence-matrix">Term Co-occurrence Matrix</h3>
<p>The co-occurrence matrix <span class="math inline">\(C \in
\mathbb{R}^{k \times k}\)</span> counts the number of documents in which
two terms appear together. For top-<span
class="math inline">\(k\)</span> terms by document frequency, <span
class="math inline">\(C_{ij} = |\{d : t_i \in d \land t_j \in
d\}|\)</span>. The matrix is normalized to <span
class="math inline">\([0, 1]\)</span> by dividing by the maximum entry
and visualized as a symmetric heatmap.</p>
</body>
</html>
