<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>05_conclusion</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<h1
id="conclusion-evidence-landscape-methodological-limitations-and-research-agenda">Conclusion:
Evidence Landscape, Methodological Limitations, and Research Agenda
</h1>
<h2 id="summary">Summary</h2>
<p>This work demonstrates that the infrastructure for computational
meta-analysis of a rapidly growing scientific field is feasible with
current technology. By combining multi-source retrieval (<span
class="math inline">\(N = 1208\)</span> papers from three databases),
LLM-based assertion extraction encoded as nanopublications, and
citation-weighted hypothesis scoring, we produce a queryable,
RDF-compatible knowledge graph that tracks the evolving evidence for
eight core Active Inference claims.</p>
<h2 id="constraints-and-methodological-scope">Constraints and
Methodological Scope</h2>
<p>Several conscious design constraints scope these findings.</p>
<h3 id="keyword-classifier-resolution">Keyword Classifier
Resolution</h3>
<p>The keyword-based classifier utilizes a deterministic priority system
that strategically routes papers to specific application domains (C1–C5)
before testing tools (B), formal theory (A1), and the qualitative
philosophy catch-all (A2). While the expanded A1 keyword set (65+
mathematical indicators) and word-boundary-aware matching substantially
suppress misclassification of formal papers into A2, keyword-based
taxonomic gating inherently lacks the granular semantic depth of latent
embedding-based approaches. Residual A2 concentration must therefore be
interpreted structurally—as a ceiling on broad theoretical generality
rather than a literal measure of exclusive philosophical focus.</p>
<h3 id="citation-network-coverage-gaps">Citation Network Coverage
Gaps</h3>
<p>The 2{,}780 intra-corpus edges spanning 700 distinct connected
components provide a meaningful topological skeleton, yet cross-source
identifier mismatches inevitably inflate the isolated component count.
Exhaustive DOI-level cross-matching would further condense the
graph.</p>
<h3 id="temporal-and-citation-count-biases">Temporal and Citation-Count
Biases</h3>
<p>Citation counts remain fundamentally subject to Matthew effects and
cumulative field-size biases. Partial-year indexing for the most recent
calendar year predictably undercounts concluding publications.
Consequently, the measured 6.63% CAGR explicitly reflects the dilutive
effect of the extensive longitudinal span (1972–2026); the localized
growth phase from 2010 onward traverses an aggressively steeper
trajectory.</p>
<h3 id="llm-extraction-fidelity">LLM Extraction Fidelity</h3>
<p>Systematic zero-shot extraction biases include over-extraction
(hallucinating claims the paper merely mentions) and direction inversion
errors (misclassifying opposing evidence as structurally supporting).
While human review and the explicit “irrelevant” filtering predicate
mitigate these hazards, they are not eliminated. Zero-shot confidence
calibration remains arguably the central open challenge for automated
evidence synthesis architectures.</p>
<h2
id="future-directions-beyond-tally-based-evidence-aggregation">Future
Directions: Beyond Tally-Based Evidence Aggregation</h2>
<p>The current scoring formula (Section 2) aggregates LLM-extracted
assertions through a simple citation-weighted tally. While this approach
provides a transparent and reproducible baseline, it leaves substantial
room for methodological sophistication. We identify six directions,
ordered by expected impact, with the first three specifically addressing
the limitations of tally-based evidence synthesis.</p>
<h3 id="hierarchical-bayesian-hypothesis-scoring">Hierarchical Bayesian
Hypothesis Scoring</h3>
<p>The most direct extension replaces the additive tally with a
<strong>hierarchical Bayesian model</strong> that treats each hypothesis
score as a latent variable inferred from noisy assertion observations.
Under this formulation, each assertion <span
class="math inline">\(a_i\)</span> contributes a likelihood term <span
class="math inline">\(P(a_i | \theta_H, \sigma)\)</span> parameterized
by the hypothesis-level evidence strength <span
class="math inline">\(\theta_H\)</span> and an observation noise term
<span class="math inline">\(\sigma\)</span> capturing LLM extraction
uncertainty. A hierarchical prior <span class="math inline">\(\theta_H
\sim \mathcal{N}(\mu_{\text{field}}, \tau^2)\)</span> pools information
across hypotheses, enabling principled shrinkage for hypotheses with
sparse evidence (e.g., H6 Clinical Utility, with only 45 assertions).
This framework naturally produces posterior credible intervals rather
than point estimates, providing honest uncertainty quantification that
the current tally-based scores cannot offer. Temporal dynamics can be
modeled through time-varying parameters <span
class="math inline">\(\theta_H(t)\)</span> using state-space
formulations that re-weight older evidence rather than treating all
cumulative assertions equally.</p>
<h3 id="causal-evidence-graphs">Causal Evidence Graphs</h3>
<p>A second-generation knowledge graph would encode not only
assertion-level relationships (paper → supports → hypothesis) but also
<strong>causal dependencies among hypotheses</strong> themselves. For
example, evidence for predictive coding (H4) often implicitly supports
FEP universality (H1), yet the tally-based approach treats them as
independent. A causal evidence graph—structured as a directed acyclic
graph (DAG) over hypotheses with edge weights learned from co-assertion
patterns—would enable cross-hypothesis evidence propagation using belief
propagation or variational message passing. This is particularly
relevant for the Active Inference literature, where hypotheses are
theoretically nested: FEP universality (H1) logically entails predictive
coding (H4), and Markov blanket realism (H3) is a prerequisite for
certain formulations of H1. Encoding these dependencies would prevent
the double-counting of evidence from papers that support multiple
related hypotheses and enable identification of which specific claims
drive support for downstream hypotheses. The resulting causal structure
itself would be a scientific contribution—a formal map of evidential
dependencies within the field’s theoretical architecture.</p>
<h3 id="evidential-diversity-and-source-weighting">Evidential Diversity
and Source Weighting</h3>
<p>The current formula weights assertions by <span
class="math inline">\(\log(1 + \text{citations}) \cdot
\text{confidence}\)</span>, treating all assertion sources
symmetrically. A more nuanced approach would introduce an
<strong>evidential diversity index</strong> that downweights correlated
evidence from papers sharing authors, institutions, or methodological
approaches. Concretely, assertions could be weighted by the inverse of
their similarity to previously counted assertions, measured via cosine
similarity of paper embeddings. This would address the observation that
H1 (FEP universality) accumulates a large neutral tally partly because
many A2 (philosophy) papers invoke the FEP without independently testing
it—a form of evidential redundancy that inflates the evidence base
without adding independent information. Additionally, assertions could
be stratified by evidence type (empirical, theoretical, review) with
configurable type-specific weights, enabling users to compute evidence
scores that privilege experimental results over theoretical
commentary.</p>
<h3 id="additional-directions">Additional Directions</h3>
<ol type="1">
<li><p><strong>Confidence calibration.</strong> A pilot study comparing
LLM-generated assertions with domain expert assessments would establish
inter-annotator agreement (<span class="math inline">\(\kappa\)</span>)
and identify systematic biases. This is the prerequisite for all
downstream improvements.</p></li>
<li><p><strong>Agentic LLM Extractors.</strong> Drawing on recent work
demonstrating LLMs as adaptive active inference agents , replacing
static prompt templates with goal-directed, actor-critic LLM
architectures could significantly solve prevailing confidence
calibration challenges.</p></li>
<li><p><strong>Domain adaptation.</strong> The framework is
domain-agnostic by design. Adaptation to foundation models, quantum
computing, or synthetic biology requires only domain-specific hypothesis
definitions and keyword lists within the A/B/C taxonomy.</p></li>
</ol>
<h2 id="broader-impact">Broader Impact</h2>
<p>The vision motivating this work is straightforward: a living
literature review—a continuously updated knowledge graph tracking what a
field claims, what evidence supports those claims, and where the
frontiers of understanding lie. This vision builds on the foundation
established by Knight et al. , who identified the development of systems
that could “encompass increased scope of relevant works,” “integrate
multiple forms of annotation and participation,” and “facilitate
integration of manual and artificial contributions” as key goals for the
field.</p>
<p>By demonstrating that LLM-driven assertion extraction can produce
scalable, queryable representations of scientific evidence—processing
<span class="math inline">\(N = 1208\)</span> papers spanning nearly
five decades (1972–2026), extracting structured semantic assertions, and
systematically evaluating 8 core hypotheses—this work provides a robust
computational machinery for realizing this vision. The generated
citation network metrics (2{,}780 edges, a density of 0.19%, and an
average in-degree of 2.3) quantify the rapid expansion of the active
inference ecosystem, which has grown to a 6.63% CAGR while diversifying
across 5 major application domains.</p>
<p>Crucially, the inherent limitations of keyword-based retrieval across
disjoint academic repositories dictate that any retrieved corpus will
contain both false positives and false negatives. There is no single
methodological threshold capable of perfectly defining inclusion or
exclusion for a dynamic, interdisciplinary research field. Therefore,
the primary contribution of this work is not simply a definitive “golden
list” of papers. Rather, it is an open-source, modularly updatable, and
versioned software package. This tool is built in reference to custom
literature bibliographies that can be iteratively curated for relevance
through time by the community.</p>
<p>The combination of multi-source retrieval, LLM-based extraction, and
probabilistic knowledge graph construction provides a reusable template
that advances each of these goals. As LLM capabilities improve and
standardized metadata adoption grows, the cost of maintaining such
systems will decrease while their utility increases. By open-sourcing
the pipeline and publishing the schema, we provide both a concrete tool
for the Active Inference community and a modular blueprint that other
fields can adapt and refine.</p>
<p>Community recommendations, actionable implications, and open
questions arising from this work are detailed in the Discussion (see
).</p>
</body>
</html>
