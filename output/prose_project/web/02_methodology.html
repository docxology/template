<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>02_methodology</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<h1 id="methodology">Methodology</h1>
<p>This section presents the methodological approach used in this
research project, demonstrating various mathematical concepts and
notation.</p>
<h2 id="mathematical-framework">Mathematical Framework</h2>
<p>We establish a rigorous mathematical foundation for our analysis.
Consider the general optimization problem:</p>
<p><span class="math display">\[\begin{equation}
\min_{x \in \mathbb{R}^n} f(x)
\label{eq:optimization_problem}
\end{equation}\]</span></p>
<p>subject to the inequality constraints:</p>
<p><span class="math display">\[\begin{equation}
g_i(x) \leq 0, \quad i = 1, \dots, m
\label{eq:inequality_constraints}
\end{equation}\]</span></p>
<p>and equality constraints:</p>
<p><span class="math display">\[\begin{equation}
h_j(x) = 0, \quad j = 1, \dots, p
\label{eq:equality_constraints}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(f: \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> denotes the objective function, and <span
class="math inline">\(g_i, h_j: \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> represent the constraint functions.</p>
<h3 id="fundamental-mathematical-concepts">Fundamental Mathematical
Concepts</h3>
<p>The derivative of a composite function follows the chain rule :</p>
<p><span class="math display">\[\begin{equation}
\frac{d}{dx} [f(g(x))] = f&#39;(g(x)) \cdot g&#39;(x)
\label{eq:chain_rule}
\end{equation}\]</span></p>
<p>For multivariable functions, the gradient is defined as the vector of
partial derivatives:</p>
<p><span class="math display">\[\begin{equation}
\nabla f(x) = \begin{pmatrix}
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_n}
\end{pmatrix}
\label{eq:gradient_definition}
\end{equation}\]</span></p>
<p>The directional derivative in direction <span
class="math inline">\(d\)</span> is given by:</p>
<p><span class="math display">\[\begin{equation}
D_d f(x) = \nabla f(x) \cdot d
\label{eq:directional_derivative}
\end{equation}\]</span></p>
<h3 id="matrix-operations-and-linear-algebra">Matrix Operations and
Linear Algebra</h3>
<p>Matrix multiplication follows the standard row-column rule:</p>
<p><span class="math display">\[\begin{equation}
(AB)_{ij} = \sum_{k=1}^{n} A_{ik} B_{kj}
\label{eq:matrix_multiplication}
\end{equation}\]</span></p>
<p>The determinant of a 2×2 matrix is computed as:</p>
<p><span class="math display">\[\begin{equation}
\det\begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} = ad - bc
\label{eq:determinant_2x2}
\end{equation}\]</span></p>
<p>For a general square matrix <span class="math inline">\(A\)</span>,
the matrix inverse satisfies:</p>
<p><span class="math display">\[\begin{equation}
A A^{-1} = A^{-1} A = I
\label{eq:matrix_inverse}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(I\)</span> denotes the identity
matrix.</p>
<h3 id="series-and-limits">Series and Limits</h3>
<p>The Taylor series expansion around <span
class="math inline">\(x_0\)</span> provides a polynomial
approximation:</p>
<p><span class="math display">\[\begin{equation}
f(x) = f(x_0) + f&#39;(x_0)(x - x_0) + \frac{f&#39;&#39;(x_0)}{2!}(x -
x_0)^2 + \frac{f&#39;&#39;&#39;(x_0)}{3!}(x - x_0)^3 + \cdots
\label{eq:taylor_series}
\end{equation}\]</span></p>
<p>The fundamental limit relationship connects differentiation and
integration through the Fundamental Theorem of Calculus :</p>
<p><span class="math display">\[\begin{equation}
\frac{d}{dx} \int_a^x f(t) \, dt = f(x)
\label{eq:fundamental_theorem_calculus}
\end{equation}\]</span></p>
<p>For definite integrals, we have:</p>
<p><span class="math display">\[\begin{equation}
\int_a^b f(x) \, dx = F(b) - F(a)
\label{eq:definite_integral}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(F\)</span> is the antiderivative of
<span class="math inline">\(f\)</span>.</p>
<h3 id="probability-and-statistics">Probability and Statistics</h3>
<p>The normal (Gaussian) distribution is characterized by its
probability density function:</p>
<p><span class="math display">\[\begin{equation}
\phi(x) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{(x -
\mu)^2}{2\sigma^2}\right)
\label{eq:normal_pdf}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mu\)</span> is the mean and <span
class="math inline">\(\sigma^2\)</span> is the variance.</p>
<p>For large samples, the central limit theorem establishes asymptotic
normality of sample means:</p>
<p><span class="math display">\[\begin{equation}
\sqrt{n} \left( \bar{X}_n - \mu \right) \xrightarrow{d} \mathcal{N}(0,
\sigma^2)
\label{eq:central_limit_theorem}
\end{equation}\]</span></p>
<p>The expected value and variance of a random variable <span
class="math inline">\(X\)</span> are defined as:</p>
<p><span class="math display">\[\begin{equation}
\mathbb{E}[X] = \int_{-\infty}^{\infty} x f_X(x) \, dx
\label{eq:expectation}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\mathbb{V}[X] = \mathbb{E}[(X - \mathbb{E}[X])^2] = \mathbb{E}[X^2] -
(\mathbb{E}[X])^2
\label{eq:variance}
\end{equation}\]</span></p>
<h3 id="advanced-calculus-theorems">Advanced Calculus Theorems</h3>
<p><strong>Theorem 1 (Mean Value Theorem).</strong> If <span
class="math inline">\(f\)</span> is continuous on <span
class="math inline">\([a,b]\)</span> and differentiable on <span
class="math inline">\((a,b)\)</span>, then there exists <span
class="math inline">\(c \in (a,b)\)</span> such that:</p>
<p><span class="math display">\[\begin{equation}
f&#39;(c) = \frac{f(b) - f(a)}{b - a}
\label{eq:mean_value_theorem}
\end{equation}\]</span></p>
<p><strong>Theorem 2 (Integration by Parts).</strong> For differentiable
functions <span class="math inline">\(u\)</span> and <span
class="math inline">\(v\)</span>:</p>
<p><span class="math display">\[\begin{equation}
\int u \, dv = uv - \int v \, du
\label{eq:integration_by_parts}
\end{equation}\]</span></p>
<p><strong>Theorem 3 (Taylor’s Theorem with Remainder).</strong> If
<span class="math inline">\(f\)</span> has <span
class="math inline">\(n+1\)</span> continuous derivatives on <span
class="math inline">\([a, x]\)</span>, then (generalizing <span
class="math inline">\(\eqref{eq:taylor_series}\)</span>):</p>
<p><span class="math display">\[\begin{equation}
f(x) = f(a) + f&#39;(a)(x-a) + \frac{f&#39;&#39;(a)}{2!}(x-a)^2 + \cdots
+ \frac{f^{(n)}(a)}{n!}(x-a)^n + R_n(x)
\label{eq:taylors_theorem}
\end{equation}\]</span></p>
<p>where the remainder term <span class="math inline">\(R_n(x) =
\frac{f^{(n+1)}(c)}{(n+1)!}(x-a)^{n+1}\)</span> for some <span
class="math inline">\(c \in (a,x)\)</span>.</p>
<h2 id="algorithm-development">Algorithm Development</h2>
<p>The proposed optimization algorithm follows a systematic approach
with the following steps:</p>
<strong>Algorithm 1: Gradient-Based Optimization</strong>
<p>The line search procedure ensures sufficient decrease in the
objective function:</p>
<p><span class="math display">\[\begin{equation}
f(x_k + \alpha_k d_k) \leq f(x_k) + c_1 \alpha_k \nabla f(x_k)^T d_k
\label{eq:armijo_condition}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(c_1 \in (0,1)\)</span> is a
constant controlling the required decrease.</p>
<h2 id="convergence-analysis">Convergence Analysis</h2>
<p>The algorithm’s convergence properties are established through
rigorous mathematical analysis.</p>
<p><strong>Theorem 4 (Global Convergence).</strong> If <span
class="math inline">\(f\)</span> is convex and continuously
differentiable, and the step sizes satisfy the Wolfe conditions <span
class="math inline">\(\eqref{eq:armijo_condition}\)</span>, then the
algorithm converges to a stationary point, i.e., <span
class="math inline">\(\lim_{k \to \infty} \nabla f(x_k) =
0\)</span>.</p>
<strong>Proof sketch:</strong>
<p><strong>Theorem 5 (Local Convergence Rate).</strong> If <span
class="math inline">\(f\)</span> is strongly convex with parameter <span
class="math inline">\(m &gt; 0\)</span> and the step sizes are constant
and appropriate, then the algorithm converges linearly:</p>
<p><span class="math display">\[\begin{equation}
\|x_{k+1} - x^*\| \leq \rho \|x_k - x^*\|
\label{eq:linear_convergence}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\rho = \max\{|\lambda_i - \alpha|,
|\lambda_i + \alpha|\} &lt; 1\)</span> for eigenvalues <span
class="math inline">\(\lambda_i\)</span> of the Hessian, and <span
class="math inline">\(x^*\)</span> is the unique minimizer.</p>
<h2 id="implementation-considerations">Implementation
Considerations</h2>
<p>Key implementation aspects include:</p>
<ul>
<li><strong>Numerical stability</strong>: Using appropriate
floating-point precision</li>
<li><strong>Termination criteria</strong>: Multiple stopping
conditions</li>
<li><strong>Performance optimization</strong>: Efficient gradient
computation</li>
<li><strong>Error handling</strong>: Robust exception management</li>
</ul>
<h2 id="latex-customization-and-rendering">LaTeX Customization and
Rendering</h2>
<p>The research template supports advanced LaTeX customization through
optional preamble configuration. An optional <code>preamble.md</code>
file can contain custom LaTeX packages and commands that are
automatically inserted before document compilation. The rendering system
ensures required packages (such as <code>graphicx</code> for figure
inclusion) are loaded automatically, while allowing researchers to add
specialized packages for mathematical notation, bibliography styles, or
document formatting. LaTeX code blocks in the preamble file are
extracted and integrated into the document compilation process.</p>
<h2 id="validation-strategy">Validation Strategy</h2>
<p>The methodology is validated through:</p>
<ul>
<li><strong>Mathematical correctness</strong>: Verification of
derivations</li>
<li><strong>Numerical accuracy</strong>: Comparison with known
solutions</li>
<li><strong>Computational efficiency</strong>: Performance
benchmarking</li>
<li><strong>Robustness testing</strong>: Edge case analysis</li>
</ul>
<p>This approach ensures both theoretical soundness and practical
applicability of the proposed methods.</p>
</body>
</html>
