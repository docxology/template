<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>S01_supplemental_methods</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<h1 id="sec:supplemental_methods">Supplemental Methods</h1>
<p>This section provides detailed methodological information
supplementing Section <span
class="math inline">\(\ref{sec:methodology}\)</span>, focusing on the
computational implementation of Ento-Linguistic analysis.</p>
<h2 id="s1.1-text-processing-pipeline-implementation">S1.1 Text
Processing Pipeline Implementation</h2>
<h3 id="s1.1.1-multi-stage-text-normalization">S1.1.1 Multi-Stage Text
Normalization</h3>
<p>Our text processing pipeline implements systematic normalization to
ensure reliable pattern detection:</p>
<p><span
class="math display">\[\begin{equation}\label{eq:text_normalization}
T_{\text{normalized}} =
\text{lowercase}(\text{strip_punct}(\text{unicode_normalize}(T)))
\end{equation}\]</span></p>
<p>where <span class="math inline">\(T\)</span> represents raw text
input and each transformation step standardizes linguistic variation
while preserving semantic content.</p>
<p><strong>Tokenization Strategy</strong>: We employ domain-aware
tokenization that recognizes scientific terminology:</p>
<p><span
class="math display">\[\begin{equation}\label{eq:domain_tokenization}
\tau(T) = \bigcup_{t \in T} \left\{\begin{array}{ll}
t &amp; \text{if } t \in \mathcal{T}_{\text{scientific}} \\
\text{word\_tokenize}(t) &amp; \text{otherwise}
\end{array}\right.
\end{equation}\]</span></p>
<p>where <span
class="math inline">\(\mathcal{T}_{\text{scientific}}\)</span> contains
curated scientific terminology that should not be further
subdivided.</p>
<h3 id="s1.1.2-linguistic-preprocessing-pipeline">S1.1.2 Linguistic
Preprocessing Pipeline</h3>
<p>The complete preprocessing pipeline includes:</p>
<ol type="1">
<li><strong>Unicode Normalization</strong>: Standardizing character
encodings</li>
<li><strong>Case Folding</strong>: Converting to lowercase for
consistency</li>
<li><strong>Punctuation Handling</strong>: Removing or preserving
scientific notation</li>
<li><strong>Number Normalization</strong>: Standardizing numerical
expressions</li>
<li><strong>Stop Word Filtering</strong>: Domain-aware removal of
non-informative terms</li>
<li><strong>Lemmatization</strong>: Reducing words to base forms using
scientific dictionaries</li>
</ol>
<h2 id="s1.2-terminology-extraction-algorithms">S1.2 Terminology
Extraction Algorithms</h2>
<h3 id="s1.2.1-domain-specific-term-identification">S1.2.1
Domain-Specific Term Identification</h3>
<p>Terminology extraction uses a multi-criteria approach combining
statistical and linguistic features:</p>
<p><span
class="math display">\[\begin{equation}\label{eq:term_extraction_score}
S(t) = \alpha \cdot \text{TF-IDF}(t) + \beta \cdot
\text{domain_relevance}(t) + \gamma \cdot \text{linguistic_features}(t)
\end{equation}\]</span></p>
<p>where weights <span class="math inline">\(\alpha, \beta,
\gamma\)</span> are calibrated for each Ento-Linguistic domain.</p>
<p><strong>Domain Relevance Scoring</strong>: Terms are scored for
relevance to specific domains using:</p>
<ul>
<li><strong>Co-occurrence Patterns</strong>: Terms frequently appearing
with domain indicators</li>
<li><strong>Semantic Similarity</strong>: Vector similarity to domain
seed terms</li>
<li><strong>Contextual Features</strong>: Syntactic patterns
characteristic of domain usage</li>
</ul>
<h3 id="s1.2.2-ambiguity-detection-framework">S1.2.2 Ambiguity Detection
Framework</h3>
<p>Ambiguity detection identifies terms with context-dependent
meanings:</p>
<p><span
class="math display">\[\begin{equation}\label{eq:ambiguity_score}
A(t) = \frac{H(\text{contexts}(t))}{\log |\text{contexts}(t)|} \cdot
\frac{|\text{meanings}(t)|}{\text{frequency}(t)}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(H(\text{contexts}(t))\)</span> is
the entropy of contextual usage patterns, measuring dispersion across
different research contexts.</p>
<h2 id="s1.3-network-construction-and-analysis">S1.3 Network
Construction and Analysis</h2>
<h3 id="s1.3.1-edge-weight-calculation">S1.3.1 Edge Weight
Calculation</h3>
<p>Network edges are weighted using multiple co-occurrence measures:</p>
<p><span
class="math display">\[\begin{equation}\label{eq:edge_weight_computation}
w(u,v) = \frac{1}{3} \left[
\frac{\text{co-occurrence}(u,v)}{\max(\text{freq}(u), \text{freq}(v))} +
\text{Jaccard}(u,v) + \text{cosine}(\vec{u}, \vec{v}) \right]
\end{equation}\]</span></p>
<p>where co-occurrence is measured within sliding windows, Jaccard
similarity captures set overlap, and cosine similarity measures semantic
relatedness.</p>
<h3 id="s1.3.2-community-detection-algorithms">S1.3.2 Community
Detection Algorithms</h3>
<p>We implement multiple community detection approaches:</p>
<p><strong>Modularity Optimization</strong>: <span
class="math display">\[\begin{equation}\label{eq:modularity}
Q = \frac{1}{2m} \sum_{ij} \left[ A_{ij} - \frac{k_i k_j}{2m} \right]
\delta(c_i, c_j)
\end{equation}\]</span></p>
<p><strong>Domain-Aware Clustering</strong>: Communities are constrained
to respect Ento-Linguistic domain boundaries while allowing cross-domain
bridging terms.</p>
<h3 id="s1.3.3-network-validation-metrics">S1.3.3 Network Validation
Metrics</h3>
<p>Network quality is assessed using:</p>
<p><span
class="math display">\[\begin{equation}\label{eq:network_validation}
V(G) = \alpha \cdot \text{modularity}(G) + \beta \cdot
\text{conductance}(G) + \gamma \cdot \text{domain_purity}(G)
\end{equation}\]</span></p>
<p>where domain purity measures the extent to which communities
correspond to Ento-Linguistic domains.</p>
<h2 id="s1.4-framing-analysis-implementation">S1.4 Framing Analysis
Implementation</h2>
<h3 id="s1.4.1-anthropomorphic-framing-detection">S1.4.1 Anthropomorphic
Framing Detection</h3>
<p>Anthropomorphic language is detected through:</p>
<p><strong>Lexical Indicators</strong>: Terms suggesting human-like
agency or intentionality <strong>Syntactic Patterns</strong>: Sentence
structures implying human-like behavior <strong>Semantic
Fields</strong>: Clusters of terms drawing from human social domains</p>
<p><strong>Detection Algorithm</strong>: <span
class="math display">\[\begin{equation}\label{eq:anthropomorphic_score}
A_{\text{anthro}}(t) = \sum_{f \in F_{\text{human}}}
\text{similarity}(t, f) \cdot w_f
\end{equation}\]</span></p>
<p>where <span class="math inline">\(F_{\text{human}}\)</span> contains
human social concept features and <span
class="math inline">\(w_f\)</span> are calibrated weights.</p>
<h3 id="s1.4.2-hierarchical-framing-analysis">S1.4.2 Hierarchical
Framing Analysis</h3>
<p>Hierarchical structures are identified by:</p>
<p><strong>Term Relationship Patterns</strong>: Chains of subordination
(superior → subordinate) <strong>Power Dynamic Indicators</strong>:
Terms implying authority, control, or submission <strong>Organizational
Metaphors</strong>: Language drawing from human institutional
structures</p>
<h2 id="s1.5-validation-framework-implementation">S1.5 Validation
Framework Implementation</h2>
<h3 id="s1.5.1-computational-validation-procedures">S1.5.1 Computational
Validation Procedures</h3>
<p><strong>Terminology Extraction Validation</strong>: -
<strong>Precision</strong>: Manual verification of extracted terms
against expert-curated lists - <strong>Recall</strong>: Coverage
assessment against comprehensive domain glossaries - <strong>Domain
Accuracy</strong>: Correct classification into Ento-Linguistic
domains</p>
<p><strong>Network Validation</strong>: - <strong>Structural
Validity</strong>: Comparison against null models - <strong>Domain
Correspondence</strong>: Alignment with theoretical domain boundaries -
<strong>Stability Analysis</strong>: Consistency across subsampling
procedures</p>
<h3 id="s1.5.2-theoretical-validation-methods">S1.5.2 Theoretical
Validation Methods</h3>
<p><strong>Inter-coder Agreement</strong>: Multiple researchers code
ambiguous passages to assess consistency.</p>
<p><strong>Theoretical Saturation</strong>: Iterative analysis until
theoretical categories are fully developed.</p>
<p><strong>Member Checking</strong>: Expert review of interpretations
and categorizations.</p>
<h2 id="s1.6-implementation-architecture">S1.6 Implementation
Architecture</h2>
<h3 id="s1.6.1-modular-software-design">S1.6.1 Modular Software
Design</h3>
<p>The implementation follows a modular architecture:</p>
<pre><code>entolinguistic/
├── text_processing/     # Text normalization and tokenization
├── terminology/         # Term extraction and classification
├── networks/           # Graph construction and analysis
├── framing/            # Framing analysis algorithms
├── validation/         # Validation and quality assurance
└── visualization/      # Result visualization</code></pre>
<h3 id="s1.6.2-data-structures-and-formats">S1.6.2 Data Structures and
Formats</h3>
<p><strong>Terminology Database</strong>:</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TerminologyEntry:</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    term: <span class="bu">str</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    domains: List[<span class="bu">str</span>]</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    contexts: List[<span class="bu">str</span>]</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    frequencies: Dict[<span class="bu">str</span>, <span class="bu">int</span>]</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    ambiguities: List[<span class="bu">str</span>]</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    framings: List[<span class="bu">str</span>]</span></code></pre></div>
<p><strong>Network Representation</strong>:</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TerminologyNetwork:</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    nodes: Dict[<span class="bu">str</span>, TerminologyEntry]</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    edges: Dict[Tuple[<span class="bu">str</span>, <span class="bu">str</span>], <span class="bu">float</span>]</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    communities: Dict[<span class="bu">str</span>, List[<span class="bu">str</span>]]</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    domain_mappings: Dict[<span class="bu">str</span>, <span class="bu">str</span>]</span></code></pre></div>
<h3 id="s1.6.3-performance-optimization">S1.6.3 Performance
Optimization</h3>
<p><strong>Scalability Considerations</strong>: - Streaming processing
for large corpora - Incremental network updates - Parallel processing
for independent analyses - Memory-efficient data structures for large
networks</p>
<p><strong>Computational Complexity</strong>: <span
class="math display">\[\begin{equation}\label{eq:method_complexity}
C(n,m,d) = O(n \log n + m \cdot d + e \cdot \log e)
\end{equation}\]</span></p>
<p>where <span class="math inline">\(n\)</span> is corpus size, <span
class="math inline">\(m\)</span> is extracted terms, <span
class="math inline">\(d\)</span> is domains, and <span
class="math inline">\(e\)</span> is network edges.</p>
<h2 id="s1.7-parameter-calibration-and-sensitivity">S1.7 Parameter
Calibration and Sensitivity</h2>
<h3 id="s1.7.1-algorithm-parameters">S1.7.1 Algorithm Parameters</h3>
<p>Critical parameters and their calibration:</p>
<h3 id="s1.7.2-sensitivity-analysis-results">S1.7.2 Sensitivity Analysis
Results</h3>
<p>Parameter sensitivity testing revealed:</p>
<p><strong>Window Size</strong>: Optimal at 50 words; smaller windows
miss long-range relationships, larger windows introduce noise.</p>
<p><strong>Similarity Threshold</strong>: 0.3 provides balance between
precision and recall; lower values increase false positives, higher
values miss subtle relationships.</p>
<p><strong>Frequency Threshold</strong>: 5 occurrences ensures
statistical reliability while maintaining coverage.</p>
<h2 id="s1.8-quality-assurance-and-reproducibility">S1.8 Quality
Assurance and Reproducibility</h2>
<h3 id="s1.8.1-automated-quality-checks">S1.8.1 Automated Quality
Checks</h3>
<p><strong>Data Quality Validation</strong>: - Text encoding
verification - Corpus completeness checks - Metadata consistency
validation</p>
<p><strong>Algorithmic Validation</strong>: - Deterministic output
verification - Cross-platform compatibility testing - Performance
regression monitoring</p>
<h3 id="s1.8.2-reproducibility-framework">S1.8.2 Reproducibility
Framework</h3>
<p><strong>Version Control</strong>: All code, data, and parameters are
version controlled with DOI minting for long-term access.</p>
<p><strong>Containerization</strong>: Analysis environments are
containerized for exact reproducibility.</p>
<p><strong>Documentation</strong>: Comprehensive documentation of all
processing steps, parameters, and decisions.</p>
<h2 id="s1.9-extensions-and-future-methods">S1.9 Extensions and Future
Methods</h2>
<h3 id="s1.9.1-advanced-semantic-analysis">S1.9.1 Advanced Semantic
Analysis</h3>
<p>Future extensions include:</p>
<p><strong>Transformer-based Embeddings</strong>: Using contextual
language models for more sophisticated semantic analysis.</p>
<p><strong>Multilingual Extensions</strong>: Cross-language terminology
mapping and comparison.</p>
<p><strong>Temporal Analysis</strong>: Tracking terminological evolution
over time using diachronic methods.</p>
<h3 id="s1.9.2-integration-with-external-resources">S1.9.2 Integration
with External Resources</h3>
<p><strong>Ontology Integration</strong>: Mapping to existing biological
ontologies and terminologies.</p>
<p><strong>Citation Network Analysis</strong>: Integrating citation
patterns with terminology usage.</p>
<p><strong>Author Network Analysis</strong>: Examining how terminology
use correlates with research communities.</p>
<p>This detailed methodological framework ensures rigorous, reproducible
Ento-Linguistic analysis while maintaining flexibility for
methodological refinement and extension.</p>
</body>
</html>
