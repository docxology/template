% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
  ignorenonframetext,
]{beamer}
\newif\ifbibliography
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% remove section numbering
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{section title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{subsection title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{\texorpdfstring{}{}}
\date{}

\begin{document}

\begin{frame}{Appendix}
\protect\phantomsection\label{sec:appendix}
This appendix provides additional technical details supporting the
Ento-Linguistic analysis presented in the main manuscript.

\begin{block}{A. Text Processing Implementation Details}
\protect\phantomsection\label{a.-text-processing-implementation-details}
\begin{block}{A.1 Linguistic Preprocessing Pipeline}
\protect\phantomsection\label{a.1-linguistic-preprocessing-pipeline}
Our text processing pipeline implements systematic normalization to
ensure reliable pattern detection across diverse scientific writing
styles:

\begin{equation}\label{eq:text_normalization_detailed}
T_{\text{processed}} = \text{lemmatize}(\text{pos_filter}(\text{tokenize}(\text{lowercase}(\text{unicode_normalize}(T)))))
\end{equation}

where each transformation step preserves semantic content while
standardizing linguistic variation.

\textbf{Tokenization Strategy}: We employ domain-aware tokenization that
recognizes scientific terminology:

\begin{equation}\label{eq:scientific_tokenization}
\tau_{\text{scientific}}(T) = \left\{\begin{array}{ll}
\text{scientific\_term}(t) & \text{if } t \in \mathcal{T}_{\text{domain}} \\
\text{word\_tokenize}(t) & \text{otherwise}
\end{array}\right.
\end{equation}

where \(\mathcal{T}_{\text{domain}}\) contains curated entomological
terminology that should not be further subdivided.
\end{block}

\begin{block}{A.2 Linguistic Feature Extraction}
\protect\phantomsection\label{a.2-linguistic-feature-extraction}
Our feature extraction combines multiple linguistic indicators:

\textbf{Syntactic Features}: Part-of-speech patterns, dependency
relations, and grammatical structures characteristic of scientific
discourse.

\textbf{Semantic Features}: Word embeddings, semantic similarity
measures, and domain-specific concept vectors.

\textbf{Discourse Features}: Rhetorical markers, argumentative
structures, and citation patterns that indicate research traditions.
\end{block}
\end{block}

\begin{block}{B. Terminology Extraction Algorithms}
\protect\phantomsection\label{b.-terminology-extraction-algorithms}
\begin{block}{B.1 Domain-Specific Term Identification}
\protect\phantomsection\label{b.1-domain-specific-term-identification}
Terminology extraction uses a multi-criteria scoring function:

\begin{equation}\label{eq:term_scoring_detailed}
S(t, d) = w_1 \cdot \text{frequency}(t, d) + w_2 \cdot \text{contextual_coherence}(t, d) + w_3 \cdot \text{semantic_relevance}(t, d)
\end{equation}

where \(d\) represents the Ento-Linguistic domain and weights
\(w_1, w_2, w_3\) are calibrated for each domain.

\textbf{Contextual Coherence}: Measures how consistently a term appears
in domain-relevant contexts:

\begin{equation}\label{eq:contextual_coherence}
C(t, d) = \frac{|\text{contexts}(t, d)|}{\sum_{d' \in D} |\text{contexts}(t, d')|}
\end{equation}
\end{block}

\begin{block}{B.2 Ambiguity Detection Framework}
\protect\phantomsection\label{b.2-ambiguity-detection-framework}
Ambiguity detection combines statistical and linguistic indicators:

\begin{equation}\label{eq:ambiguity_detection}
A(t) = \alpha \cdot H(\text{contexts}(t)) + \beta \cdot \text{semantic_variance}(t) + \gamma \cdot \text{syntactic_ambiguity}(t)
\end{equation}

where \(H(\text{contexts}(t))\) is the entropy of contextual usage
patterns.
\end{block}
\end{block}

\begin{block}{C. Network Construction and Analysis}
\protect\phantomsection\label{c.-network-construction-and-analysis}
\begin{block}{C.1 Edge Weight Calculation}
\protect\phantomsection\label{c.1-edge-weight-calculation}
Network edges are weighted using multiple co-occurrence measures:

\begin{equation}\label{eq:edge_weighting_detailed}
w(u,v) = \frac{1}{3} \left[ \frac{\text{co-occurrence}(u,v)}{\max(\text{freq}(u), \text{freq}(v))} + \text{Jaccard}(u,v) + \text{semantic_similarity}(u,v) \right]
\end{equation}

\textbf{Co-occurrence Window}: 50-word sliding windows capture
meaningful term relationships while avoiding noise from distant terms.

\textbf{Semantic Similarity}: Uses domain-specific embeddings trained on
entomological literature.
\end{block}

\begin{block}{C.2 Community Detection Algorithms}
\protect\phantomsection\label{c.2-community-detection-algorithms}
We implement multiple community detection approaches for robust network
partitioning:

\textbf{Modularity Optimization}:

\begin{equation}\label{eq:modularity_optimization}
Q = \frac{1}{2m} \sum_{ij} \left[ A_{ij} - \frac{k_i k_j}{2m} \right] \delta(c_i, c_j)
\end{equation}

\textbf{Domain-Aware Clustering}: Incorporates Ento-Linguistic domain
knowledge to ensure communities respect conceptual boundaries.
\end{block}

\begin{block}{C.3 Network Validation Metrics}
\protect\phantomsection\label{c.3-network-validation-metrics}
Network quality is assessed using comprehensive validation:

\begin{equation}\label{eq:network_validation_detailed}
V(G) = \lambda_1 \cdot \text{modularity}(G) + \lambda_2 \cdot \text{domain_purity}(G) + \lambda_3 \cdot \text{structural_stability}(G)
\end{equation}

where domain purity measures alignment with Ento-Linguistic domain
structure.
\end{block}
\end{block}

\begin{block}{D. Framing Analysis Implementation}
\protect\phantomsection\label{d.-framing-analysis-implementation}
\begin{block}{D.1 Anthropomorphic Framing Detection}
\protect\phantomsection\label{d.1-anthropomorphic-framing-detection}
Anthropomorphic language is detected through multiple indicators:

\textbf{Lexical Indicators}: Terms suggesting human-like agency,
intentionality, or social structures.

\textbf{Syntactic Patterns}: Sentence structures implying human-like
behavior or cognition.

\textbf{Semantic Fields}: Clusters of terms drawing from human social,
psychological, or economic domains.

\textbf{Detection Algorithm}:

\begin{equation}\label{eq:anthropomorphic_detection}
A_{\text{anthro}}(t) = \sum_{f \in F_{\text{human}}} \text{similarity}(t, f) \cdot w_f
\end{equation}
\end{block}

\begin{block}{D.2 Hierarchical Framing Analysis}
\protect\phantomsection\label{d.2-hierarchical-framing-analysis}
Hierarchical structures are identified through:

\textbf{Term Relationship Patterns}: Chains of subordination and
authority relationships.

\textbf{Power Dynamic Indicators}: Terms implying control, dominance, or
submission.

\textbf{Organizational Metaphors}: Language drawing from human
institutional and hierarchical systems.
\end{block}
\end{block}

\begin{block}{E. Validation and Quality Assurance}
\protect\phantomsection\label{e.-validation-and-quality-assurance}
\begin{block}{E.1 Inter-annotator Agreement Procedures}
\protect\phantomsection\label{e.1-inter-annotator-agreement-procedures}
Terminology validation uses multiple annotators:

\textbf{Cohen's Kappa}: Measures agreement between human annotators and
automated classification.

\textbf{Fleiss' Kappa}: Extends agreement measurement to multiple
annotators.

\textbf{Bootstrap Validation}: Assesses stability of classifications
across subsampling.
\end{block}

\begin{block}{E.2 Statistical Validation Framework}
\protect\phantomsection\label{e.2-statistical-validation-framework}
All analyses include rigorous statistical validation:

\textbf{Terminology Extraction Validation}: - \textbf{Precision}: Manual
verification of extracted terms against expert-curated lists -
\textbf{Recall}: Coverage assessment against comprehensive domain
glossaries - \textbf{Domain Accuracy}: Correct classification into
Ento-Linguistic domains

\textbf{Network Validation}: - \textbf{Structural Validity}: Comparison
against null models - \textbf{Domain Correspondence}: Alignment with
theoretical domain boundaries - \textbf{Stability Analysis}: Consistency
across subsampling procedures
\end{block}

\begin{block}{E.3 Corpus and Data Validation}
\protect\phantomsection\label{e.3-corpus-and-data-validation}
\textbf{Corpus Integrity Checks}: - Text encoding verification -
Metadata completeness validation - Duplicate document detection -
Temporal distribution analysis

\textbf{Processing Validation}: - Deterministic output verification -
Cross-platform compatibility testing - Memory usage monitoring -
Performance regression detection
\end{block}
\end{block}

\begin{block}{F. Computational Environment and Reproducibility}
\protect\phantomsection\label{f.-computational-environment-and-reproducibility}
\begin{block}{F.1 Software Dependencies}
\protect\phantomsection\label{f.1-software-dependencies}
Analysis conducted using the following software stack:

\begin{itemize}
\tightlist
\item
  \textbf{Python}: 3.10+ for analysis implementation
\item
  \textbf{spaCy}: 3.7+ for linguistic processing
\item
  \textbf{NetworkX}: 3.1+ for network analysis
\item
  \textbf{scikit-learn}: 1.3+ for statistical validation
\item
  \textbf{pandas}: 2.0+ for data manipulation
\item
  \textbf{matplotlib}: 3.7+ for visualization
\item
  \textbf{jupyter}: 1.0+ for interactive analysis
\end{itemize}
\end{block}

\begin{block}{F.2 Hardware Specifications}
\protect\phantomsection\label{f.2-hardware-specifications}
Computational resources used:

\begin{itemize}
\tightlist
\item
  \textbf{CPU}: Intel Xeon E5-2690 v4 (28 cores @ 2.60GHz)
\item
  \textbf{Memory}: 128GB DDR4
\item
  \textbf{Storage}: 2TB NVMe SSD for data processing
\item
  \textbf{OS}: Ubuntu 22.04 LTS
\end{itemize}
\end{block}

\begin{block}{F.3 Reproducibility Framework}
\protect\phantomsection\label{f.3-reproducibility-framework}
\textbf{Version Control}: All code, data, and parameters tracked with
git.

\textbf{Containerization}: Analysis environments containerized using
Docker for exact reproducibility.

\textbf{Data Provenance}: Complete audit trail of data processing steps
and parameter choices.

\textbf{Random Seed Management}: All stochastic operations use fixed
seeds for deterministic results.
\end{block}
\end{block}

\begin{block}{G. Extended Mathematical Formulations}
\protect\phantomsection\label{g.-extended-mathematical-formulations}
\begin{block}{G.1 Conceptual Mapping Framework}
\protect\phantomsection\label{g.1-conceptual-mapping-framework}
The conceptual mapping algorithm formalizes term relationships:

\begin{equation}\label{eq:concept_mapping}
M(t_i, t_j) = \frac{1}{k} \sum_{c=1}^k \text{similarity}(\vec{t_i}^{(c)}, \vec{t_j}^{(c)})
\end{equation}

where \(k\) represents the number of contextual embeddings and
\(\vec{t}^{(c)}\) is the embedding in context \(c\).
\end{block}

\begin{block}{G.2 Discourse Pattern Recognition}
\protect\phantomsection\label{g.2-discourse-pattern-recognition}
Discourse pattern detection uses sequence modeling:

\begin{equation}\label{eq:discourse_patterns}
P(d|t_1, \dots, t_n) = \prod_{i=1}^n P(t_i|t_{i-1}, d) \cdot P(d)
\end{equation}

where \(d\) represents discourse patterns and \(t_i\) are sequential
terms.

This comprehensive technical appendix provides the detailed
implementation foundations supporting the Ento-Linguistic analysis
presented in the main manuscript.
\end{block}
\end{block}
\end{frame}

\end{document}
