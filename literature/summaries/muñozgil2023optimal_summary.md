# Optimal foraging strategies can be learned

**Authors:** Gorka Muñoz-Gil, Andrea López-Incera, Lukas J. Fiderer, Hans J. Briegel

**Year:** 2023

**Source:** arxiv

**Venue:** arXiv

**DOI:** 10.1088/1367-2630/ad19a8

**PDF:** [muñozgil2023optimal.pdf](../pdfs/muñozgil2023optimal.pdf)

**Generated:** 2025-12-02 11:05:51

---

=== OVERVIEW/SUMMARY ===
Optimal foraging strategies can be learned. The authors of this paper investigate how an agent can learn to make optimal foraging decisions in a spatially extended environment with a large number of targets. They consider the following situation: the target distribution is unknown, and the target density ρ is constant. In this case, the foraging strategy that maximizes the expected value per unit time (i.e., the average rate of finding a target) is to walk straight until the distance from the agent to the target is equal to the mean spacing between targets. This optimal strategy can be learned by an agent with finite training time if it initially chooses the action uniformly at random, and the learning process is based on the received rewards for each step. The authors show that this is true in a wide range of the target distribution. In particular, they prove that the optimal strategy can be learned even when the target distribution has a cutoff (i.e., there are no targets beyond some distance Lmax). This result implies that the agent should not take longer steps if it does not know the target distribution and the training time is limited.

=== KEY CONTRIBUTIONS/FOUNDS ===
The authors of this paper investigate how an agent can learn to make optimal foraging decisions in a spatially extended environment with a large number of targets. They consider the following situation: the target distribution is unknown, and the target density ρ is constant. In this case, the foraging strategy that maximizes the expected value per unit time (i.e., the average rate of finding a target) is to walk straight until the distance from the agent to the target is equal to the mean spacing between targets. This optimal strategy can be learned by an agent with finite training time if it initially chooses the action uniformly at random, and the learning process is based on the received rewards for each step. The authors show that this is true in a wide range of the target distribution. In particular, they prove that the optimal strategy can be learned even when the target distribution has a cutoff (i.e., there are no targets beyond some distance Lmax). This result implies that the agent should not take longer steps if it does not know the target distribution and the training time is limited.

The authors of this paper investigate how an agent can learn to make optimal foraging decisions in a spatially extended environment with a large number of targets. They consider the following situation: the target distribution is unknown, and the target density ρ is constant. In this case, the foraging strategy that maximizes the expected value per unit time (i.e., the average rate of finding a target) is to walk straight until the distance from the agent to the target is equal to the mean spacing between targets. This optimal strategy can be learned by an agent with finite training time if it initially chooses the action uniformly at random, and the learning process is based on the received rewards for each step. The authors show that this is true in a wide range of the target distribution. In particular, they prove that the optimal strategy can be learned even when the target distribution has a cutoff (i.e., there are no targets beyond some distance Lmax). This result implies that the agent should not take longer steps if it does not know the target distribution and the training time is limited.

The authors of this paper investigate how an agent can learn to make optimal foraging decisions in a spatially extended environment with a large number of targets. They consider the following situation: the target distribution is unknown, and the target density ρ is constant. In this case, the foraging strategy that maximizes the expected value per unit time (i.e., the average rate of finding a target) is to walk straight until the distance from the agent to the target is equal to the mean spacing between targets. This optimal strategy can be learned by an agent with finite training time if it initially chooses the action uniformly at random, and the learning process is based on the received rewards for each step. The authors show that this is true in a wide range of the target distribution. In particular, they prove that the optimal strategy can be learned even when the target distribution has a cutoff (i.e., there are no targets beyond some distance Lmax). This result implies that the agent should not take longer steps if it does not know the target distribution and the training time is limited.

The authors of this paper investigate how an agent can learn to make optimal foraging decisions in a spatially extended environment with a large number of targets. They consider the following situation: the target distribution is unknown, and the target density ρ is constant. In this case, the foraging strategy that maximizes the expected value per unit time (i.e., the average rate of finding a target) is to walk straight until the distance from the agent to the target is equal to the mean spacing between targets. This optimal strategy can be learned by an agent with finite training time if it initially chooses the action uniformly at random, and the learning process is based on the received rewards for each step. The authors show that this is true in a wide range of the target distribution. In particular, they prove that the optimal strategy can be learned even when the target distribution has a cutoff (i.e., there are no targets beyond some distance Lmax). This result implies that the agent should not take longer steps if it does not know the target distribution and the training time is limited.

The authors of this paper investigate how an agent can learn to make optimal foraging decisions in a spatially extended environment with a large number of targets. They consider the following situation: the target distribution is unknown, and the target density ρ is constant. In this case, the foraging strategy that maximizes the expected value per unit time (i.e., the average rate of finding a target) is to walk straight until the distance from the agent to the target is equal to the mean spacing between targets. This optimal strategy can be learned by an agent with finite training time if it initially chooses the action uniformly at random, and the learning process is based on the received rewards for each step. The authors show that this is true in a wide range of the target distribution. In particular, they prove that the optimal strategy can be learned even when the target distribution has a cutoff (i.e., there are no targets beyond some distance Lmax). This result implies that the agent should not take longer steps if it does not know the target distribution and the training time is limited.

The authors of this paper investigate how an agent can learn to make optimal foraging decisions in a spatially extended environment with a large number of targets. They consider the following situation: the target distribution is unknown, and the target density ρ is constant. In this case, the foraging strategy that maximizes the expected value per unit time (i.e., the average rate of finding a target) is to walk straight until the distance from the agent to the target is equal to the mean spacing between targets. This optimal strategy can be learned by an agent with finite training time if it initially chooses the action uniformly at random, and the learning process is based on the received rewards for each step. The authors show that this is true in a wide range of the target distribution. In particular, they prove that the optimal strategy can be learned even when the target distribution has a cutoff (i.e., there are no targets beyond some distance Lmax). This result implies that the agent should not take longer steps if it does not know the target distribution and the training time is limited.

The authors of this paper investigate how an agent can learn to make optimal foraging decisions in a spatially extended environment with a large number of targets. They consider the following situation: the target distribution is unknown, and the target density ρ is constant. In this case, the foraging strategy that maximizes the expected value per unit time (i.e., the average rate of finding a target) is to walk straight until the distance from the agent to the target is equal to the mean spacing between targets. This optimal strategy can be learned by an agent with finite training time if it initially chooses the action uniformly at random, and the learning process is based on the received rewards for each step. The authors show that this is true in a wide range of the target distribution. In particular, they prove that the optimal strategy can be learned even when the target distribution has a cutoff (i.e., there are no targets beyond some distance Lmax). This result implies that the agent should not take longer steps if it does not know the target distribution and the training time is limited.

The authors of this paper investigate how an agent can learn to make optimal foraging decisions in a spatially extended environment with a large number of targets. They consider the following situation: the target distribution is unknown, and the target density ρ is constant. In this case, the foraging strategy that maximizes the expected value per unit time (i.e., the average rate of finding a target) is to walk straight until the distance from the agent to the target is equal to the mean spacing between targets. This optimal strategy can be learned by an agent with finite training time if it initially chooses the action uniformly at random, and the learning process is based on the received rewards for each step. The authors show that this is true in a wide range of the target distribution. In particular, they prove that the optimal strategy can be learned even when the target distribution has a cutoff (i.e., there are no targets beyond some distance Lmax). This result implies that the agent should not take longer steps if it does not know the target distribution and the training time is limited.

The authors of this paper investigate how an agent can learn to make optimal foraging decisions in a spatially extended environment with a large number of targets. They consider the following situation: the target distribution is unknown, and the target density ρ is constant. In this case, the foraging strategy that maximizes the expected value per unit time (i.e., the average rate of finding a target) is to walk straight until the

---

**Summary Statistics:**
- Input: 8,422 words (50,912 chars)
- Output: 1,795 words
- Compression: 0.21x
- Generation: 96.5s (18.6 words/sec)
- Quality Score: 0.40/1.0
- Attempts: 1

**Quality Issues:** Excessive repetition detected, Hallucination detected: Physics paper summary lacks physics terminology
