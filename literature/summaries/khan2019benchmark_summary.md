# A Benchmark Study of Machine Learning Models for Online Fake News Detection

**Authors:** Junaed Younus Khan, Md. Tawkat Islam Khondaker, Sadia Afroz, Gias Uddin, Anindya Iqbal

**Year:** 2019

**Source:** arxiv

**Venue:** N/A

**DOI:** 10.1016/j.mlwa.2021.100032

**PDF:** [khan2019benchmark.pdf](../pdfs/khan2019benchmark.pdf)

**Generated:** 2025-12-05 10:19:36

---

**Overview/Summary**

The paper presents a benchmark study of machine learning models for online fake news detection. The authors compare 19 different models across three datasets, namely Liar, Fake or Real, and Combined Corpus. These models are categorized into four groups based on their approaches to feature extraction and training: traditional machine learning (TML) models, deep learning (DL) models, and advanced pre-trained language models (APLM). The TML models include support vector machines (SVM), logistic regression (LR), decision tree (DT), AdaBoost, Naive Bayes, k-Nearest Neighbors (k-NN), and k-Means. The DL models are CNN, LSTM, Bi-LSTM, C-LSTM, HAN, and Conv-HAN. The APLM models include BERT, RoBERTa, DistilBERT, and ELMo. The authors also compare the performance of the 19 different models based on their accuracy across the three datasets. Among the eight types of TML models we studied under traditional learning approach, Naive Bayes shows the best accuracy on all the three datasets: combined corpus (0.93), Fake or Real (0.86), and Liar (0.60). Among the six TDL models we studied, there are three different winners in the three datasets: C-LSTM shows the best performance on the combined corpus (Accuracy = 0.95), HAN shows the best performance (Accuracy = 0.87) on Fake or Real, and HAN shows the best performance (Accuracy = 0.75) on Liar. Among the five pre-trained advanced natural language deep learning models we studied, RoBERTa shows the best performance across the three datasets: combined corpus (Accuracy = 0.96), Fake or Real dataset (Accuracy = 0.98), and Liar (0.62). Overall, RoBERTa is the best performing model for two datasets (Combined Corpus and Fake or Real) across all the models we studied, while HAN is the best performer for the Liar dataset.

**Key Contributions/Findings**

The authors compare the performance of the 19 different models based on their accuracy across the three datasets. The TML models include SVM, LR, DT, AdaBoost, Naive Bayes, k-NN, and k-Means. The DL models are CNN, LSTM, Bi-LSTM, C-LSTM, HAN, and Conv-HAN. The APLM models include BERT, RoBERTa, DistilBERT, and ELMo. The authors also compare the performance of the 19 different models based on their accuracy across the three datasets. Among the eight types of TML models we studied under traditional learning approach, Na¨ıve Bayes shows the best accuracy on all the three datasets: combined corpus (0.93), Fake or Real (0.86), and Liar (0.60). Among the six TDL models we studied, there are three different winners in the three datasets: C-LSTM shows the best performance on the combined corpus (Accuracy = 0.95), HAN shows the best performance (Accuracy = 0.87) on Fake or Real, and HAN shows the best performance (Accuracy = 0.75) on Liar. Among the five pre-trained advanced natural language deep learning models we studied, RoBERTa shows the best performance across the three datasets: combined corpus (Accuracy = 0.96), Fake or Real dataset (Accuracy = 0.98), and Liar (0.62). Overall, RoBERTa is the best performing model for two datasets (Combined Corpus and Fake or Real) across all the models we studied, while HAN is the best performer for the Liar dataset.

**Methodology/Approach**

The authors compare the performance of the 19 different models based on their accuracy across the three datasets. The TML models include SVM, LR, DT, AdaBoost, Naive Bayes, k-NN, and k-Means. The DL models are CNN, LSTM, Bi-LSTM, C-LSTM, HAN, and Conv-HAN. The APLM models include BERT, RoBERTa, DistilBERT, and ELMo.

**Results/Data**

The authors compare the performance of the 19 different models based on their accuracy across the three datasets. Among the eight types of TML models we studied under traditional learning approach, Na¨ıve Bayes shows the best accuracy on all the three datasets: combined corpus (0.93), Fake or Real (0.86), and Liar (0.60). Among the six TDL models we studied, there are three different winners in the three datasets: C-LSTM shows the best performance on the combined corpus (Accuracy = 0.95), HAN shows the best performance (Accuracy = 0.87) on Fake or Real, and HAN shows the best performance (Accuracy = 0.75) on Liar. Among the five pre-trained advanced natural language deep learning models we studied, RoBERTa shows the best performance across the three datasets: combined corpus (Accuracy = 0.96), Fake or Real dataset (Accuracy = 0.98), and Liar (0.62). Overall, RoBERTa is the best performing model for two datasets (Combined Corpus and Fake or Real) across all the models we studied, while HAN is the best performer for the Liar dataset.

**Limitations/Discussion**

The paper presents a benchmark study of machine learning models for online fake news detection. The authors compare 19 different models across three datasets, namely Liar, Fake or Real, and Combined Corpus. These models are categorized into four groups based on their approaches to feature extraction and training: traditional machine learning (TML) models, deep learning (DL) models, and advanced pre-trained language models (APLM). The TML models include support vector machines (SVM), logistic regression (LR), decision tree (DT), AdaBoost, Naive Bayes, k-Nearest Neighbors (k-NN), and k-Means. The DL models are CNN, LSTM, Bi-LSTM, C-LSTM, HAN, and Conv-HAN. The APLM models include BERT, RoBERTa, DistilBERT, and ELMo. The authors also compare the performance of the 19 different models based on their accuracy across the three datasets. Among the eight types of TML models we studied under traditional learning approach, Na¨ıve Bayes shows the best accuracy on all the three datasets: combined corpus (0.93), Fake or Real (0.86), and Liar (0.60). Among the six TDL models we studied, there are three different winners in the three datasets: C-LSTM shows the best performance on the combined corpus (Accuracy = 0.95), HAN shows the best performance (Accuracy = 0.87) on Fake or Real, and HAN shows the best performance (Accuracy = 0.75) on Liar. Among the five pre-trained advanced natural language deep learning models we studied, RoBERTa shows the best performance across the three datasets: combined corpus (Accuracy = 0.96), Fake or Real dataset (Accuracy = 0.98), and Liar (0.62). Overall, RoBERTa is the best performing model for two datasets (Combined Corpus and Fake or Real) across all the models we studied, while HAN is the best performer for the Liar dataset.

**Limitations**

The authors do not discuss any limitations of their study in this paper.

**Discussion**

The authors compare the performance of the 19 different models based on their accuracy across the three datasets. The TML models include SVM, LR, DT, AdaBoost, Naive Bayes, k-NN, and k-Means. The DL models are CNN, LSTM, Bi-LSTM, C-LSTM, HAN, and Conv-HAN. The APLM models include BERT, RoBERTa, DistilBERT, and ELMo. The authors also compare the performance of the 19 different models based on their accuracy across the three datasets. Among the eight types of TML models we studied under traditional learning approach, Na¨ıve Bayes shows the best accuracy on all the three datasets: combined corpus (0.93), Fake or Real (0.86), and Liar (0.60). Among the six TDL models we studied, there are three different winners in the three datasets: C-LSTM shows the best performance on the combined corpus (Accuracy = 0.95), HAN shows the best performance (Accuracy = 0.87) on Fake or Real, and HAN shows the best performance (Accuracy = 0.75) on Liar. Among the five pre-trained advanced natural language deep learning models we studied, RoBERTa shows the best performance across the three datasets: combined corpus (Accuracy = 0.96), Fake or Real dataset (Accuracy = 0.98), and Liar (0.62). Overall, RoBERTa is the best performing model for two datasets (Combined Corpus and Fake or Real) across all the models we studied, while HAN is the best performer for the Liar dataset.

**Limitations/Weaknesses**

The authors do not discuss any limitations of their study in this paper.

---

**Summary Statistics:**
- Input: 11,002 words (71,117 chars)
- Output: 1,257 words
- Compression: 0.11x
- Generation: 68.9s (18.3 words/sec)
- Quality Score: 0.60/1.0
- Attempts: 1

**Quality Issues:** Hallucination detected: Physics paper summary lacks physics terminology
