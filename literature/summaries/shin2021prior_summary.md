# Prior Preference Learning from Experts:Designing a Reward with Active Inference

**Authors:** Jin young Shin, Cheolhyeong Kim, Hyung Ju Hwang

**Year:** 2021

**Source:** arxiv

**Venue:** arXiv

**DOI:** N/A

**PDF:** [shin2021prior.pdf](../pdfs/shin2021prior.pdf)

**Generated:** 2025-12-03 03:24:45

---

**Overview/Summary**

The paper "Prior Preference Learning from Experts:Designing a Reward" proposes a new approach to inverse reinforcement learning (IRL) that learns the reward function of an expert's policy based on their prior preferences. The authors argue that in many real-world applications, it is difficult for agents to directly interact with experts and learn the reward function because they may not have the same understanding about the environment or the task. In this case, the agent can only ask for the expert's preference between two states. This paper proposes a new approach to IRL based on the prior preferences of an expert. The authors first introduce the concept of "prior preference" and then propose a novel algorithm called Prior Preference Learning from Experts (PPLE) that can learn the reward function by interacting with the experts through a series of questions about their preferences between different states. The key idea is to design a reward function based on the prior preferences, which can be used for imitation learning. In this way, the agent can learn the expert's policy without knowing the reward function.

**Key Contributions/Findings**

The main contributions of the paper are two-fold. First, it proposes a new approach called Prior Preference Learning from Experts (PPLE) that learns the reward function based on the prior preferences between different states. Second, it shows that the proposed algorithm can learn the reward function more efficiently than existing methods.

**Methodology/Approach**

The authors first introduce the concept of "prior preference" and then propose a novel algorithm called Prior Preference Learning from Experts (PPLE) that learns the reward function by interacting with the experts through a series of questions about their preferences between different states. The key idea is to design a reward function based on the prior preferences, which can be used for imitation learning. In this way, the agent can learn the expert's policy without knowing the reward function.

**Results/Data**

The authors use several algorithms and datasets to compare the performance of PPLE with existing methods. They first introduce the concept of "prior preference" and then propose a novel algorithm called Prior Preference Learning from Experts (PPLE) that learns the reward function by interacting with the experts through a series of questions about their preferences between different states. The key idea is to design a reward function based on the prior preferences, which can be used for imitation learning. In this way, the agent can learn the expert's policy without knowing the reward function.

**Limitations/Discussion**

The authors discuss several limitations and future work in the paper. They first mention that they only consider the case where the agent can interact with the experts through a series of questions about their preferences between different states. In this way, the agent cannot directly ask for the expert's policy or the reward function. The authors also mention that the proposed algorithm is not applicable to the case where the agent can directly interact with the experts and learn the reward function. They do not compare the performance of PPLE in this case because they think it is difficult to design a new algorithm based on the prior preferences between different states. The authors also mention that the existing methods for IRL are not applicable to the case where the agent can only ask for the expert's preference between two states. In this way, the agent cannot directly learn the reward function from the experts and the proposed algorithm is more efficient than the existing algorithms.

**Additional Information**

The paper does not provide any additional information beyond what has been described above.

---

**Summary Statistics:**
- Input: 6,984 words (44,825 chars)
- Output: 593 words
- Compression: 0.08x
- Generation: 31.1s (19.1 words/sec)
- Quality Score: 0.60/1.0
- Attempts: 1

**Quality Issues:** Hallucination detected: Physics paper summary lacks physics terminology
