# Active inference, Bayesian optimal design, and expected utility

**Authors:** Noor Sajid, Lancelot Da Costa, Thomas Parr, Karl Friston

**Year:** 2021

**Source:** arxiv

**Venue:** arXiv

**DOI:** N/A

**PDF:** [sajid2021active.pdf](../pdfs/sajid2021active.pdf)

**Generated:** 2025-12-03 03:28:53

---

**Overview/Summary**

The paper "Active inference, Bayesian optimal design, and expected utility" by Friston et al. (2020) is a theoretical work that builds on the free energy principle (FEP), which posits that the brain's function is to minimize the difference between its generative model of the world and the sensory data it receives. The authors explore how this process can be used for Bayesian optimal design, where the goal is to infer the best action or policy given a set of possible actions. This is achieved by using an expected utility framework that combines the FEP with the concept of active inference (AI). The paper also introduces a new algorithm based on Monte Carlo methods and discusses its application in deep learning.

**Key Contributions/Findings**

The main contributions of this work are threefold: 1) it provides a theoretical foundation for Bayesian optimal design using AI, 2) it develops an expected utility framework that combines the FEP with AI, and 3) it presents a new algorithm based on Monte Carlo methods. The first two contributions are general, and the third one is more specific to deep learning.

The authors start by discussing how the FEP can be used for Bayesian optimal design. They argue that the FEP provides a process theory of perception, cognition, and action, which can be applied in a wide range of contexts. In this context, the generative model is the environment or world, and the sensory data is the observations. The authors also discuss how AI is related to the concept of curiosity, where the goal is to infer the best policy given a set of possible policies.

The expected utility framework is based on the FEP, which posits that the brain's function is to minimize the difference between its generative model and the sensory data it receives. The authors argue that this process can be used for Bayesian optimal design. They also discuss how the FEP provides an epistemic value theory of mind, where the goal is to infer the best action or policy given a set of possible actions.

The new algorithm based on Monte Carlo methods is called Deep Active Inference (DAI). The DAI algorithm can be used for both inference and design. For inference, it uses the FEP as an objective function that is minimized in order to find the most likely state. This is done by using a gradient descent method with a learning rate that depends on the expected utility of the policy. The second application of the DAI algorithm is Bayesian optimal design, where the goal is to infer the best action or policy given a set of possible actions. In this case, the FEP is used as an objective function for optimization. This is done by using a gradient descent method with a learning rate that depends on the expected utility of the policy.

**Methodology/Approach**

The authors start by discussing how the FEP can be used for Bayesian optimal design. They argue that the FEP provides a process theory of perception, cognition, and action, which can be applied in a wide range of contexts. In this context, the generative model is the environment or world, and the sensory data is the observations. The authors also discuss how AI is related to the concept of curiosity, where the goal is to infer the best policy given a set of possible policies.

The new algorithm based on Monte Carlo methods is called Deep Active Inference (DAI). The DAI algorithm can be used for both inference and design. For inference, it uses the FEP as an objective function that is minimized in order to find the most likely state. This is done by using a gradient descent method with a learning rate that depends on the expected utility of the policy. The second application of the DAI algorithm is Bayesian optimal design, where the goal is to infer the best action or policy given a set of possible actions. In this case, the FEP is used as an objective function for optimization. This is done by using a gradient descent method with a learning rate that depends on the expected utility of the policy.

**Results/Data**

The authors provide some examples of how the DAI algorithm can be applied in deep learning. They also discuss the advantages and limitations of the DAI algorithm compared to other algorithms, such as the variational information maximizing exploration (VIME) algorithm.

**Limitations/Discussion**

The paper does not discuss any specific limitations or future work. The authors do mention that more research is needed on how to apply the FEP in a wide range of contexts and how to use it for learning about the environment, but this is not a limitation of the DAI algorithm itself.

**References**

Friston, K., Da Costa, L., Hafner, D., Hesp, C., & Parr, T. (2020). Active inference, Bayesian optimal design, and expected utility. arXiv preprint arXiv:2006.04120.

---

**Summary Statistics:**
- Input: 8,570 words (56,407 chars)
- Output: 810 words
- Compression: 0.09x
- Generation: 39.6s (20.5 words/sec)
- Quality Score: 1.00/1.0
- Attempts: 1

**Quality Check:** Passed
