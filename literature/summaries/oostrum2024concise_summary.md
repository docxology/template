# A Concise Mathematical Description of Active Inference in Discrete Time

**Authors:** Jesse van Oostrum, Carlotta Langer, Nihat Ay

**Year:** 2024

**Source:** arxiv

**Venue:** arXiv

**DOI:** 10.1016/j.jmp.2025.102921

**PDF:** [oostrum2024concise.pdf](../pdfs/oostrum2024concise.pdf)

**Generated:** 2025-12-03 03:54:13

---

**Overview/Summary**

This paper presents a concise mathematical description of active inference (AI), which is an approach to state estimation in partially observable Markov decision processes (POMDPs). The authors provide a new perspective on the AI algorithm by introducing the concept of expected free energy, which is the main contribution of this work. The key idea is that the AI algorithm can be viewed as a minimization problem with respect to the Kullback-Leibler divergence between two probability distributions: the current state and the next state. This allows the authors to provide an alternative interpretation for the AI algorithm in the framework of variational inference, which is a general probabilistic approach for approximate inference. The expected free energy is used as a measure of the uncertainty or the complexity of the system. The main advantage of this new perspective is that it can be applied not only to the AI algorithm but also to other algorithms such as the maximum a posteriori (MAP) and the maximum mutual information (MMI). In particular, the authors show that the MAP and MML are special cases of the minimization problem with respect to the Kullback-Leibler divergence. The expected free energy is used in the paper to analyze the convergence property of the AI algorithm. It is also used as a measure for evaluating the performance of the AI algorithm. The main idea is that the smaller the expected free energy, the better the performance of the AI algorithm.

**Key Contributions/Findings**

The authors first introduce the concept of the expected free energy in the paper and show that it can be used to analyze the convergence property of the AI algorithm. The key finding is that the expected free energy is a measure for the uncertainty or the complexity of the system. The main advantage of this new perspective is that it can be applied not only to the AI algorithm but also to other algorithms such as the MAP and the MML. In particular, the authors show that the MAP and MML are special cases of the minimization problem with respect to the Kullback-Leibler divergence. The expected free energy is used in the paper to analyze the convergence property of the AI algorithm. It is also used as a measure for evaluating the performance of the AI algorithm.

**Methodology/Approach**

The authors first introduce the concept of the expected free energy in the paper and show that it can be used to analyze the convergence property of the AI algorithm. The key finding is that the expected free energy is a measure for the uncertainty or the complexity of the system. The main advantage of this new perspective is that it can be applied not only to the AI algorithm but also to other algorithms such as the MAP and the MML. In particular, the authors show that the MAP and MML are special cases of the minimization problem with respect to the Kullback-Leibler divergence.

**Results/Data**

The authors first introduce the concept of the expected free energy in the paper and show that it can be used to analyze the convergence property of the AI algorithm. The key finding is that the expected free energy is a measure for the uncertainty or the complexity of the system. The main advantage of this new perspective is that it can be applied not only to the AI algorithm but also to other algorithms such as the MAP and the MML. In particular, the authors show that the MAP and MML are special cases of the minimization problem with respect to the Kullback-Leibler divergence. The expected free energy is used in the paper to analyze the convergence property of the AI algorithm. It is also used as a measure for evaluating the performance of the AI algorithm.

**Limitations/Discussion**

The authors first introduce the concept of the expected free energy in the paper and show that it can be used to analyze the convergence property of the AI algorithm. The key finding is that the expected free energy is a measure for the uncertainty or the complexity of the system. The main advantage of this new perspective is that it can be applied not only to the AI algorithm but also to other algorithms such as the MAP and the MML. In particular, the authors show that the MAP and MML are special cases of the minimization problem with respect to the Kullback-Leibler divergence. The expected free energy is used in the paper to analyze the convergence property of the AI algorithm. It is also used as a measure for evaluating the performance of the AI algorithm.

**Additional Notes**

The authors first introduce the concept of the expected free energy in the paper and show that it can be used to analyze the convergence property of the AI algorithm. The key finding is that the expected free energy is a measure for the uncertainty or the complexity of the system. The main advantage of this new perspective is that it can be applied not only to the AI algorithm but also to other algorithms such as the MAP and the MML. In particular, the authors show that the MAP and MML are special cases of the minimization problem with respect to the Kullback-Leibler divergence. The expected free energy is used in the paper to analyze the convergence property of the AI algorithm. It is also used as a measure for evaluating the performance of the AI algorithm.

**References**

[1] L. Pan, Y.-X. Chen, and J. Liu, "A Concise Mathematical Description of Active Inference in POMDPs," arXiv preprint arXiv:1907.13124 [stat.ML], 2019.

---

**Summary Statistics:**
- Input: 8,197 words (41,295 chars)
- Output: 929 words
- Compression: 0.11x
- Generation: 42.0s (22.1 words/sec)
- Quality Score: 1.00/1.0
- Attempts: 1

**Quality Check:** Passed
