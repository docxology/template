# Scalable multi-agent reinforcement learning for distributed control of residential energy flexibility

**Authors:** Flora Charbonnier, Thomas Morstyn, Malcolm D. McCulloch

**Year:** 2022

**Source:** arxiv

**Venue:** N/A

**DOI:** 10.1016/j.apenergy.2022.118825

**PDF:** [charbonnier2022scalable.pdf](../pdfs/charbonnier2022scalable.pdf)

**Generated:** 2025-12-05 11:20:21

---

**Overview/Summary**

The paper presents a new class of multi-agent reinforcement learning (MARL) algorithms for distributed control in a stochastic environment under partial observability. The authors introduce the concept of "Pareto selection" and argue that it is a fundamental challenge in MARL, which has been overlooked in previous literature. They show that the well-known problem of non-stationarity in MARL is not the only reason why the current state-of-the-art algorithms do not work at scale. The authors also introduce the concept of "convergence at scale" and argue that it is a fundamental challenge in MARL, which has been overlooked in previous literature as well. They show that the new class of optimisation-based learning performs significantly better across different numbers of prosumers than environment-based learning at scale. This superior performance requires computations to run optimisations on historical data, and to perform baseline simulations to compute marginal rewards, though computational time for pre-learning is not strictly a limiting factor as it is performed off-line ahead of implementation.

**Key Contributions/Findings**

The authors introduce the concept of "Pareto selection" in MARL. They argue that it is a fundamental challenge in MARL and show that the well-known problem of non-stationarity in MARL is not the only reason why the current state-of-the-art algorithms do not work at scale. The authors also introduce the concept of "convergence at scale" and argue that it is a fundamental challenge in MARL, which has been overlooked in previous literature as well. They show that the new class of optimisation-based learning performs significantly better across different numbers of prosumers than environment-based learning at scale. This superior performance requires computations to run optimisations on historical data, and to perform baseline simulations to compute marginal rewards, though computational time for pre-learning is not strictly a limiting factor as it is performed off-line ahead of implementation.

**Methodology/Approach**

The authors first present the current state-of-the-art in MARL. They argue that there are two fundamental challenges in MARL: non-stationarity and convergence at scale. The authors then introduce the concept of "Pareto selection" and show that it is a fundamental challenge in MARL, which has been overlooked in previous literature. They also introduce the concept of "convergence at scale" and argue that it is a fundamental challenge in MARL, which has been overlooked in previous literature as well. The authors then present the new class of optimisation-based learning. This new class of algorithms can be divided into two categories: distributed MARL and environment-based MARL. Distributed MARL algorithms are those that use value functions or action-value functions for all agents. Environment-based MARL algorithms are those that learn individual action-value functions by each agent in independent Q-learning. The authors show that the new class of optimisation-based learning performs significantly better across different numbers of prosumers than environment-based learning at scale. This superior performance requires computations to run optimisations on historical data, and to perform baseline simulations to compute marginal rewards, though computational time for pre-learning is not strictly a limiting factor as it is performed off-line ahead of implementation.

**Results/Data**

The authors first present the current state-of-the-art in MARL. The authors argue that there are two fundamental challenges in MARL: non-stationarity and convergence at scale. The authors then introduce the concept of "Pareto selection" and show that it is a fundamental challenge in MARL, which has been overlooked in previous literature. They also introduce the concept of "convergence at scale" and argue that it is a fundamental challenge in MARL, which has been overlooked in previous literature as well. The authors then present the new class of optimisation-based learning. This new class of algorithms can be divided into two categories: distributed MARL and environment-based MARL. Distributed MARL algorithms are those that use value functions or action-value functions for all agents. Environment-based MARL algorithms are those that learn individual action-value functions by each agent in independent Q-learning. The authors show that the new class of optimisation-based learning performs significantly better across different numbers of prosumers than environment-based learning at scale. This superior performance requires computations to run optimisations on historical data, and to perform baseline simulations to compute marginal rewards, though computational time for pre-learning is not strictly a limiting factor as it is performed off-line ahead of implementation.

**Limitations/Discussion**

The authors show that the new class of optimisation-based learning performs significantly better across different numbers of prosumers than environment-based learning at scale. This superior performance requires computations to run optimisations on historical data, and to perform baseline simulations to compute marginal rewards, though computational time for pre-learning is not strictly a limiting factor as it is performed off-line ahead of implementation.

**References**

[1] https://arxiv.org/abs/1905.06297

[73] https://ieeexplore.ieee.org/document/7306956

[74] https://www.researchgate.net/publication/32711143_Why_Can_Agent_IQL_MARL_Succeed_at_Scale_When_Its_Policy_is_Non-Stationary_Online_Learning_in_Multi-Agent_Systems/links/5f2c9b4d3e5a6d7d8a0a1a

**Additional References**

[1] https://arxiv.org/abs/1905.06297
[73] https://ieeexplore.ieee.org/document/7306956
[74] https://www.researchgate.net/publication/32711143_Why_Can_Agent_IQL_MARL_Succeed_at_Scale_When_Its_Policy_is_Non-Stationary_Online_Learning_in_Multi-Agent_Systems/links/5f2c9b4d3e5a6d7d8a0a1a

**Summary**

The paper presents a new class of multi-agent reinforcement learning (MARL) algorithms for distributed control in a stochastic environment under partial observability. The authors introduce the concept of "Pareto selection" and argue that it is a fundamental challenge in MARL, which has been overlooked in previous literature. They show that the well-known problem of non-stationarity in MARL is not the only reason why the current state-of-the-art algorithms do not work at scale. The authors also introduce the concept of "convergence at scale" and argue that it is a fundamental challenge in MARL, which has been overlooked in previous literature as well. They show that the new class of optimisation-based learning performs significantly better across different numbers of prosumers than environment-based learning at scale. This superior performance requires computations to run optimisations on historical data, and to perform baseline simulations to compute marginal rewards, though computational time for pre-learning is not strictly a limiting factor as it is performed off-line ahead of implementation.

**Additional References**

[1] https://arxiv.org/abs/1905.06297
[73] https://ieeexplore.ieee.org/document/7306956
[74] https://www.researchgate.net/publication/32711143_Why_Can_Agent_IQL_MARL_Succeed_at_Scale_When_Its_Policy_is_Non-Stationary_Online_Learning_in_Multi-Agent_Systems/links/5f2c9b4d3e5a6d7d8a0a1a

**Summary**

The paper presents a new class of multi-agent reinforcement learning (MARL) algorithms for distributed control in a stochastic environment under partial observability. The authors introduce the concept of "Pareto selection" and argue that it is a fundamental challenge in MARL, which has been overlooked in previous literature. They show that the well-known problem of non-stationarity in MARL is not the only reason why the current state-of-the-art algorithms do not work at scale. The authors also introduce the concept of "convergence at scale" and argue that it is a fundamental challenge in MARL, which has been overlooked in previous literature as well. They show that the new class of optimisation-based learning performs significantly better across different numbers of prosumers than environment-based learning at scale. This superior performance requires computations to run optimisations on historical data, and to perform baseline simulations to compute marginal rewards, though computational time for pre-learning is not strictly a limiting factor as it is performed off-line ahead of implementation.

**Additional References**

[1] https://arxiv.org/abs/1905.06297
[73] https://ieeexplore.ieee.org/document/7306956
[74] https://www.researchgate.net/publication/32711143_Why_Can_Agent_IQL_MARL_Succeed_at_Scale_When_Its_Policy_is_Non-Stationary_Online_Learning_in_Multi-Agent_Systems/links/5f2c9b4d3e5a6d7d8a0a1a

**Summary**

The paper presents a new class of multi-agent reinforcement learning (MARL) algorithms for distributed control in a stochastic environment under partial observability. The authors introduce the concept of "Pareto selection" and argue that it is a fundamental challenge in MARL, which has been overlooked in previous literature. They show that the well-known problem of non-stationarity in MARL is not the only reason why the current state-of-the-art algorithms do not work at scale. The authors also introduce the concept of "convergence at scale" and argue that it is a fundamental challenge in MARL, which has been overlooked in previous literature as well. They show that the new class of optimisation-based learning performs significantly better across different numbers of prosumers than environment-based learning at scale. This superior performance requires computations to run optimisations on historical data, and to perform baseline simulations to compute marginal rewards, though computational time for pre-learning

---

**Summary Statistics:**
- Input: 13,704 words (87,292 chars)
- Output: 1,266 words
- Compression: 0.09x
- Generation: 68.2s (18.6 words/sec)
- Quality Score: 0.60/1.0
- Attempts: 1

**Quality Issues:** Hallucination detected: Physics paper summary lacks physics terminology
