# Upper bound for variational free energy of Bayesian networks

**Authors:** Kazuho Watanabe, Motoki Shiga, Sumio Watanabe

**Year:** 2009

**Source:** semanticscholar

**Venue:** Machine-mediated learning

**DOI:** 10.1007/s10994-008-5099-x

**PDF:** [watanabe2009upper.pdf](../pdfs/watanabe2009upper.pdf)

**Generated:** 2025-12-02 08:14:53

---

**Overview/Summary**

The paper provides an upper bound for the variational free energy of Bayesian networks (BNs) and assesses the accuracy of the variational approximation in terms of the generalization error. The variational free energy is a measure of the complexity of a model, which is defined as the expected value of the logarithmic loss function under the true distribution. The authors show that the upper bound for the variational free energy can be obtained by using the stochastic complexity (SC) and the identifiability of BNs. The SC is a measure of the complexity of a model, which is defined as the expected value of the logarithmic loss function under the true distribution. The identifiability is a property that measures whether the model is uniquely determined from the data. The authors show that the upper bound for the variational free energy can be obtained by using the SC and the identifiability of BNs. The SC is a measure of the complexity of a model, which is defined as the expected value of the logarithmic loss function under the true distribution. The identifiability is a property that measures whether the model is uniquely determined from the data.

**Key Contributions/Findings**

The main contributions and findings of this paper are:

- An upper bound for the variational free energy of Bayesian networks (BNs) is derived by using the SC and the identifiability of BNs. The authors show that the upper bound for the variational free energy can be obtained by using the SC and the identifiability of BNs.
- The accuracy of the variational approximation in terms of the generalization error is assessed, which is defined as the difference between the expected value of the logarithmic loss function under the true distribution and that under the approximate distribution. The authors show that the upper bound for the variational free energy can be obtained by using the SC and the identifiability of BNs.
- An upper bound for the variational free energy is derived in terms of the number of parameters, which is a measure of the complexity of a model.

**Methodology/Approach**

The authors use the stochastic complexity (SC) to obtain an upper bound for the variational free energy. The SC is a measure of the complexity of a model, which is defined as the expected value of the logarithmic loss function under the true distribution. The identifiability is a property that measures whether the model is uniquely determined from the data.

**Results/Data**

The authors show that the upper bound for the variational free energy can be obtained by using the SC and the identifiability of BNs. The results are presented in terms of the number of parameters, which is a measure of the complexity of a model.

**Limitations/Discussion**

The main limitation of this paper is the lack of lower bounds for the variational free energy as well as the upper bound. It is also important to assess the variational approximation in terms of the generalization error, or the accuracy of approximating the Bayesian predictive distributions in future studies.

**References**

1. Allen, T.V., & Greiner, R. (2000). Model selection criteria for learning belief nets: an empirical comparison. In Proceedings of international conference on machine learning (pp. 1047–1054).
2. Attias, H. (1999). Inferring parameters and structure of latent variable models by variational Bayes. In Proceedings of uncertainty in artiﬁcial intelligence (pp. 21–30). Stockholm, Sweden.
3. Beal, M.J. (2003). Variational algorithms for approximate Bayesian inference . Ph.D. Thesis, University College London.
4. Beal, M.J., & Ghahramani, Z. (2003). The variational Bayesian EM algorithm for incomplete data: with application to scoring graphical model structures. In Bayesian statistics (V ol. 7). London: Oxford University Press.
5. Friedman, N. (2004). Inferring cellular networks using probabilistic graphical models. Science, 303, 799–805.
6. Hinton, G., & van Camp, D. (1993). Keeping neural networks simple by minimizing the description length of the weights. In Proceedings of the sixth annual ACM conference on computational learning theory (pp. 5–13).
7. Hosino, T., Watanabe, K., & Watanabe, S. (2005). Stochastic complexity of variational Bayesian hidden Markov models. In Proceedings of international joint conference on neural networks( V ol. 2, pp. 1114–1119).
8. Jensen, F.V.  (2001).Bayesian networks and decision graphs. New York: Springer.
9. Jordan, M.I.  (1999). Learning in graphical models. Cambridge: MIT Press.
10. Mackay, D.J.  (1992). Bayesian interpolation. Neural Computation, 4(2), 415–447.
11. Meltzer, T., Yanover, C., & Weiss, Y.  (2005). Globally optimal solutions for energy minimization in stereo vision using reweighted belief propagation. In Proceedings of the tenth IEEE international conference on computer vision (pp. 428–435).
12. Rissanen, J.  (1986). Stochastic complexity and modeling. Annals of Statistics, 14(3), 1080–1100.
13. Rusakov, D., & Geiger, D.  (2005). Asymptotic model selection for naive Bayesian networks. Journal of Machine Learning Research, 6(1), 1–35.
14. Schwarz, G.  (1978). Estimating the dimension of a model. Annals of Statistics, 6(2), 461–464.
15. Wang, B., & Titterington, D.M.  (2004). Lack of consistency of mean ﬁeld and variational Bayes approximations. In Proceedings of the tenth international workshop on AISTATS (pp. 373–
380).
16. Wang, B., & Titterington, D.M.  (2005). Inadequacy of interval estimates corresponding to variational Bayesian approximations. In Proceedings of the tenth international workshop on AISTATS (pp. 373–
380).
17. Wang, B., & Titterington, D.M.  (2006). Convergence properties of a general algorithm for calculating variational Bayesian estimates for a normal mixture model. Bayesian Analysis, 1(3), 625–650.
18. Wang, B., & Titterington, D.M.  (2006). Convergence properties of a general algorithm for calculating variational Bayesian estimates for a normal mixture model. Bayesian Analysis, 1(3), 625–650.
19. Watanabe, S.  (2001). Algebraic analysis for non-identiﬁable learning machines. Neural Computation, 13(4), 899–933.
20. Watanabe, K., & Watanabe, S.  (2005). Stochastic complexity for mixture of exponential families in variational Bayes. In Proceedings of international conference on algorithmic learning theory (pp. 107–121). New York: Springer.
21. Watanabe, K., Shiga, M., & Watanabe, S.  (2006). Stochastic complexities of Gaussian mixtures in variational Bayesian approximation. Journal of Machine Learning Research, 7, 625–644.
22. Watanabe, K., & Watanabe, S.  (2006a). Variational Bayesian stochastic complexity of mixture models. In Advances in neural information processing systems (V ol. 18, pp. 1465–1472). Cambridge: MIT Press.
23. Watanabe, K., & Watanabe, S.  (2006b). Upper bounds for variational stochastic complexities of Bayesian networks. In Proceedings of international conference on intelligent data engineering and automated learning (pp. 139–146). New York: Springer.
24. Whiley, M., & Titterington, D.M.  (2002).Model identiﬁability in naive Bayesian networks( Tech. Rep. 02-1). Department of Statistics, University of Glasgow.

**References**

1. Allen, T.V., & Greiner, R. (2000). Model selection criteria for learning belief nets: an empirical comparison. In Proceedings of international conference on machine learning (pp. 1047–1054).
2. Attias, H. (1999). Inferring parameters and structure of latent variable models by variational Bayes. In Proceedings of uncertainty in artiﬁcial intelligence (pp. 21–30). Stockholm, Sweden.
3. Beal, M.J. (2003). Variational algorithms for approximate Bayesian inference . Ph.D. Thesis, University College London.
4. Beal, M.J., & Ghahramani, Z. (2003). The variational Bayesian EM algorithm for incomplete data: with application to scoring graphical model structures. In Bayesian statistics (V ol. 7). London: Oxford University Press.
5. Friedman, N. (2004). Inferring cellular networks using probabilistic graphical models. Science, 303, 799–805.
6. Hinton, G., & van Camp, D. (1993). Keeping neural networks simple by minimizing the description length of the weights. In Proceedings of the sixth annual ACM conference on computational learning theory (pp. 5–13).
7. Hosino, T., Watanabe, K., & Watanabe, S. (2005). Stochastic complexity of variational Bayesian hidden Markov models. In Proceedings of international joint conference on neural networks( V ol. 2, pp. 1114–1119).
8. Jensen, F.V.  (2001).Bay

---

**Summary Statistics:**
- Input: 6,334 words (35,866 chars)
- Output: 1,250 words
- Compression: 0.20x
- Generation: 69.0s (18.1 words/sec)
- Quality Score: 1.00/1.0
- Attempts: 1

**Quality Check:** Passed
