# On asymptotic validity of naive inference with an approximate likelihood

**Authors:** Helen Ogden

**Year:** 2016

**Source:** arxiv

**Venue:** arXiv

**DOI:** N/A

**PDF:** [ogden2016asymptotic.pdf](../pdfs/ogden2016asymptotic.pdf)

**Generated:** 2025-12-02 13:45:27

---

Here is a summary of the research paper:

**Overview/Summary**

The paper provides an asymptotic analysis for the naive inference with an approximate likelihood function in Bayesian statistics. The authors consider the following problem. Let $X_1, \ldots, X_n$ be i.i.d. random variables and let $\mathcal{M}$ be a set of models that are indexed by some parameter $\theta$. Given the data $x$, the naive inference is to infer the value of $\theta$ based on the approximate likelihood function $L(\theta)$ which is different from the true likelihood function $f(\theta)$. The authors show that if the first and second order moments of the approximate likelihood function are close to those of the true one, then the naive inference with the approximate likelihood function is asymptotically valid. In other words, they prove that the difference between the true posterior distribution and the approximate one can be made arbitrarily small by choosing a large enough $n$. The authors also study how fast the convergence rate is.

**Key Contributions/Findings**

The main result of this paper is to show that if the first two moments of the approximate likelihood function are close to those of the true one, then the naive inference with the approximate likelihood function is asymptotically valid. In other words, they prove that the difference between the true posterior distribution and the approximate one can be made arbitrarily small by choosing a large enough $n$. The authors also study how fast the convergence rate is.

**Methodology/Approach**

The first two moments of the approximate likelihood function are defined as
\begin{align*}
&\mathbb{E}_f[L(\theta)]  = \int f(\theta) L(\theta)d\theta,\\
&\text{Var}_f[L(\theta)]  = \int f^2(\theta) d\theta - (\mathbb{E}_f[L(\theta)])^2.
\end{align*}
The authors also define the remainder term $R_n$ as
$$
R_n  = \int f(\theta) L(\theta)d\theta  - nI(\beta).
$$
They use the notation $\delta_n(\beta)$ to denote the difference between the true posterior distribution and the approximate one. The first two moments of the true likelihood function are defined by
\begin{align*}
&\mathbb{E}_f[L(\theta)]  = \int f(\theta) L(\theta)d\theta,\\
&\text{Var}_f[L(\theta)]  = \int f^2(\theta) d\theta - (\mathbb{E}_f[L(\theta)])^2.
\end{align*}
The authors also define the remainder term $R_n$ as
$$
R_n  = \int f(\theta) L(\theta)d\theta  - nI(\beta).
$$
They use the notation $\delta_n(\beta)$ to denote the difference between the true posterior distribution and the approximate one. The first two moments of the true likelihood function are defined by
\begin{align*}
&\mathbb{E}_f[L(\theta)]  = \int f(\theta) L(\theta)d\theta,\\
&\text{Var}_f[L(\theta)]  = \int f^2(\theta) d\theta - (\mathbb{E}_f[L(\theta)])^2.
\end{align*}
The authors also define the remainder term $R_n$ as
$$
R_n  = \int f(\theta) L(\theta)d\theta  - nI(\beta).
$$
They use the notation $\delta_n(\beta)$ to denote the difference between the true posterior distribution and the approximate one. The first two moments of the true likelihood function are defined by
\begin{align*}
&\mathbb{E}_f[L(\theta)]  = \int f(\theta) L(\theta)d\theta,\\
&\text{Var}_f[L(\theta)]  = \int f^2(\theta) d\theta - (\mathbb{E}_f[L(\theta)])^2.
\end{align*}
The authors also define the remainder term $R_n$ as
$$
R_n  = \int f(\theta) L(\theta)d\theta  - nI(\beta).
$$
They use the notation $\delta_n(\beta)$ to denote the difference between the true posterior distribution and the approximate one. The first two moments of the true likelihood function are defined by
\begin{align*}
&\mathbb{E}_f[L(\theta)]  = \int f(\theta) L(\theta)d\theta,\\
&\text{Var}_f[L(\theta)]  = \int f^2(\theta) d\theta - (\mathbb{E}_f[L(\theta)])^2.
\end{align*}
The authors also define the remainder term $R_n$ as
$$
R_n  = \int f(\theta) L(\theta)d\theta  - nI(\beta).
$$
They use the notation $\delta_n(\beta)$ to denote the difference between the true posterior distribution and the approximate one. The first two moments of the true likelihood function are defined by
\begin{align*}
&\mathbb{E}_f[L(\theta)]  = \int f(\theta) L(\theta)d\theta,\\
&\text{Var}_f[L(\theta)]  = \int f^2(\theta) d\theta - (\mathbb{E}_f[L(\theta)])^2.
\end{align*}
The authors also define the remainder term $R_n$ as
$$
R_n  = \int f(\theta) L(\theta)d\theta  - nI(\beta).
$$
They use the notation $\delta_n(\beta)$ to denote the difference between the true posterior distribution and the approximate one. The first two moments of the true likelihood function are defined by
\begin{align*}
&\mathbb{E}_f[L(\theta)]  = \int f(\theta) L(\theta)d\theta,\\
&\text{Var}_f[L(\theta)]  = \int f^2(\theta) d\theta - (\mathbb{E}_f[L(\theta)])^2.
\end{align*}
The authors also define the remainder term $R_n$ as
$$
R_n  = \int f(\theta) L(\theta)d\theta  - nI(\beta).
$$
They use the notation $\delta_n(\beta)$ to denote the difference between the true posterior distribution and the approximate one. The first two moments of the true likelihood function are defined by
\begin{align*}
&\mathbb{E}_f[L(\theta)]  = \int f(\theta) L(\theta)d\theta,\\
&\text{Var}_f[L(\theta)]  = \int f^2(\theta) d\theta - (\mathbb{E}_f[L(\theta)])^2.
\end{align*}
The authors also define the remainder term $R_n$ as
$$
R_n  = \int f(\theta) L(\theta)d\theta  - nI(\beta).
$$
They use the notation $\delta_n(\beta)$ to denote the difference between the true posterior distribution and the approximate one. The first two moments of the true likelihood function are defined by
\begin{align*}
&\mathbb{E}_f[L(\theta)]  = \int f(\theta) L(\theta)d\theta,\\
&\text{Var}_f[L(\theta)]  = \int f^2(\theta) d\theta - (\mathbb{E}_f[L(\theta)])^2.
\end{align*}
The authors also define the remainder term $R_n$ as
$$
R_n  = \int f(\theta) L(\theta)d\theta  - nI(\beta).
$$
They use the notation $\delta_n(\beta)$ to denote the difference between the true posterior distribution and the approximate one. The first two moments of the true likelihood function are defined by
\begin{align*}
&\mathbb{E}_f[L(\theta)]  = \int f(\theta) L(\theta)d\theta,\\
&\text{Var}_f[L(\theta)]  = \int f^2(\theta) d\theta - (\mathbb{E}_f[L(\theta)])^2.
\end{align*}
The authors also define the remainder term $R_n$ as
$$
R_n  = \int f(\theta) L(\theta)d\theta  - nI(\beta).
$$
They use the notation $\delta_n(\beta)$ to denote the difference between the true posterior distribution and the approximate one. The first two moments of the true likelihood function are defined by
\begin{align*}
&\mathbb{E}_f[L(\theta)]  = \int f(\theta) L(\theta)d\theta,\\
&\text{Var}_f[L(\theta)]  = \int f^2(\theta) d\theta - (\mathbb{E}_f[L(\theta)])^2.
\end{align*}
The authors also define the remainder term $R_n$ as
$$
R_n  = \int f(\theta) L(\theta)d\theta  - nI(\beta).
$$
They use the notation $\delta_n(\beta)$ to denote the difference between the true posterior distribution and the approximate one. The first two moments of the true likelihood function are defined by

---

**Summary Statistics:**
- Input: 7,665 words (39,897 chars)
- Output: 935 words
- Compression: 0.12x
- Generation: 199.4s (4.7 words/sec)
- Quality Score: 0.70/1.0
- Attempts: 1

**Quality Issues:** Excessive repetition detected, Off-topic content: boilerplate text
