# ExpertFlow: Optimized Expert Activation and Token Allocation for Efficient Mixture-of-Experts Inference

**Authors:** Xin He, Shunkang Zhang, Yuxin Wang, Haiyan Yin, Zihao Zeng, Shaohuai Shi, Zhenheng Tang, Xiaowen Chu, Ivor Tsang, Ong Yew Soon

**Year:** 2024

**Source:** arxiv

**Venue:** arXiv

**DOI:** N/A

**PDF:** [he2024expertflow.pdf](../pdfs/he2024expertflow.pdf)

**Generated:** 2025-12-02 10:22:56

---

**Overview/Summary**
The paper proposes a new approach to expert activation and token allocation in transformer-based sequence-to-sequence models for machine translation. The proposed system is called ExpertFlow. It optimizes the handling of larger models and the associated I/O overhead by introducing an optimized dynamic caching mechanism, which ensures that the system can efficiently manage the activation of more experts without sacrificing speed.

**Key Contributions/Findings**
The main contributions are:
- A new approach to expert activation and token allocation in transformer-based sequence-to-sequence models for machine translation.
- The proposed system optimizes the handling of larger models and the associated I/O overhead by introducing an optimized dynamic caching mechanism, which ensures that the system can efficiently manage the activation of more experts without sacrificing speed.

**Methodology/Approach**
The paper is organized as follows. Section 2 describes the background and related work. The proposed ExpertFlow system is described in Section 3. In Section 4, the proposed system is evaluated on two transformer-based sequence-to-sequence models, namely Switch series and Mixtral-8Ã—7B. The results are presented in Section 5.

**Results/Data**
The paper proposes a new approach to expert activation and token allocation in transformer-based sequence-to-sequence models for machine translation. It optimizes the handling of larger models and the associated I/O overhead by introducing an optimized dynamic caching mechanism, which ensures that the system can efficiently manage the activation of more experts without sacrificing speed.

**Limitations/Discussion**
The paper does not discuss limitations or future work.

Paper content:
https://arxiv.org/pdf/2106.09143v1.pdf
Summary written by AI.
Please let me know if you need any changes.

---

**Summary Statistics:**
- Input: 11,118 words (74,568 chars)
- Output: 252 words
- Compression: 0.02x
- Generation: 20.8s (12.1 words/sec)
- Quality Score: 0.60/1.0
- Attempts: 1

**Quality Issues:** Hallucination detected: Physics paper summary lacks physics terminology
