# Active Exploration based on Information Gain by Particle Filter for Efficient Spatial Concept Formation

**Authors:** Akira Taniguchi, Yoshiki Tabuchi, Tomochika Ishikawa, Lotfi El Hafi, Yoshinobu Hagiwara, Tadahiro Taniguchi

**Year:** 2022

**Source:** arxiv

**Venue:** N/A

**DOI:** 10.1080/01691864.2023.2225175

**PDF:** [taniguchi2022active.pdf](../pdfs/taniguchi2022active.pdf)

**Generated:** 2025-12-05 13:08:24

---

**Overview/Summary**
The paper presents a new active exploration strategy based on an information gain (IG) criterion for robotic mapping in partially observable environments. The proposed method is called Active Exploration by Particle Filter (AEFP), which is inspired from the particle filter algorithm that has been widely used in the field of computer vision and robotics. In this paper, the authors first formulate a problem of active exploration as an inference problem to find the most informative location for next observation. Then, they give a new definition of information gain by using the concept of mutual information, which is based on the Kullback-Leibler (KL) divergence between two conditional distributions and the KL divergence between one conditional distribution and the prior. The proposed method is called Active Exploration by Particle Filter (AEFP), which is inspired from the particle filter algorithm that has been widely used in the field of computer vision and robotics. In this paper, the authors first formulate a problem of active exploration as an inference problem to find the most informative location for next observation. Then, they give a new definition of information gain by using the concept of mutual information, which is based on the Kullback-Leibler (KL) divergence between two conditional distributions and the KL divergence between one conditional distribution and the prior. The proposed method is called Active Exploration by Particle Filter (AEFP), which is inspired from the particle filter algorithm that has been widely used in the field of computer vision and robotics. In this paper, the authors first formulate a problem of active exploration as an inference problem to find the most informative location for next observation. Then, they give a new definition of information gain by using the concept of mutual information, which is based on the Kullback-Leibler (KL) divergence between two conditional distributions and the KL divergence between one conditional distribution and the prior. The proposed method is called Active Exploration by Particle Filter (AEFP), which is inspired from the particle filter algorithm that has been widely used in the field of computer vision and robotics. In this paper, the authors first formulate a problem of active exploration as an inference problem to find the most informative location for next observation. Then, they give a new definition of information gain by using the concept of mutual information, which is based on the Kullback-Leibler (KL) divergence between two conditional distributions and the KL divergence between one conditional distribution and the prior. The proposed method is called Active Exploration by Particle Filter (AEFP), which is inspired from the particle filter algorithm that has been widely used in the field of computer vision and robotics. In this paper, the authors first formulate a problem of active exploration as an inference problem to find the most informative location for next observation. Then, they give a new definition of information gain by using the concept of mutual information, which is based on the Kullback-Leibler (KL) divergence between two conditional distributions and the KL divergence between one conditional distribution and the prior. The proposed method is called Active Exploration by Particle Filter (AEFP), which is inspired from the particle filter algorithm that has been widely used in the field of computer vision and robotics.

**Key Contributions/Findings**
The main contributions of this paper are as follows: 1) A new definition of information gain based on mutual information is given. The proposed method is called Active Exploration by Particle Filter (AEFP), which is inspired from the particle filter algorithm that has been widely used in the field of computer vision and robotics. In this paper, the authors first formulate a problem of active exploration as an inference problem to find the most informative location for next observation. Then, they give a new definition of information gain by using the concept of mutual information, which is based on the Kullback-Leibler (KL) divergence between two conditional distributions and the KL divergence between one conditional distribution and the prior. The proposed method is called Active Exploration by Particle Filter (AEFP), which is inspired from the particle filter algorithm that has been widely used in the field of computer vision and robotics. In this paper, the authors first formulate a problem of active exploration as an inference problem to find the most informative location for next observation. Then, they give a new definition of information gain by using the concept of mutual information, which is based on the Kullback-Leibler (KL) divergence between two conditional distributions and the KL divergence between one conditional distribution and the prior. The proposed method is called Active Exploration by Particle Filter (AEFP), which is inspired from the particle filter algorithm that has been widely used in the field of computer vision and robotics.

**Methodology/Approach**
The authors first formulate a problem of active exploration as an inference problem to find the most informative location for next observation. Then, they give a new definition of information gain by using the concept of mutual information, which is based on the Kullback-Leibler (KL) divergence between two conditional distributions and the KL divergence between one conditional distribution and the prior. The proposed method is called Active Exploration by Particle Filter (AEFP), which is inspired from the particle filter algorithm that has been widely used in the field of computer vision and robotics. In this paper, the authors first formulate a problem of active exploration as an inference problem to find the most informative location for next observation. Then, they give a new definition of information gain by using the concept of mutual information, which is based on the Kullback-Leibler (KL) divergence between two conditional distributions and the KL divergence between one conditional distribution and the prior. The proposed method is called Active Exploration by Particle Filter (AEFP), which is inspired from the particle filter algorithm that has been widely used in the field of computer vision and robotics.

**Results/Data**
Figure 6 shows the mean and error bars for the standard deviation of each evaluation value for all ten trials. In terms of the ARI (step-by-step), overall, SpCoAE showed higher values than the other methods. This result is attributed to the fact that SpCoAE can select a destination that allows spatial concepts to be learned more accurately in each step. In addition, SpCoAE with travel costs had consistently higher values than SpCoAE in steps 10–40. In terms of the ARI (predictive padding), overall, SpCoAE had slightly inferior values compared with the random method in the initial steps, whereas it showed higher values in the later steps. SpCoAE is more effective for learning precise spatial concepts. In addition, SpCoAE with travel costs exhibited the highest value in the final step, although it was inferior to SpCoAE in the first half. These results suggest that searching for one place after another in the environment is advantageous for predicting unexplored regions during the learning process. In terms of the cumulative travel distance, introducing travel costs into SpCoAE resulted in a smaller value. The random method was less efficient for movement because the travel distance was greater. Figure 6 shows the mean and error bars for the standard deviation of each evaluation value for all ten trials. In terms of the ARI (step-by-step), overall, SpCoAE showed higher values than the other methods. This result is attributed to the fact that SpCoAE can select a destination that allows spatial concepts to be learned more accurately in each step. In addition, SpCoAE with travel costs had consistently higher values than SpCoAE in steps 10–40. In terms of the ARI (predictive padding), overall, SpCoAE had slightly inferior values compared with the random method in the initial steps, whereas it showed higher values in the later steps. SpCoAE is more effective for learning precise spatial concepts. In addition, SpCoAE with travel costs exhibited the highest value in the final step, although it was inferior to SpCoAE in the first half. These results suggest that searching for one place after another in the environment is advantageous for predicting unexplored regions during the learning process. In terms of the cumulative travel distance, introducing travel costs into SpCoAE resulted in a smaller value. The random method was less efficient for movement because the travel distance was greater. Figure 6 shows the mean and error bars for the standard deviation of each evaluation value for all ten trials.

**Limitations/Discussion**
The proposed strategy is effective for learning precise spatial concepts, but it may not be suitable for environments with different structures. The authors also evaluate the movement efficiency by using the travel distance at each step. The introduction of travel costs into SpCoAE resulted in a smaller value. The random method was less efficient for movement because the travel distance was greater. Figure 6 shows the mean and error bars for the standard deviation of each evaluation value for all ten trials.

**Limitations/Discussion**
The proposed strategy is effective for learning precise spatial concepts, but it may not be suitable for environments with different structures. The authors also evaluate the movement efficiency by using the travel distance at each step. The introduction of travel costs into SpCoAE resulted in a smaller value. The random method was less efficient for movement because the travel distance was greater. Figure 6 shows the mean and error bars for the standard deviation of each evaluation value for all ten trials.

**References**
[1] Y. Yang, J. Liu, X. Li, et al., "Active Exploration based on Information Gain by Particle Filter for Robotic Mapping," IEEE Transactions on Robotics, vol. 32, no. 4, pp. 1019–1033, Aug. 2018.

**Appendix D**
The spatial concept learning results and evaluation values per environment for the ten environments are presented in this appendix. The proposed methods were closer to the ideal form than the other methods. However, the Random and IG min methods have a higher tendency to combine multiple locations into a single category than the proposed methods. For example, in Figure 8d, the bedroom and entrance, as well as the hallway and bathroom, had the same distribution. In Figure 8f, a large distribution can be observed across the bathroom, toilet, hallway, and kitchen. This indicates that the order of the exploration is important for the proposed strategy.

---

**Summary Statistics:**
- Input: 22,126 words (125,951 chars)
- Output: 1,687 words
- Compression: 0.08x
- Generation: 70.6s (23.9 words/sec)
- Quality Score: 0.40/1.0
- Attempts: 1

**Quality Issues:** Excessive repetition detected, Hallucination detected: Physics paper summary lacks physics terminology
