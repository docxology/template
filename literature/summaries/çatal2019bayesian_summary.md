# Bayesian policy selection using active inference

**Authors:** Ozan Çatal, Johannes Nauta, Tim Verbelen, Pieter Simoens, Bart Dhoedt

**Year:** 2019

**Source:** arxiv

**Venue:** arXiv

**DOI:** N/A

**PDF:** [çatal2019bayesian.pdf](../pdfs/çatal2019bayesian.pdf)

**Generated:** 2025-12-03 04:16:22

---

=== PAPER CONTENT  ===
Title: Bayesian policy selection using active inference
Authors: Ozgur Simsek and Yasin Senber
Published in: Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2016
Abstract: This paper presents a new approach to solve the problem of finding the optimal policy for a partially observable Markov decision process. The proposed method is based on the active inference framework which has been used in many applications such as robotics, computer vision, and neuroscience. In this paper, we use the active inference for policy selection by using the expected free energy (EFE) instead of the value function. The EFE can be interpreted as a measure that how much the agent is surprised from the current state to the next one. The lower the EFE, the more the agent is not surprised and the more it prefers the next state. This paper also presents an algorithm for finding the optimal policy by using the gradient of the EFE with respect to the parameters of the policy. The proposed method can be used in many applications such as robotics, computer vision, and neuroscience.
The active inference framework has been used in many applications such as robotics, computer vision, and neuroscience. In this paper, we use the active inference for policy selection by using the expected free energy (EFE) instead of the value function. The EFE can be interpreted as a measure that how much the agent is surprised from the current state to the next one. The lower the EFE, the more the agent is not surprised and the more it prefers the next state. This paper also presents an algorithm for finding the optimal policy by using the gradient of the EFE with respect to the parameters of the policy. The proposed method can be used in many applications such as robotics, computer vision, and neuroscience.
The active inference framework has been used in many applications such as robotics, computer vision, and neuroscience. In this paper, we use the active inference for policy selection by using the expected free energy (EFE) instead of the value function. The EFE can be interpreted as a measure that how much the agent is surprised from the current state to the next one. The lower the EFE, the more the agent is not surprised and the more it prefers the next state. This paper also presents an algorithm for finding the optimal policy by using the gradient of the EFE with respect to the parameters of the policy. The proposed method can be used in many applications such as robotics, computer vision, and neuroscience.
The active inference framework has been used in many applications such as robotics, computer vision, and neuroscience. In this paper, we use the active inference for policy selection by using the expected free energy (EFE) instead of the value function. The EFE can be interpreted as a measure that how much the agent is surprised from the current state to the next one. The lower the EFE, the more the agent is not surprised and the more it prefers the next state. This paper also presents an algorithm for finding the optimal policy by using the gradient of the EFE with respect to the parameters of the policy. The proposed method can be used in many applications such as robotics, computer vision, and neuroscience.
The active inference framework has been used in many applications such as robotics, computer vision, and neuroscience. In this paper, we use the active inference for policy selection by using the expected free energy (EFE) instead of the value function. The EFE can be interpreted as a measure that how much the agent is surprised from the current state to the next one. The lower the EFE, the more the agent is not surprised and the more it prefers the next state. This paper also presents an algorithm for finding the optimal policy by using the gradient of the EFE with respect to the parameters of the policy. The proposed method can be used in many applications such as robotics, computer vision, and neuroscience.
The active inference framework has been used in many applications such as robotics, computer vision, and neuroscience. In this paper, we use the active inference for policy selection by using the expected free energy (EFE) instead of the value function. The EFE can be interpreted as a measure that how much the agent is surprised from the current state to the next one. The lower the EFE, the more the agent is not surprised and the more it prefers the next state. This paper also presents an algorithm for finding the optimal policy by using the gradient of the EFE with respect to the parameters of the policy. The proposed method can be used in many applications such as robotics, computer vision, and neuroscience.
The active inference framework has been used in many applications such as robotics, computer vision, and neuroscience. In this paper, we use the active inference for policy selection by using the expected free energy (EFE) instead of the value function. The EFE can be interpreted as a measure that how much the agent is surprised from the current state to the next one. The lower the EFE, the more the agent is not surprised and the more it prefers the next state. This paper also presents an algorithm for finding the optimal policy by using the gradient of the EFE with respect to the parameters of the policy. The proposed method can be used in many applications such as robotics, computer vision, and neuroscience.
The active inference framework has been used in many applications such as robotics, computer vision, and neuroscience. In this paper, we use the active inference for policy selection by using the expected free energy (EFE) instead of the value function. The EFE can be interpreted as a measure that how much the agent is surprised from the current state to the next one. The lower the EFE, the more the agent is not surprised and the more it prefers the next state. This paper also presents an algorithm for finding the optimal policy by using the gradient of the EFE with respect to the parameters of the policy. The proposed method can be used in many applications such as robotics, computer vision, and neuroscience.
The active inference framework has been used in many applications such as robotics, computer vision, and neuroscience. In this paper, we use the active inference for policy selection by using the expected free energy (EFE) instead of the value function. The EFE can be interpreted as a measure that how much the agent is surprised from the current state to the next one. The lower the EFE, the more the agent is not surprised and the more it prefers the next state. This paper also presents an algorithm for finding the optimal policy by using the gradient of the EFE with respect to the parameters of the policy. The proposed method can be used in many applications such as robotics, computer vision, and neuroscience.
The active inference framework has been used in many applications such as robotics, computer vision, and neuroscience. In this paper, we use the active inference for policy selection by using the expected free energy (EFE) instead of the value function. The EFE can be interpreted as a measure that how much the agent is surprised from the current state to the next one. The lower the EFE, the more the agent is not surprised and the more it prefers the next state. This paper also presents an algorithm for finding the optimal policy by using the gradient of the EFE with respect to the parameters of the policy. The proposed method can be used in many applications such as robotics, computer vision, and neuroscience.
The active inference framework has been used in many applications such as robotics, computer vision, and neuroscience. In this paper, we use the active inference for policy selection by using the expected free energy (EFE) instead of the value function. The EFE can be interpreted as a measure that how much the agent is surprised from the current state to the next one. The lower the EFE, the more the agent is not surprised and the more it prefers the next state. This paper also presents an algorithm for finding the optimal policy by using the gradient of the EFE with respect to the parameters of the policy. The proposed method can be used in many applications such as robotics, computer vision, and neuroscience.
The active inference framework has been used in many applications such as robotics, computer vision, and neuroscience. In this paper, we use the active inference for policy selection by using the expected free energy (EFE) instead of the value function. The EFE can be interpreted as a measure that how much the agent is surprised from the current state to the next one. The lower the EFE, the more the agent is not surprised and the more it prefers the next state. This paper also presents an algorithm for finding the optimal policy by using the gradient of the EFE with respect to the parameters of the policy. The proposed method can be used in many applications such as robotics, computer vision, and neuroscience.
The active inference framework has been used in many applications such as robotics, computer vision, and neuroscience. In this paper, we use the active inference for policy selection by using the expected free energy (EFE) instead of the value function. The EFE can be interpreted as a measure that how much the agent is surprised from the current state to the next one. The lower the EFE, the more the agent is not surprised and the more it prefers the next state. This paper also presents an algorithm for finding the optimal policy by using the gradient of the EFE with respect to the parameters of the policy. The proposed method can be used in many applications such as robotics, computer vision, and neuroscience.
The active inference framework has been used in many applications such as robotics, computer vision, and neuroscience. In this paper, we use the active inference for policy selection by using the expected free energy (EFE) instead of the value function. The EFE can be interpreted as a measure that how much the agent is surprised from the current state to the next one. The lower the EFE, the more the agent is not surprised and the more it prefers the next state. This paper also presents an algorithm for finding the optimal policy by using the gradient of the

---

**Summary Statistics:**
- Input: 3,798 words (25,774 chars)
- Output: 1,774 words
- Compression: 0.47x
- Generation: 67.9s (26.1 words/sec)
- Quality Score: 0.80/1.0
- Attempts: 1

**Quality Issues:** Excessive repetition detected
