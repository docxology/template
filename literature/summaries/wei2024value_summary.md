# Value of Information and Reward Specification in Active Inference and POMDPs

**Authors:** Ran Wei

**Year:** 2024

**Source:** arxiv

**Venue:** arXiv

**DOI:** N/A

**PDF:** [wei2024value.pdf](../pdfs/wei2024value.pdf)

**Generated:** 2025-12-03 05:33:56

---

**Overview/Summary**

The paper studies the theoretical connection between active inference and reinforcement learning (RL), showing that the epistemic value in the expected free energy (EFE) objective of active inference can be seen as an approximation to the Bayes optimal RL policy in partially observable Markov decision processes (POMDPs). The results also suggest that, from the perspective of RL, the specification of EFE needs to balance reward with information gain in the environment, via an appropriate temperature parameter ($\lambda$). Conversely, from the perspective of active inference, an EFE minimizing agent will pursue a Bayes optimal RL policy, under a suitable $\lambda$. The paper also discusses the notion of "Bayes optimal" in the spirit of active inference (in closed-loop), as well as extensions of POMDPs. It appears that the notion of "Bayes optimal" in the spirit of active inference (in closed-loop), as well as extensions of POMDPs, may not be restricted to the usual sense of Bayesian decision theory (i.e., maximizing utility; Howard 1966; Raiffa and Schlaifer 2000; Berger 2013); Berger 2013).

**Key Contributions/Findings**

The paper shows that the epistemic value in the EFE objective of active inference can be seen as an approximation to the Bayes optimal RL policy in POMDPs, achieving a linear improvement in regret compared to a naive policy which doesn't take into account the value of information. The results also suggest that, from the perspective of RL, the specification of EFE needs to balance reward with information gain in the environment, via an appropriate $\lambda$. Conversely, from the perspective of active inference, an EFE minimizing agent will pursue a Bayes optimal RL policy, under a suitable $\lambda$.

**Methodology/Approach**

The paper studies the theoretical connection between active inference and reinforcement learning (RL), showing that the epistemic value in the EFE objective of active inference can be seen as an approximation to the Bayes optimal RL policy in partially observable Markov decision processes (POMDPs). The results also suggest that, from the perspective of RL, the specification of EFE needs to balance reward with information gain in the environment, via an appropriate $\lambda$. Conversely, from the perspective of active inference, an EFE minimizing agent will pursue a Bayes optimal RL policy, under a suitable $\lambda$.

**Results/Data**

The paper does not present any new data or results. It only presents the theoretical analysis and findings that are based on existing work in the field.

**Limitations/Discussion**

The paper does not discuss limitations of the research. The authors suggest that the notion of "Bayes optimal" in the spirit of active inference (in closed-loop), as well as extensions of POMDPs, may not be restricted to the usual sense of Bayesian decision theory (i.e., maximizing utility; Howard 1966; Raiffa and Schlaifer 2000; Berger 2013). The authors also mention that one reading of the complete class theorem (Wald 1947; Brown 1981) is that, for any pair of reward function and choices, there exists some prior beliefs that render the choices Bayes optimal, in a decision theoretic sense (Berger 2013).

**References**

Agarwal, A., Jiang, N., Kakade, S. M., and Sun, W. Reinforcement learning: Theory and algorithms. CS Dept., UW Seattle, Seattle, WA, USA, T ech. Rep , 32:96, 2019.
Araya, M., Buffet, O., Thomas, V., and Charpillet, F. A pomdp ext ension with belief-dependent rewards. Advances in neural information processing systems , 23, 2010.
Barp, A., Da Costa, L., França, G., Girolami, M. I., Jordan, M. I., and Pavliotis, G. A. Geometric methods for sampling, optimization, inference, and adaptive agents. In Handbook of Statistics , volume 46, pages 21–78. Elsevier, 2022.
Belghazi, M. I., Baratin, A., Rajeshwar, S., Ozair, Y., Bengio, Y., Courville, A., and Hjelm, D. Mutual information neural estimation. In International conference on machine learning , pages 531–540. PMLR, 2018.
Doshi-Velez, F., and Konidaris, G. Hidden parameter markov de cision processes: A semiparametric regression approach for discovering latent task parametrizations. In IJCAI: proceedings of the conference , volume 2016, page 1432. NIH Public Access, 2016.
Duan, Y., Schulman, J., Chen, X., Bartlett, P. L., and Abbeel, P. rl2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779 , 2016.
Friston, K., Da Costa, L., Parr, T., Sajid, N., Veselic, S., Neacsu, V., and Smith, R. Active inference on discrete state-spaces: A synthesis. Journal of Mathematical Psychology , 99:102447, 2020.
Sajid, N., Tenka, S., Da Costa, L., Parr, T., and Friston, K. Active inference as a model of agency. arXiv preprint arXiv:2401.12917 , 2024.

**Additional Information**

The paper does not present any additional information that is not already in the text.

---

**Summary Statistics:**
- Input: 16,673 words (89,335 chars)
- Output: 746 words
- Compression: 0.04x
- Generation: 47.1s (15.8 words/sec)
- Quality Score: 1.00/1.0
- Attempts: 1

**Quality Check:** Passed
