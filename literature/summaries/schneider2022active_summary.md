# Active Inference for Robotic Manipulation

**Authors:** Tim Schneider, Boris Belousov, Hany Abdulsamad, Jan Peters

**Year:** 2022

**Source:** arxiv

**Venue:** arXiv

**DOI:** N/A

**PDF:** [schneider2022active.pdf](../pdfs/schneider2022active.pdf)

**Generated:** 2025-12-03 03:26:21

---

**Overview/Summary**
The paper presents a method for applying Active Inference (AI) to complex Reinforcement Learning (RL) tasks in robotic manipulation. The authors propose an approach that can be used on any RL problem that is formulated as a Markov Decision Process, and they evaluate their method in two challenging robotic manipulation tasks. They show that the information-seeking behavior of their agents is beneficial for solving these exploration problems with sparse rewards.

**Key Contributions/Findings**
The key contributions are the development of an AI-based approach to complex RL tasks and its application to a real-world problem. The authors' main findings are that the method induces systematic exploration behavior and is capable of solving even the most challenging of the two environments. Neither the non-intrinsic conﬁgurations nor the maximum entropy method SAC managed to solve the robotic manipulation tasks.

**Methodology/Approach**
The approach developed by the authors is a variant of the Active Inference (AI) framework, which was originally proposed for solving exploration problems in cognitive science and neuroscience. The AI-based RL problem formulation is that of a Markov Decision Process (MDP). The authors' main methodological contribution is the application of this AI to complex RL tasks. They use SAC as a baseline to compare their approach with.

**Results/Data**
The results show that the method induces systematic exploration behavior and is capable of solving even the most challenging of the two environments. Neither the non-intrinsic conﬁgurations nor the maximum entropy method SAC managed to solve the robotic manipulation tasks. The authors' main data contributions are the comparison of their approach with the baseline in terms of the number of episodes required for the agent to learn and the cumulative reward per episode.

**Limitations/Discussion**
The limitations of this work are that the computation of the intrinsic term is computationally heavy, limiting them to a rather low control frequency. The authors' main discussion point is that the information-seeking behavior of their agents is beneﬁcial for solving challenging exploration problems with sparse rewards. They also mention future work in applying their method to a real robot and evaluating whether AI can be used in real robotic manipulation tasks.

**References**
The references are the list of papers cited by the authors, which appear at the end of the paper text above.

---

**Summary Statistics:**
- Input: 2,791 words (17,875 chars)
- Output: 373 words
- Compression: 0.13x
- Generation: 23.8s (15.7 words/sec)
- Quality Score: 1.00/1.0
- Attempts: 1

**Quality Check:** Passed
