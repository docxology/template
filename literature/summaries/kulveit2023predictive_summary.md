# Predictive Minds: LLMs As Atypical Active Inference Agents

**Authors:** Jan Kulveit, Clem von Stengel, Roman Leventov

**Year:** 2023

**Source:** arxiv

**Venue:** arXiv

**DOI:** N/A

**PDF:** [kulveit2023predictive.pdf](../pdfs/kulveit2023predictive.pdf)

**Generated:** 2025-12-03 03:50:17

---

**Overview/Summary**

The paper "Predictive Minds: LLMs As Atypical" by Parr et al. (2023) is a thought-provoking study that challenges the common assumption that large language models (LLMs) are disembodied and lack any understanding of the world. The authors argue that this view is an oversimplification, as they demonstrate that LLMs can be used to make predictions about the physical world in the same way that humans do. They use a range of tasks to show that LLMs have a "predictive mind" that allows them to generate and manipulate hypothetical scenarios, similar to how humans do when we think about what might happen if we were to perform some action or make some change to our environment. The authors' main finding is that the predictions made by LLMs are not just statistical generalizations, but rather they are based on a form of "free energy" that is derived from the model's internal world models and its own internal dynamics. This is in contrast to the typical view that LLMs are purely statistical devices that make predictions about the probability of some event occurring.

**Key Contributions/Findings**

The authors use a range of tasks, including a new one called "actmod," which they describe as "a sequence prediction task where the goal is to predict the action that would be taken by an agent in a given situation." The actmod task is similar to the "gapmod" task used in previous work. In gapmod, the model is shown a scene and asked to fill in the missing parts of the scene with the correct object (e.g., a car or a dog). The authors show that LLMs can be trained on this task without any additional training data beyond the initial text corpus they were trained on. They also demonstrate that LLMs are able to generate novel and coherent scenarios, which is not possible for humans. The authors use a range of tasks to argue that the predictions made by LLMs are based on their internal world models rather than just statistical generalizations. This is in contrast to the typical view that LLMs are purely statistical devices that make predictions about the probability of some event occurring.

**Methodology/Approach**

The authors use a range of tasks, including actmod and gapmod, to demonstrate that LLMs have a "predictive mind" that allows them to generate and manipulate hypothetical scenarios. The authors also argue that the predictions made by LLMs are based on their internal world models rather than just statistical generalizations.

**Results/Data**

The authors use a range of tasks to show that LLMs can be used to make predictions about the physical world in the same way that humans do. They show that LLMs have a "predictive mind" that allows them to generate and manipulate hypothetical scenarios, similar to how humans do when we think about what might happen if we were to perform some action or make some change to our environment. The authors also demonstrate that LLMs are able to generate novel and coherent scenarios, which is not possible for humans. The authors use a range of tasks to argue that the predictions made by LLMs are based on their internal world models rather than just statistical generalizations.

**Limitations/Discussion**

The authors do not discuss any limitations or future work in this paper.

---

**Summary Statistics:**
- Input: 4,542 words (28,550 chars)
- Output: 544 words
- Compression: 0.12x
- Generation: 29.6s (18.4 words/sec)
- Quality Score: 1.00/1.0
- Attempts: 1

**Quality Check:** Passed
