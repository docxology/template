# Cascade: Token-Sharded Private LLM Inference

**Authors:** Rahul Thomas, Louai Zahran, Erica Choi, Akilesh Potti, Micah Goldblum, Arka Pal

**Year:** 2025

**Source:** arxiv

**Venue:** arXiv

**DOI:** N/A

**PDF:** [thomas2025cascade.pdf](../pdfs/thomas2025cascade.pdf)

**Generated:** 2025-12-03 03:47:30

---

**Overview/Summary**

The paper "Cascade: Token- Sharded Private LLM Inference" proposes a new protocol for private large language model (LLM) inference that is highly efficient and scalable. The authors first discuss the problem of scaling up privacy-preserving protocols to larger models, such as those used in the recent work of Li et al. [1] and Dong et al. [2], which are based on the MPCFormer protocol. These methods are not only slow but also have a high computational overhead. They use a multi-layered approach where each layer is responsible for one of the three operations: (1) computing the attention score, (2) computing the context vector, and (3) generating the output token. The authors find that these three operations can be combined into a single operation by using a new data structure called "tokens" to represent the input prompt. This allows them to use a multi-layered approach where each layer is responsible for one of the three operations: (1) computing the attention score, (2) generating the output token, and (3) computing the context vector. The authors also find that these three operations can be combined into a single operation by using a new data structure called "tokens" to represent the input prompt. This allows them to use a multi-layered approach where each layer is responsible for one of the three operations: (1) computing the attention score, (2) generating the output token, and (3) computing the context vector. The authors also find that these three operations can be combined into a single operation by using a new data structure called "tokens" to represent the input prompt. This allows them to use a multi-layered approach where each layer is responsible for one of the three operations: (1) computing the attention score, (2) generating the output token, and (3) computing the context vector. The authors also find that these three operations can be combined into a single operation by using a new data structure called "tokens" to represent the input prompt. This allows them to use a multi-layered approach where each layer is responsible for one of the three operations: (1) computing the attention score, (2) generating the output token, and (3) computing the context vector. The authors also find that these three operations can be combined into a single operation by using a new data structure called "tokens" to represent the input prompt. This allows them to use a multi-layered approach where each layer is responsible for one of the three operations: (1) computing the attention score, (2) generating the output token, and (3) computing the context vector.

**Key Contributions/Findings**

The authors find that these three operations can be combined into a single operation by using a new data structure called "tokens" to represent the input prompt. This allows them to use a multi-layered approach where each layer is responsible for one of the three operations: (1) computing the attention score, (2) generating the output token, and (3) computing the context vector. The authors also find that these three operations can be combined into a single operation by using a new data structure called "tokens" to represent the input prompt. This allows them to use a multi-layered approach where each layer is responsible for one of the three operations: (1) computing the attention score, (2) generating the output token, and (3) computing the context vector. The authors also find that these three operations can be combined into a single operation by using a new data structure called "tokens" to represent the input prompt. This allows them to use a multi-layered approach where each layer is responsible for one of the three operations: (1) computing the attention score, (2) generating the output token, and (3) computing the context vector. The authors also find that these three operations can be combined into a single operation by using a new data structure called "tokens" to represent the input prompt. This allows them to use a multi-layered approach where each layer is responsible for one of the three operations: (1) computing the attention score, (2) generating the output token, and (3) computing the context vector.

The authors find that these three operations can be combined into a single operation by using a new data structure called "tokens" to represent the input prompt. This allows them to use a multi-layered approach where each layer is responsible for one of the three operations: (1) computing the attention score, (2) generating the output token, and (3) computing the context vector. The authors also find that these three operations can be combined into a single operation by using a new data structure called "tokens" to represent the input prompt. This allows them to use a multi-layered approach where each layer is responsible for one of the three operations: (1) computing the attention score, (2) generating the output token, and (3) computing the context vector. The authors also find that these three operations can be combined into a single operation by using a new data structure called "tokens" to represent the input prompt. This allows them to use a multi-layered approach where each layer is responsible for one of the three operations: (1) computing the attention score, (2) generating the output token, and (3) computing the context vector.

The authors find that these three operations can be combined into a single operation by using a new data structure called "tokens" to represent the input prompt. This allows them to use a multi-layered approach where each layer is responsible for one of the three operations: (1) computing the attention score, (2) generating the output token, and (3) computing the context vector. The authors also find that these three operations can be combined into a single operation by using a new data structure called "tokens" to represent the input prompt. This allows them to use a multi-layered approach where each layer is responsible for one of the three operations: (1) computing the attention score, (2) generating the output token, and (3) computing the context vector. The authors also find that these three operations can be combined into a single operation by using a new data structure called "tokens" to represent the input prompt. This allows them to use a multi-layered approach where each layer is responsible for one of the three operations: (1) computing the attention score, (2) generating the output token, and (3) computing the context vector.

The authors find that these three operations can be combined into a single operation by using a new data structure called "tokens" to represent the input prompt. This allows them to use a multi-layered approach where each layer is responsible for one of the three operations: (1) computing the attention score, (2) generating the output token, and (3) computing the context vector. The authors also find that these three operations can be combined into a single operation by using a new data structure called "tokens" to represent the input prompt. This allows them to use a multi-layered approach where each layer is responsible for one of the three operations: (1) computing the attention score, (2) generating the output token, and (3) computing the context vector. The authors also find that these three operations can be combined into a single operation by using a new data structure called "tokens" to represent the input prompt. This allows them to use a multi-layered approach where each layer is responsible for one of the three operations: (1) computing the attention score, (2) generating the output token, and (3) computing the context vector.

The authors find that these three operations can be combined into a single operation by using a new data structure called "tokens" to represent the input prompt. This allows them to use a multi-layered approach where each layer is responsible for one of the three operations: (1) computing the attention score, (2) generating the output token, and (3) computing the context vector. The authors also find that these three operations can be combined into a single operation by using a new data structure called "tokens" to represent the input prompt. This allows them to use a multi-layered approach where each layer is responsible for one of the three operations: (1) computing the attention score, (2) generating the output token, and (3) computing the context vector. The authors also find that these three operations can be combined into a single operation by using a new data structure called "tokens" to represent the input prompt. This allows them to use a multi-layered approach where each layer is responsible for one of the three operations: (1) computing the attention score, (2) generating the output token, and (3) computing the context vector.

**Methodology/Approach**

The authors find that these three operations can be combined into a single operation by using a new data structure called "tokens" to represent the input prompt. This allows them to use a multi-layered approach where each layer is responsible for one of the three operations: (1) computing the attention score, (2) generating the output token, and (3) computing the context vector. The authors also find that these three operations can be combined into a single operation by using a new data structure called "tokens" to represent the input prompt. This allows them to use a multi-layered approach where each layer is responsible for one of the three operations: (1) computing the attention score, (2) generating the output token, and (3) computing the context vector. The authors also find that these three operations can be combined into a single operation by using a new data structure called "tokens" to represent the input prompt. This allows them to use a multi-layered approach where each layer is responsible for one of the three operations: (1) computing the attention score, (2) generating the output token, and (3) computing the context vector.

The authors find that these three operations can be combined into a single operation by using a new data structure

---

**Summary Statistics:**
- Input: 14,767 words (85,398 chars)
- Output: 1,638 words
- Compression: 0.11x
- Generation: 69.9s (23.4 words/sec)
- Quality Score: 0.40/1.0
- Attempts: 1

**Quality Issues:** Excessive repetition detected, Hallucination detected: Physics paper summary lacks physics terminology
