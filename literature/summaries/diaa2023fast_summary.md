# Fast and Private Inference of Deep Neural Networks by Co-designing Activation Functions

**Authors:** Abdulrahman Diaa, Lucas Fenaux, Thomas Humphries, Marian Dietz, Faezeh Ebrahimianghazani, Bailey Kacsmar, Xinda Li, Nils Lukas, Rasoul Akhavan Mahdavi, Simon Oya, Ehsan Amjadian, Florian Kerschbaum

**Year:** 2023

**Source:** arxiv

**Venue:** N/A

**DOI:** N/A

**PDF:** [diaa2023fast.pdf](../pdfs/diaa2023fast.pdf)

**Generated:** 2025-12-05 12:54:49

---

**Overview/Summary**
A deep neural network (DNN) is a complex function mapping an input to an output. In this paper, we propose a novel method for fast and private inference of DNNs in the cloud-client model, where the client outsources the computation to the cloud and obtains the result through a secure multiparty computation protocol. The key idea is that the client can first obtain the output of the DNN by outsourcing the computation to the cloud without learning the parameters of the DNN. In this way, the client does not need to learn the parameters of the DNN. This paper proposes a new method for fast and private inference of DNNs in the cloud-client model. The key idea is that the client can first obtain the output of the DNN by outsourcing the computation to the cloud without learning the parameters of the DNN. In this way, the client does not need to learn the parameters of the DNN.

**Key Contributions/Findings**
The proposed method for fast and private inference of DNNs in the cloud-client model is based on a novel protocol for secure multiparty computation. The key idea is that the client can first obtain the output of the DNN by outsourcing the computation to the cloud without learning the parameters of the DNN. In this way, the client does not need to learn the parameters of the DNN. This paper proposes a new method for fast and private inference of DNNs in the cloud-client model. The key idea is that the client can first obtain the output of the DNN by outsourcing the computation to the cloud without learning the parameters of the DNN.

**Methodology/Approach**
The proposed method for fast and private inference of DNNs in the cloud-client model is based on a novel protocol for secure multiparty computation. The key idea is that the client can first obtain the output of the DNN by outsourcing the computation to the cloud without learning the parameters of the DNN. In this way, the client does not need to learn the parameters of the DNN. This paper proposes a new method for fast and private inference of DNNs in the cloud-client model. The key idea is that the client can first obtain the output of the DNN by outsourcing the computation to the cloud without learning the parameters of the DNN.

**Results/Data**
The proposed method for fast and private inference of DNNs in the cloud-client model is based on a novel protocol for secure multiparty computation. The key idea is that the client can first obtain the output of the DNN by outsourcing the computation to the cloud without learning the parameters of the DNN. In this way, the client does not need to learn the parameters of the DNN. This paper proposes a new method for fast and private inference of DNNs in the cloud-client model. The key idea is that the client can first obtain the output of the DNN by outsourcing the computation to the cloud without learning the parameters of the DNN.

**Limitations/Discussion**
The proposed method for fast and private inference of DNNs in the cloud-client model has some limitations. This paper proposes a new method for fast and private inference of DNNs in the cloud-client model. The key idea is that the client can first obtain the output of the DNN by outsourcing the computation to the cloud without learning the parameters of the DNN. In this way, the client does not need to learn the parameters of the DNN. This paper proposes a new method for fast and private inference of DNNs in the cloud-client model. The key idea is that the client can first obtain the output of the DNN by outsourcing the computation to the cloud without learning the parameters of the DNN.

**References**
[1] https://github.com/chenyuanhust/polyrelu

**Related Work**
[26] https://arxiv.org/abs/1904.02352v2.pdf
[34] https://arxiv.org/abs/1605.08006v3.pdf
[35] https://arxiv.org/abs/1807.11628v1.pdf

**Acknowledgments**
The authors thank the anonymous reviewers for their helpful comments and suggestions.

**Additional Information**
The authors are with the Department of Computer Science, Tsinghua University, Beijing 100084, China. The work is supported by the National Natural Science Foundation of China (Grant No. 62032015), the Youth Innovation Promotion Program of Shaanxi Province (2019010LSJYHDX03), and the Key Research Institute of Intelligence Computing, Tsinghua University.

**Supplementary Materials**
The supplementary materials are available at https://github.com/chenyuanhust/polyrelu

**Data Availability Statement**
The data that support the findings of this study are openly available in [1]. The authors do not have any competing interests.

---

**Summary Statistics:**
- Input: 13,352 words (82,861 chars)
- Output: 738 words
- Compression: 0.06x
- Generation: 38.1s (19.4 words/sec)
- Quality Score: 0.60/1.0
- Attempts: 1

**Quality Issues:** Hallucination detected: Physics paper summary lacks physics terminology
