# ML-Schema: Exposing the Semantics of Machine Learning with Schemas and Ontologies

**Authors:** Gustavo Correa Publio, Diego Esteves, Agnieszka Ławrynowicz, Panče Panov, Larisa Soldatova, Tommaso Soru, Joaquin Vanschoren, Hamid Zafar

**Year:** 2018

**Source:** arxiv

**Venue:** N/A

**DOI:** N/A

**PDF:** [publio2018mlschema.pdf](../pdfs/publio2018mlschema.pdf)

**Generated:** 2025-12-05 10:27:01

---

**Overview/Summary**

The paper introduces ML-SCHEMA, a lightweight schema for modeling the Machine Learning (ML) domain. The authors argue that the existing ontologies and vocabularies in this area are too coarse-grained to provide explicit semantics of the ML models, making them difficult to interpret by human users. They propose a new vocabulary for exposing the semantics of the ML models, which aligns more fine-grained ontologies and vocabularies. The authors also argue that these existing resources may be used to make the semantics of the ML models better interpretable for human users.

**Key Contributions/Findings**

The main contribution of this paper is the introduction of a new vocabulary for exposing the semantics of the ML models, which aligns more fine-grained ontologies and vocabularies. The authors show that these existing resources may be used to make the semantics of the ML models better interpretable by human users.

**Methodology/Approach**

The authors first present an overview of the existing vocabularies for exchanging machine learning metadata in a high level of interoperability, which is designed to provide a simple and lightweight vocabulary for achieving this goal. The principal components are: mex-ALGO4, which describes algorithms and hyperparameters; mex-CORE5, which is the basis to describe an experiment, its configurations and executions; and mex-PERF6, the layer to map the outcomes (i.e., performance measures). They also present a new vocabulary for exposing the semantics of the ML models. The authors show that these existing resources may be used to make the semantics of the ML models better interpretable by human users.

**Results/Data**

The paper does not contain any results or data, as it is a theoretical study on how to expose the semantics of the ML models and align them with more fine-grained ontologies and vocabularies. The authors do not present any experimental results in this paper. The new vocabulary for exposing the semantics of the ML models may be used to make the semantics of the ML models better interpretable by human users.

**Limitations/Discussion**

The main weakness of the existing vocabularies is that they are too coarse-grained to provide explicit semantics of the ML models, making them difficult to interpret by human users. The authors do not mention any future work or limitations in this paper.

**References**

[1] Robert Arp, Barry Smith, and Andrew D. Spear. Building Ontologies with Basic Formal Ontology. The MIT Press, 2015.
[2] Anita Bandrowski, Ryan Brinkman, Mathias Brochhausen, Matthew H. Brush, Bill Bug, Marcus C. Chibucos, Kevin Clancy, Mélanie Courtot, Dirk Derom, Michel Dumontier, Liju Fan, Gilberto Fragoso, Frank Gibson, Alejandra Gonzalez-Beltran, Melissa A. Haendel, Yongqun He, Mervi Heiskanen, Tina Hernandez-Boussard, Mark Jensen, Yu Lin, Allyson L. Lister, Phillip Lord, James Malone, Elisabetta Manduchi, Monnie McGee, Norman Morrison, James A. Overton, Helen Parkinson, Bjoern Peters, Philippe Rocca-Serra, Alan Ruttenberg, Susanna-Asunta Sansone, Richard H. Scheuermann, Daniel Schober, Barry Smith, Larisa N. Soldatova, Christian J. Stoeckert, Jr., Chris F. Taylor, Carlo Torniai, Jessica A. Turner, Randi Vita, and Jie Zheng. The ontology for biomedical investigations. PLoS ONE, 11(4):1–19, 04 2016.
[3] Diego Esteves, Agnieszka Lawrynowicz, Pance Panov, Larisa N. Soldatova, Tommaso Soru, and Joaquin Vanschoren. Ml schema core specification. Technical report, W3C, 10 2016. http://www.w3.org/2016/10/mls/.
[4] Diego Esteves, Diego Moussallem, Ciro Baron Neto, Tommaso Soru, Ricardo Usbeck, Markus Ackermann, and Jens Lehmann. MEX vocabulary: a lightweight interchange format for machine learning experiments. In Proceedings of the 11th International Conference on Semantic Systems, SEMANTICS 2015, Vienna, Austria, September 15-17, 2015, pages 169–176, 2015.
[5] C. Maria Keet, Agnieszka Lawrynowicz, Claudia d’Amato, Alexandros Kalousis, Phong Nguyen, Raúl Palma, Robert Stevens, and Melanie Hilario. The data mining optimization ontology. J. Web Sem., 32:43–53, 2015.
[6] Timothy Lebo, Satya Sahoo, Deborah McGuinness, Khalid Belhajjame, James Cheney, David Corsar, Daniel Garijo, Stian Soiland-Reyes, Stephan Zednik, and Jun Zhao. Prov-o: The prov ontology. W3C Recommendation, 30, 2013.
[7] Ciro Baron Neto, Diego Esteves, Tommaso Soru, Diego Moussallem, Andre Valdestilhas, and Edgard Marx. WASOTA: What are the states of the art? In SEMANTiCS (Posters, Demos, SuCCESS), 2016.
[8] Pance Panov, Larisa N. Soldatova, and Saso Dzeroski. Ontology of core data mining entities. Data Min. Knowl. Discov., 28(5-6):1222–1265, 2014.
[9] Olga Tcheremenskaia, Romualdo Benigni, Ivelina Nikolova, Nina Jeliazkova, Sylvia E Escher, Monika Batke, Thomas Baier, Vladimir Poroikov, Alexey Lagunin, Micha Rautenberg, and Barry Hardy. OpenTox predictive toxicology framework: toxicological ontology and semantic media wiki-based opentoxipedia. Journal of Biomedical Semantics, 2012.
[10] Joaquin Vanschoren, Jan N Van Rijn, Bernd Bischl, and Luis Torgo. Openml: networked science in machine learning. ACM SIGKDD Explorations Newsletter, 15(2):49–60, 2014.

**Limitations/Discussion**

The main weakness of the existing vocabularies is that they are too coarse-grained to provide explicit semantics of the ML models, making them difficult to interpret by human users. The authors do not mention any future work or limitations in this paper.

---

**Summary Statistics:**
- Input: 2,164 words (15,730 chars)
- Output: 782 words
- Compression: 0.36x
- Generation: 52.8s (14.8 words/sec)
- Quality Score: 1.00/1.0
- Attempts: 1

**Quality Check:** Passed
