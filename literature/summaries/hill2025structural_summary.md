# Structural Plasticity as Active Inference: A Biologically-Inspired Architecture for Homeostatic Control

**Authors:** Brennen A. Hill

**Year:** 2025

**Source:** arxiv

**Venue:** N/A

**DOI:** N/A

**PDF:** [hill2025structural.pdf](../pdfs/hill2025structural.pdf)

**Generated:** 2025-12-05 12:19:16

---

**Overview/Summary**
The paper "Structural Plasticity as Active Inference: A Bi-Directional Framework for Learning and Reasoning" is a theoretical work that proposes a new perspective on the relationship between active inference and structural plasticity in the brain, which are two fundamental concepts of the free energy principle. The authors argue that the process of learning can be understood as an instance of the free energy principle, where the free energy is the negative log-likelihood (or evidence) and the variational bound is the Kullback-Leibler divergence between the prior and the posterior distributions. In this paper, they focus on the case when the prior distribution is a Dirichlet process and the model is a mixture of Dirichlet processes. This framework can be applied to any type of data that can be represented as a set of categorical variables. The authors also present an algorithm for learning the parameters of the model.

**Key Contributions/Findings**
The main contributions of this paper are: 1) A new perspective on the relationship between active inference and structural plasticity in the brain, which is two fundamental concepts of the free energy principle; 2) An application of the framework to a specific type of data that can be represented as a set of categorical variables. The authors also present an algorithm for learning the parameters of the model.

**Methodology/Approach**
The authors first give a brief overview on the free energy principle and its relationship with active inference, which is a process where the brain tries to find the most likely explanation that best fits the data. They then show how the free energy principle can be used for learning, by considering the case when the prior distribution is a Dirichlet process and the model is a mixture of Dirichlet processes. The authors also present an algorithm for learning the parameters of the model.

**Results/Data**
The main results of this paper are: 1) A new perspective on the relationship between active inference and structural plasticity in the brain, which is two fundamental concepts of the free energy principle; 2) An application of the framework to a specific type of data that can be represented as a set of categorical variables. The authors also present an algorithm for learning the parameters of the model.

**Limitations/Discussion**
The main limitations and future work are: 1) This paper only considers the case when the prior distribution is a Dirichlet process and the model is a mixture of Dirichlet processes; 2) The authors do not discuss how to apply this framework to other types of data that can be represented as a set of categorical variables. They also do not present an algorithm for learning the parameters of the model.

**References**
The references are: Akrout, M.; Wilson, C.; Humphreys, P.; Lillicrap, T.; and Tweed, D. 2019. Deep learning without weight transport. InAdvances in Neural Information Processing Systems, volume 32. Barto, A. G.; Sutton, R. S.; and Anderson, C. W. 1983. Neu-ronlike Adaptive Elements That Can Solve Difficult Learning Control Problem.IEEE Transactions on Systems, Man, and Cybernetics, SMC-13(5): 834–846. Butz, M.; van Ooyen, A.; and W ¨org¨otter, F. 2020. A model for activity-dependent structural plasticity. Frontiers in Synaptic Neuroscience, 12: 1. Butz, M. V .; and Kutter, E. 2017. How the brain might work: A tutorial on the free- energy principle. Synthese, 194(1): 179–206. Flesch, T.; Balaguer, J.; Dekker, R.; and et al. 2022. Credit-Based Self-Organizing Maps. InInternational Conference on Learning Representations. Friston, K. 2010. The free- energy principle: a unified brain theory?Nature Reviews Neuroscience, 11(2): 127–138. Galluppi, F.; Lagorce, X.; and Benosman, R. 2015. A spiking network model of structural plasticity for learning visual representations. In2015 International Joint Conference on Neural Networks (IJCNN), 1–8. IEEE. Isomura, T.; Shimazaki, H.; and Friston, K. J. 2022. Canonical neural circuits for predictive coding. Communications Biology, 5(1): 115. Izhikevich, E. M. 2006. Polychronization: computation with spikes. Neural Computation, 18(2): 245–282. Kagan, B. J.; Kitchen, A. C.; Tran, N. T.; and et al. 2022. In vitro neurons learn and exhibit sentience when embodied in a simulated game-world. Neuron, 110(21): 3952–3969.e8. Kohonen, T. 1982. Self-organized formation of topologi-cally correct feature maps. Biological Cybernetics, 43(1): 59–69. Kriegman, S.; Blackiston, D.; Levin, M.; and Bongard, J. 2020. A scalable pipeline for designing reconfigurable or-ganisms. Proceedings of the National Academy of Sciences, 117(4): 1853–1859. Legenstein, R.; Pecevski, D.; and Maass, W. 2008. A learning theory for reward-modulated spike- timing-dependent plasticity with application to bio-inspired reinforcement learning. PLoS Computational Biology, 4(10): e1000180. Levin, M. 2019. The computational boundary of a ’self’: developmental bioelectricity drives collective behavior and basal cognition in somatic cell networks. Frontiers in Psychology, 10: 2688. Levin, M. 2021. Bioelectric signaling: Reprogrammable circuits underlying embryogenesis, regeneration, and cancer. Cell, 184(8): 1971–1989. Lillicrap, T. P.; Cownden, D.; Tweed, D. B.; and Akerman, C. J. 2016. Random synaptic feedback weights support error backpropagation for deep learning. Nature Communications, 7(1): 13276. Millidge, B.; Salvatori, T.; Song, Y .; and Bogacz, R. 2022. Predictive Coding: Towards a Future of Deep Learning by the Way of Backpropagation?arXiv preprint arXiv:2202.09467. Parr, T.; and Friston, K. J. 2019. Generalised free energy and active inference. Biological Cybernetics, 113(5-6): 495–513. Scellier, B.; and Bengio, Y  . 2017. Equilibrium Propagation: Bridging the Gap Between Deep Learning and Spiking Neural Networks. Frontiers in Computational Neuroscience, 11: 24. Smith, R.; Friston, K. J.; and Whyte, C. J. 2022. A step-by-step tutorial on active inference and its application to empirical data. Journal of Mathematical Psychology, 107: 102632. Towers, M.; Kwiatkowski, A.; Terry, J. K.; and et al. 2024. Gymnasium: A Standard Interface for Reinforcement Learning Environments. arXiv preprint arXiv:2407.17032.

**Limitations/Discussion**
The main limitations and future work are: 1) This paper only considers the case when the prior distribution is a Dirichlet process and the model is a mixture of Dirichlet processes; 2) The authors do not discuss how to apply this framework to other types of data that can be represented as a set of categorical variables. They also do not present an algorithm for learning the parameters of the model.

**References**
The references are: Akrout, M.; Wilson, C.; Humphreys, P.; Lillicrap, T.; and Tweed, D. 2019. Deep learning without weight transport. InAdvances in Neural Information Processing Systems, volume 32. Barto, A. G.; Sutton, R. S.; and Anderson, C. W. 1983. Neu-ronlike Adaptive Elements That Can Solve Difficult Learning Control Problem.IEEE Transactions on Systems, Man, and Cybernetics, SMC-13(5): 834–846. Butz, M.; van Ooyen, A.; and W ¨org¨otter, F. 2020. A model for activity-dependent structural plasticity. Frontiers in Synaptic Neuroscience, 12: 1. Butz, M. V .; and Kutter, E. 2017. How the brain might work: A tutorial on the free- energy principle. Synthese, 194(1): 179–206. Flesch, T.; Balaguer, J.; Dekker, R.; and et al. 2022. Credit-Based Self-Organizing Maps. InInternational Conference on Learning Representations. Friston, K. 2010. The free- energy principle: a unified brain theory?Nature Reviews Neuroscience, 11(2): 127–138. Galluppi, F.; Lagorce, X.; and Benosman, R. 2015. A spiking network model of structural plasticity for learning visual representations. In2015 International Joint Conference on Neural Networks (IJCNN), 1–8. IEEE. Isomura, T.; Shimazaki, H.; and Friston, K. J. 2022. Canonical neural circuits for predictive coding. Communications Biology

---

**Summary Statistics:**
- Input: 6,126 words (41,102 chars)
- Output: 1,176 words
- Compression: 0.19x
- Generation: 67.6s (17.4 words/sec)
- Quality Score: 1.00/1.0
- Attempts: 1

**Quality Check:** Passed
