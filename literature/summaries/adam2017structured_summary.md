# Structured Variational Inference for Coupled Gaussian Processes

**Authors:** Vincent Adam

**Year:** 2017

**Source:** arxiv

**Venue:** arXiv

**DOI:** N/A

**PDF:** [adam2017structured.pdf](../pdfs/adam2017structured.pdf)

**Generated:** 2025-12-03 03:27:02

---

**Overview/Summary**

The paper "Structured Variational Inference for Coupled Gaussian Processes" by James Hensman and Alexander G de G Matthews is an extension of the variational expectation maximisation (VFE) approximation to regression settings with multiple Gaussian processes. The authors build upon the VFE approach that has been used in a variety of different contexts, including GP classification [3], Bayesian linear regression [9] and GAMs [4]. The main contribution of this paper is an extension of the VFE algorithm to capture richer posterior dependencies between latent functions. This is achieved by increasing the complexity of the variational distribution while keeping the computational cost of the resulting inference low.

**Key Contributions/Findings**

The authors' approach goes beyond the classical mean-ﬁeld approximation and provides a more general way to perform inference in the CGP setting. The VFE algorithm has been used for GP regression [3], Bayesian linear regression [9] and GAMs [4]. This paper is an extension of this work, which captures richer posterior dependencies between latent functions while keeping the computational cost of the resulting inference low. The authors' approach is very general and provides a fast and scalable way to perform inference in a class of models that are broadly used in statistics such as those for GAMs.

**Methodology/Approach**

The VFE algorithm has been used for GP regression [3], Bayesian linear regression [9] and GAMs [4]. The main contribution of this paper is an extension of the VFE algorithm to capture richer posterior dependencies between latent functions. This is achieved by increasing the complexity of the variational distribution while keeping the computational cost of the resulting inference low. The authors' approach is very general and provides a fast and scalable way to perform inference in a class of models that are broadly used in statistics such as those for GAMs.

**Results/Data**

The paper does not provide any data or results. It only presents an overview of the VFE algorithm and its extension to capture richer posterior dependencies between latent functions. The authors do not present any experimental results or comparisons with other methods. The authors also do not discuss any limitations of their approach.

**Limitations/Discussion**

The paper does not provide any data or results. It only presents an overview of the VFE algorithm and its extension to capture richer posterior dependencies between latent functions. The authors do not present any experimental results or comparisons with other methods. The authors also do not discuss any limitations of their approach.

**References**

[1] Carl Edward Rasmussen and Christopher KI Williams. Gaussian processes for machine learning, volume 1. MIT press Cambridge, 2006.
[2] Michalis K Titsias. Variational learning of inducing variables in sparse gaussian processes. In International Conference on Artiﬁcial Intelligence and Statistics, pages 567–574, 2009.
[3] Matthias Bauer, Mark van der Wilk, and Carl Edward Rasmussen. Understanding probabilistic sparse gaussian process approximations. In Advances in Neural Information Processing Systems, pages 1533–1541, 2016.
[4] Trevor Hastie and Robert Tibshirani. Generalized additive models. Wiley Online Library, 1990.
[5] Alan D Saul, James Hensman, Aki Vehtari, and Neil D Lawrence. Chained gaussian processes. In Artiﬁcial Intelligence and Statistics, pages 1431–1440, 2016.
[6] Vincent Adam, James Hensman, and Maneesh Sahani. Scalable transformed additive signal decomposition by non- conjugate gaussian process inference. In Machine Learning for Signal Processing (MLSP), 2016 IEEE 26th International Workshop on, pages 1–6. IEEE, 2016.
[7] Richard E Turner and Maneesh Sahani. Two problems with variational expectation maximisation for time-series models. Bayesian Time series models, pages 115–138, 2011.
[8] Ryan J Giordano, Tamara Broderick, and Michael I Jordan. Linear response methods for accurate covariance estimates from mean ﬁeld variational bayes. In Advances in Neural Information Processing Systems, pages 1441–1449, 2015.
[9] Dustin Tran, David Blei, and Edo M Airoldi. Copula variational inference. In Advances in Neural Information Processing Systems, pages 3564–3572, 2015.
[10] Matthew Hoffman and David Blei. Stochastic structured variational inference. In Artiﬁcial Intelligence and Statistics, pages 361–369, 2015.
[11] James Hensman, Alexander G de G Matthews, and Zoubin Ghahramani. Scalable variational Gaussian process classiﬁcation. In Proceedings of the Eighteenth International Conference on
Artiﬁcial Intelligence and Statistics, 2015.

**Additional Notes**

The paper does not provide any data or results. It only presents an overview of the VFE algorithm and its extension to capture richer posterior dependencies between latent functions. The authors do not present any experimental results or comparisons with other methods. The authors also do not discuss any limitations of their approach.

---

**Summary Statistics:**
- Input: 2,306 words (14,836 chars)
- Output: 727 words
- Compression: 0.32x
- Generation: 41.7s (17.4 words/sec)
- Quality Score: 0.60/1.0
- Attempts: 1

**Quality Issues:** Hallucination detected: Physics paper summary lacks physics terminology
