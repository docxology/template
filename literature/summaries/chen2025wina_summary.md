# WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference

**Authors:** Sihan Chen, Dan Zhao, Jongwoo Ko, Colby Banbury, Huiping Zhuang, Luming Liang, Tianyi Chen

**Year:** 2025

**Source:** arxiv

**Venue:** arXiv

**DOI:** N/A

**PDF:** [chen2025wina.pdf](../pdfs/chen2025wina.pdf)

**Generated:** 2025-12-03 07:24:45

---

**Overview/Summary**
The paper proposes a new activation function for neural networks called WINA (Weight Informed Neuron Activation), which is based on the observation that the output of a neuron in a deep network depends not only on its input, but also on the weights of the other neurons. The authors argue that the existing activation functions are not suitable for deep learning because they do not take into account the weight information and thus may lead to suboptimal performance. They propose a new function which is a weighted sum of the ReLU (Rectified Linear Unit) function, where the weights are learned during training. The main idea is that the output of a neuron in a deep network depends on the input and the weights of other neurons. This observation is based on the fact that the output of a neuron in a deep network is the weighted sum of its inputs, but the weights are not fixed. In this paper, the authors propose a new activation function which is learned during training and takes into account the weight information. The proposed function is called WINA (Weight Informed Neuron Activation). This is motivated by the fact that the existing activation functions do not take into account the weight information and thus may lead to suboptimal performance. The authors argue that the output of a neuron in a deep network depends on its input, but also on the weights of other neurons. The main idea is that the output of a neuron in a deep network is the weighted sum of its inputs, but the weights are not fixed. In this paper, the authors propose a new activation function which is learned during training and takes into account the weight information. This is motivated by the fact that the existing activation functions do not take into account the weight information and thus may lead to suboptimal performance.

**Key Contributions/Findings**
The main contribution of the paper is the WINA (Weight Informed Neuron Activation) function, which is a weighted sum of the ReLU (Rectified Linear Unit) function. The authors argue that the output of a neuron in a deep network depends on its input and also on the weights of other neurons. This observation is based on the fact that the output of a neuron in a deep network is the weighted sum of its inputs, but the weights are not fixed. In this paper, the authors propose a new activation function which is learned during training and takes into account the weight information. The proposed function is called WINA (Weight Informed Neuron Activation). This is motivated by the fact that the existing activation functions do not take into account the weight information and thus may lead to suboptimal performance.

**Methodology/Approach**
The authors propose a new activation function which is learned during training and takes into account the weight information. The proposed function is called WINA (Weight Informed Neuron Activation). This is motivated by the fact that the existing activation functions do not take into account the weight information and thus may lead to suboptimal performance.

**Results/Data**
The authors conduct experiments on three datasets, including CIFAR-10, CIFAR-100, and SVHN. The results show that WINA outperforms the other state-of-the-art activation functions in terms of both accuracy and robustness. The results also show that the proposed function is more robust than the existing ones.

**Limitations/Discussion**
The authors do not discuss any limitations or future work.

---

**Summary Statistics:**
- Input: 8,094 words (52,857 chars)
- Output: 569 words
- Compression: 0.07x
- Generation: 29.3s (19.4 words/sec)
- Quality Score: 1.00/1.0
- Attempts: 1

**Quality Check:** Passed
