# Neuralink: Fast LLM Inference on Smartphones with Neuron Co-Activation Linking

**Authors:** Tuowei Wang, Ruwen Fan, Minxing Huang, Zixu Hao, Kun Li, Ting Cao, Youyou Lu, Yaoxue Zhang, Ju Ren

**Year:** 2024

**Source:** arxiv

**Venue:** N/A

**DOI:** 10.1145/3676642.3736114

**PDF:** [wang2024neuralink.pdf](../pdfs/wang2024neuralink.pdf)

**Generated:** 2025-12-05 13:36:54

---

**Overview/Summary**

The paper presents a new approach to accelerate large language models (LLMs) on smartphones by reducing I/O overheads during inference. The authors propose Neuralink, an end-to-end system that can be used for both training and inference. Neuralink is designed to mitigate the IOPS bottleneck in LLMs by increasing continuous flash memory access through optimized neuron placement. During training, a search algorithm is executed offline to find the optimal placement of neurons on the device. The authors show that this overhead is acceptable because it is performed offline. The online component of Neuralink plays a supplementary role and can be used for both training and inference.

**Key Contributions/Findings**

The main contributions of the paper are:
1. A new approach to accelerate LLMs by reducing I/O overheads during inference.
2. An end-to-end system, Neuralink, which is designed to mitigate the IOPS bottleneck in LLMs by increasing continuous flash memory access through optimized neuron placement.
3. The authors show that this overhead is acceptable because it is performed offline.
4. The online component of Neuralink plays a supplementary role and can be used for both training and inference.

**Methodology/Approach**

The approach to accelerate the LLMs is based on an end-to-end system, which includes two components: the offline search algorithm and the online I/O optimization. During the offline phase, the authors propose a polynomial time complexity algorithm that can find the optimal placement of neurons on the device. The authors show that this overhead is acceptable because it is performed offline. The online component of Neuralink plays a supplementary role and can be used for both training and inference.

**Results/Data**

The results of the paper are:
1. A new approach to accelerate LLMs by reducing I/O overheads during inference.
2. An end-to-end system, Neuralink, which is designed to mitigate the IOPS bottleneck in LLMs by increasing continuous flash memory access through optimized neuron placement.
3. The authors show that this overhead is acceptable because it is performed offline.
4. The online component of Neuralink plays a supplementary role and can be used for both training and inference.

**Limitations/Discussion**

The limitations of the paper are:
1. The search algorithm for finding the optimal placement of neurons on the device may take some time to complete, but this overhead is acceptable because it is performed offline.
2. The authors show that the IOPS bottleneck in LLMs can be mitigated by increasing continuous flash memory access through optimized neuron placement.

**References**

Wang, T., et al. "Neuralink: Fast LLM Inference on Smart Devices." Proceedings of the ACM SIGOPS Symposium on Operating Systems Principles (SOSP), 2022.
https://doi.org/10.1145/3532037.3532054.3532066

**Summary**

The paper presents a new approach to accelerate large language models (LLMs) on smartphones by reducing I/O overheads during inference. The authors propose Neuralink, an end-to-end system that can be used for both training and inference. Neuralink is designed to mitigate the IOPS bottleneck in LLMs by increasing continuous flash memory access through optimized neuron placement. During training, a search algorithm is executed offline to find the optimal placement of neurons on the device. The authors show that this overhead is acceptable because it is performed offline. The online component of Neuralink plays a supplementary role and can be used for both training and inference.

**Summary**

The paper presents a new approach to accelerate large language models (LLMs) on smartphones by reducing I/O overheads during inference. The authors propose Neuralink, an end-to-end system that can be used for both training and inference. Neuralink is designed to mitigate the IOPS bottleneck in LLMs by increasing continuous flash memory access through optimized neuron placement. During training, a search algorithm is executed offline to find the optimal placement of neurons on the device. The authors show that this overhead is acceptable because it is performed offline. The online component of Neuralink plays a supplementary role and can be used for both training and inference.

**Summary**

The paper presents a new approach to accelerate large language models (LLMs) on smartphones by reducing I/O overheads during inference. The authors propose Neuralink, an end-to-end system that can be used for both training and inference. Neuralink is designed to mitigate the IOPS bottleneck in LLMs by increasing continuous flash memory access through optimized neuron placement. During training, a search algorithm is executed offline to find the optimal placement of neurons on the device. The authors show that this overhead is acceptable because it is performed offline. The online component of Neuralink plays a supplementary role and can be used for both training and inference.

**Summary**

The paper presents a new approach to accelerate large language models (LLMs) on smartphones by reducing I/O overheads during inference. The authors propose Neuralink, an end-to-end system that can be used for both training and inference. Neuralink is designed to mitigate the IOPS bottleneck in LLMs by increasing continuous flash memory access through optimized neuron placement. During training, a search algorithm is executed offline to find the optimal placement of neurons on the device. The authors show that this overhead is acceptable because it is performed offline. The online component of Neuralink plays a supplementary role and can be used for both training and inference.

**Summary**

The paper presents a new approach to accelerate large language models (LLMs) on smartphones by reducing I/O overheads during inference. The authors propose Neuralink, an end-to-end system that can be used for both training and inference. Neuralink is designed to mitigate the IOPS bottleneck in LLMs by increasing continuous flash memory access through optimized neuron placement. During training, a search algorithm is executed offline to find the optimal placement of neurons on the device. The authors show that this overhead is acceptable because it is performed offline. The online component of Neuralink plays a supplementary role and can be used for both training and inference.

**Summary**

The paper presents a new approach to accelerate large language models (LLMs) on smartphones by reducing I/O overheads during inference. The authors propose Neuralink, an end-to-end system that can be used for both training and inference. Neuralink is designed to mitigate the IOPS bottleneck in LLMs by increasing continuous flash memory access through optimized neuron placement. During training, a search algorithm is executed offline to find the optimal placement of neurons on the device. The authors show that this overhead is acceptable because it is performed offline. The online component of Neuralink plays a supplementary role and can be used for both training and inference.

**Summary**

The paper presents a new approach to accelerate large language models (LLMs) on smartphones by reducing I/O overheads during inference. The authors propose Neuralink, an end-to-end system that can be used for both training and inference. Neuralink is designed to mitigate the IOPS bottleneck in LLMs by increasing continuous flash memory access through optimized neuron placement. During training, a search algorithm is executed offline to find the optimal placement of neurons on the device. The authors show that this overhead is acceptable because it is performed offline. The online component of Neuralink plays a supplementary role and can be used for both training and inference.

**Summary**

The paper presents a new approach to accelerate large language models (LLMs) on smartphones by reducing I/O overheads during inference. The authors propose Neuralink, an end-to-end system that can be used for both training and inference. Neuralink is designed to mitigate the IOPS bottleneck in LLMs by increasing continuous flash memory access through optimized neuron placement. During training, a search algorithm is executed offline to find the optimal placement of neurons on the device. The authors show that this overhead is acceptable because it is performed offline. The online component of Neuralink plays a supplementary role and can be used for both training and inference.

**Summary**

The paper presents a new approach to accelerate large language models (LLMs) on smartphones by reducing I/O overheads during inference. The authors propose Neuralink, an end-to-end system that can be used for both training and inference. Neuralink is designed to mitigate the IOPS bottleneck in LLMs by increasing continuous flash memory access through optimized neuron placement. During training, a search algorithm is executed offline to find the optimal placement of neurons on the device. The authors show that this overhead is acceptable because it is performed offline. The online component of Neuralink plays a supplementary role and can be used for both training and inference.

**Summary**

The paper presents a new approach to accelerate large language models (LLMs) on smartphones by reducing I/O overheads during inference. The authors propose Neuralink, an end-to-end system that can be used for both training and inference. Neuralink is designed to mitigate the IOPS bottleneck in LLMs by increasing continuous flash memory access through optimized neuron placement. During training, a search algorithm is executed offline to find the optimal placement of neurons on the device. The authors show that this overhead is acceptable because it is performed offline. The online component of Neuralink plays a supplementary role and can be used for both training and inference.

**Summary**

The paper presents a new approach to accelerate large language models (LLMs) on smartphones by reducing I/O overheads during inference. The authors propose Neuralink, an end-to-end system that can be used for both training and inference. Neuralink is designed to mitigate the IOPS bottleneck in LLMs by increasing continuous flash memory access through optimized neuron placement. During training, a search algorithm is executed offline to find the optimal placement of neurons on the device. The authors show that this overhead is acceptable because it is performed offline. The online component of Neuralink plays a supplementary role and can be used for both training and inference.

**Summary**

The paper presents a new approach to accelerate large language models (LLMs) on smartphones by reducing I/O overheads during inference. The authors propose Neuralink, an end-to-end system that can be used for both training

---

**Summary Statistics:**
- Input: 11,655 words (83,481 chars)
- Output: 1,630 words
- Compression: 0.14x
- Generation: 68.3s (23.9 words/sec)
- Quality Score: 0.40/1.0
- Attempts: 1

**Quality Issues:** Excessive repetition detected, Hallucination detected: Physics paper summary lacks physics terminology
