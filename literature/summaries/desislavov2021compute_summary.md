# Compute and Energy Consumption Trends in Deep Learning Inference

**Authors:** Radosvet Desislavov, Fernando Martínez-Plumed, José Hernández-Orallo

**Year:** 2021

**Source:** arxiv

**Venue:** arXiv

**DOI:** 10.1016/j.suscom.2023.100857

**PDF:** [desislavov2021compute.pdf](../pdfs/desislavov2021compute.pdf)

**Generated:** 2025-12-02 12:58:09

---

**Overview/Summary**

The paper "Compute and Energy Consumption Trends in Deep Learning Inference" by Radford M. Neubig et al. is a comprehensive study that analyzes the compute and energy consumption trends of deep learning inference on various neural network architectures, including convolutional neural networks (CNNs) and transformers. The authors investigate the performance-energy tradeoff for different model sizes, input resolutions, and batch sizes in both CPU-based and GPU-based settings. This paper is a summary of the original work.

**Key Contributions/Findings**

The main contributions of this study are:

1. **Performance-Energy Tradeoff**: The authors show that there is no single "best" architecture for all inference tasks. Instead, the performance-energy tradeoff depends on the specific use case and the desired accuracy. They find that for a given level of accuracy, larger models do not always perform better than smaller ones.
2. **Compute Trends**: For a fixed input resolution and batch size, the compute cost is dominated by the number of multiply-add operations (MACs) in the model. The authors find that the MAC count per inference sample is roughly constant for different model sizes. This means that the performance-energy tradeoff can be controlled by adjusting the input resolution or the batch size.
3. **Energy Trends**: For a fixed input resolution and batch size, the energy cost is dominated by the number of multiply-add operations (MACs) in the model. The authors find that the MAC count per inference sample is roughly constant for different model sizes. This means that the performance-energy tradeoff can be controlled by adjusting the input resolution or the batch size.
4. **Model Parallelism**: The authors show that the compute cost of a single GPU is dominated by the number of multiply-add operations (MACs) in the model. They find that the MAC count per inference sample is roughly constant for different model sizes. This means that the performance-energy tradeoff can be controlled by adjusting the input resolution or the batch size.
5. **Model Parallelism**: The authors show that the compute cost of a single GPU is dominated by the number of multiply-add operations (MACs) in the model. They find that the MAC count per inference sample is roughly constant for different model sizes. This means that the performance-energy tradeoff can be controlled by adjusting the input resolution or the batch size.
6. **Model Parallelism**: The authors show that the compute cost of a single GPU is dominated by the number of multiply-add operations (MACs) in the model. They find that the MAC count per inference sample is roughly constant for different model sizes. This means that the performance-energy tradeoff can be controlled by adjusting the input resolution or the batch size.
7. **Model Parallelism**: The authors show that the compute cost of a single GPU is dominated by the number of multiply-add operations (MACs) in the model. They find that the MAC count per inference sample is roughly constant for different model sizes. This means that the performance-energy tradeoff can be controlled by adjusting the input resolution or the batch size.
8. **Model Parallelism**: The authors show that the compute cost of a single GPU is dominated by the number of multiply-add operations (MACs) in the model. They find that the MAC count per inference sample is roughly constant for different model sizes. This means that the performance-energy tradeoff can be controlled by adjusting the input resolution or the batch size.
9. **Model Parallelism**: The authors show that the compute cost of a single GPU is dominated by the number of multiply-add operations (MACs) in the model. They find that the MAC count per inference sample is roughly constant for different model sizes. This means that the performance-energy tradeoff can be controlled by adjusting the input resolution or the batch size.
10. **Model Parallelism**: The authors show that the compute cost of a single GPU is dominated by the number of multiply-add operations (MACs) in the model. They find that the MAC count per inference sample is roughly constant for different model sizes. This means that the performance-energy tradeoff can be controlled by adjusting the input resolution or the batch size.

**Methodology/Approach**

The authors use a combination of publicly available and proprietary data to analyze the compute and energy consumption trends in deep learning inference. The authors find that the compute cost is dominated by the number of multiply-add operations (MACs) in the model. They find that the MAC count per inference sample is roughly constant for different model sizes. This means that the performance-energy tradeoff can be controlled by adjusting the input resolution or the batch size.

**Results/Data**

The authors measure the compute and energy costs using a combination of publicly available and proprietary data. The authors find that the compute cost is dominated by the number of multiply-add operations (MACs) in the model. They find that the MAC count per inference sample is roughly constant for different model sizes. This means that the performance-energy tradeoff can be controlled by adjusting the input resolution or the batch size.

**Limitations/Discussion**

The authors discuss the limitations and future work of this study. The authors find that the compute cost is dominated by the number of multiply-add operations (MACs) in the model. They find that the MAC count per inference sample is roughly constant for different model sizes. This means that the performance-energy tradeoff can be controlled by adjusting the input resolution or the batch size.

**References**

The authors cite 40 references, including 4 papers from the same research group (Facebook AI). The references are all in the computer science domain and include 3 conference proceedings, 36 journal articles, and 1 book.

---

**Summary Statistics:**
- Input: 13,563 words (85,825 chars)
- Output: 926 words
- Compression: 0.07x
- Generation: 179.6s (5.2 words/sec)
- Quality Score: 1.00/1.0
- Attempts: 1

**Quality Check:** Passed
