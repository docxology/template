# In vitro neural networks minimise variational free energy

**Authors:** Takuya Isomura, Karl J. Friston

**Year:** 2018

**Source:** semanticscholar

**Venue:** Scientific Reports

**DOI:** 10.1038/s41598-018-35221-w

**PDF:** [isomura2018vitro.pdf](../pdfs/isomura2018vitro.pdf)

**Generated:** 2025-12-02 08:09:23

---

**Overview/Summary**

The paper "In vitro neural networks minimise variational free energy" by Takashi and colleagues is an empirical study on the neural code in terms of Bayesian inference that demonstrates meaningful reductions in variational free energy in in vitro neural networks. The authors first introduce a mathematical framework for understanding the neural code based on the free-energy principle, which is a probabilistic theory of brain function. They then show that this theory can be used to understand the self-organising processes observed in in vitro neural networks and demonstrate the utility of inference as a basis for understanding the neural code and the function of neural networks. The authors find that the functional specialisation of neuronal responses, which is a progressive increase in accuracy with an accompanying complexity cost followed by a simplification of the encoding, can be explained by this theory.

**Key Contributions/Findings**

The main findings of the paper are that the functional specialisation observed in the in vitro neural networks can be understood as a form of Ockham's principle. The authors find that the accuracy of the neuronal responses increases with an accompanying complexity cost followed by a simplification of the encoding, which is similar to a recent deep learning study. This suggests that the same sort of functional specialisation may also emerge in the in vivo brain.

**Methodology/Approach**

The authors use a mathematical framework for understanding the neural code based on the free-energy principle, which is a probabilistic theory of brain function. The authors then show that this theory can be used to understand the self-organising processes observed in in vitro neural networks and demonstrate the utility of inference as a basis for understanding the neural code and the function of neural networks. The authors use an in vitro experimental setup where the spontaneous firing patterns of neurons reach a developmentally stable period at around 18 days in vitro, which is the age range used to record the data. The data were recorded from 23 cultures.

**Results/Data**

The main results are that the accuracy of the neuronal responses increases with an accompanying complexity cost followed by a simplification of the encoding, which is similar to a recent deep learning study. This suggests that the same sort of functional specialisation may also emerge in the in vivo brain. The authors find that the accuracy actually fell over time to a small extent with learning.

**Limitations/Discussion**

The main weakness of the paper is that it does not discuss the relationship between the variables in the MDP scheme and the concentrations of neurotransmitters and neuromodulators. In the subsequent work, the authors will touch on this issue by asking how alterations in the level of neurotransmitters and neuromodulators influence Bayesian inference and learning evinced by in vitro neural networks.

**References**

The references are not included in

---

**Summary Statistics:**
- Input: 10,102 words (68,308 chars)
- Output: 464 words
- Compression: 0.05x
- Generation: 27.0s (17.2 words/sec)
- Quality Score: 1.00/1.0
- Attempts: 1

**Quality Check:** Passed
