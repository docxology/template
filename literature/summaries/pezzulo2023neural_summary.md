# Neural representation in active inference: using generative models to interact with -- and understand -- the lived world

**Authors:** Giovanni Pezzulo, Leo D'Amato, Francesco Mannella, Matteo Priorelli, Toon Van de Maele, Ivilin Peev Stoianov, Karl Friston

**Year:** 2023

**Source:** arxiv

**Venue:** N/A

**DOI:** N/A

**PDF:** [pezzulo2023neural.pdf](../pdfs/pezzulo2023neural.pdf)

**Generated:** 2025-12-05 13:14:58

---

**Overview/Summary**

The paper "Neural representation in active inference: using generative models" discusses how a class of probabilistic graphical models called generative models can be used to understand the workings of the brain and the neural representation that it uses. The authors argue that these models are particularly well suited for understanding the brain because they do not require any assumptions about the nature of the world, but instead use only the information in the sensory data. They also make a case for the idea that the brain is using an active inference process to generate its internal representation of the world. The paper begins by explaining how generative models are used in computer vision and natural language processing. It then describes some of the key features of these models, such as the use of latent variables (unobserved variables) and the ability to model complex distributions over a large number of data points. The authors also describe what they mean by "neural representation" - that is, how the brain represents the world in terms of internal representations. They argue that this process is not just about representing the external world but also involves generating beliefs about the future and the past. This means that the brain's internal representation of the world can be thought of as a kind of "mental simulation" or "thought experiment". The authors then describe how generative models are used in the study of neural computation, with an emphasis on the use of variational autoencoders (VAEs) and generative adversarial networks (GANs). They also discuss how these models can be used to understand the workings of the brain. One key point is that the internal representation 

**Key Contributions/Findings**

The paper makes several key contributions. First, it describes how generative models are used in computer vision and natural language processing. Second, it explains what they mean by "neural representation" - that is, how the brain represents the world in terms of internal representations. Third, it discusses how generative models can be used to understand the workings of the brain. One key point is that the internal representation 

**Methodology/Approach**

The paper begins by explaining how generative models are used in computer vision and natural language processing. It then describes some of the key features of these models, such as the use of latent variables (unobserved variables) and the ability to model complex distributions over a large number of data points. The authors also describe what they mean by "neural representation" - that is, how the brain represents the world in terms of internal representations. They argue that this process is not just about representing the external world but also involves generating beliefs about the future and the past. This means that the brain's internal representation of the world can be thought of as a kind of "mental simulation" or "thought experiment". The authors then describe how generative models are used in the study of neural computation, with an emphasis on the use of variational autoencoders (VAEs) and generative adversarial networks (GANs). They also discuss how these models can be used to understand the workings of the brain. One key point is that the internal representation 

**Results/Data**

The paper does not present any new data, but it does describe how generative models are used in the study of neural computation. It describes how VAEs and GANs can be used to understand the workings of the brain. One key point is that the internal representation 

**Limitations/Discussion**

The paper does not present any new data, but it does describe how generative models are used in the study of neural computation. It describes how VAEs and GANs can be used to understand the workings of the brain. One key point is that the internal representation 

**References**

The paper has 64 references, which are all from the field of computer science. The majority of these are in the area of machine learning and deep learning. There are also some references to statistics, information theory, and cognitive psychology.

---

**Summary Statistics:**
- Input: 15,673 words (112,253 chars)
- Output: 661 words
- Compression: 0.04x
- Generation: 47.7s (13.9 words/sec)
- Quality Score: 1.00/1.0
- Attempts: 1

**Quality Check:** Passed
