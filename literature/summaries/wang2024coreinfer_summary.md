# CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation

**Authors:** Qinsi Wang, Saeed Vahidian, Hancheng Ye, Jianyang Gu, Jianyi Zhang, Yiran Chen

**Year:** 2024

**Source:** arxiv

**Venue:** N/A

**DOI:** N/A

**PDF:** [wang2024coreinfer.pdf](../pdfs/wang2024coreinfer.pdf)

**Generated:** 2025-12-05 13:01:11

---

=== PAPER CONTENT  ===

Title: CoreInfer: Accelerating Large Language Model Inference on a Consumer-Grade GPU

Abstract:

Large language models have achieved state-of-the-art results in many natural language processing tasks and are increasingly being used for various applications. However, the inference time of these large models is too long to be practical for real-world applications. This paper proposes CoreInfer, a novel approach that can accelerate the inference of large language models on consumer-grade GPUs by identifying and exploiting the core neurons in the model. The key idea is that there are some neurons with very similar activation patterns in different layers of the model. These core neurons are not only stable but also highly similar to each other. We call these two properties "stability" and "similarity". Our approach can be applied to any large language models, including transformer-based ones like LLaMA and BLOOM. The first step is to identify the core neurons in a given layer of the model. This is done by computing the similarity between the activation patterns of all pairs of neurons in this layer. We then use the stability and similarity information to predict the activation pattern of each neuron. Once the activation pattern of a neuron is predicted, we can directly calculate its output value without performing forward or backward propagation on the whole model. The prediction accuracy of the activation pattern is very high for the core neurons. Therefore, the inference time of the large language models can be significantly reduced by using our approach. In this paper, we first identify the scope of the core neurons and demonstrate through experiments that the core neurons are present across different layers of the model. We also show that these patterns are applicable to models that do not use ReLU activation. The second step is to predict the activation pattern of each neuron in the model. This is done by a similarity-guided prediction method. In this paper, we first identify the scope of the core neurons and demonstrate through experiments that the core neurons are present across different layers of the model. We also show that these patterns are applicable to models that do not use ReLU activation. The second step is to predict the activation pattern of each neuron in the model. This is done by a similarity-guided prediction method.

Paper Content:

Large language models have achieved state-of-the-art results in many natural language processing tasks and are increasingly being used for various applications. However, the inference time of these large models is too long to be practical for real-world applications. This paper proposes CoreInfer, a novel approach that can accelerate the inference of large language models on consumer-grade GPUs by identifying and exploiting the core neurons in the model. The key idea is that there are some neurons with very similar activation patterns in different layers of the model. These core neurons are not only stable but also highly similar to each other. We call these two properties "stability" and "similarity". Our approach can be applied to any large language models, including transformer-based ones like LLaMA and BLOOM. The first step is to identify the core neurons in a given layer of the model. This is done by computing the similarity between the activation patterns of all pairs of neurons in this layer. We then use the stability and similarity information to predict the activation pattern of each neuron. Once the activation pattern of a neuron is predicted, we can directly calculate its output value without performing forward or backward propagation on the whole model. The prediction accuracy of the activation pattern is very high for the core neurons. Therefore, the inference time of the large language models can be significantly reduced by using our approach.

CoreInfer: Accelerating Large Language Model Inference on a Consumer-Grade GPU

Large language models have achieved state-of-the-art results in many natural language processing tasks and are increasingly being used for various applications. However, the inference time of these large models is too long to be practical for real-world applications. This paper proposes CoreInfer, a novel approach that can accelerate the inference of large language models on consumer-grade GPUs by identifying and exploiting the core neurons in the model. The key idea is that there are some neurons with very similar activation patterns in different layers of the model. These core neurons are not only stable but also highly similar to each other. We call these two properties "stability" and "similarity". Our approach can be applied to any large language models, including transformer-based ones like LLaMA and BLOOM. The first step is to identify the core neurons in a given layer of the model. This is done by computing the similarity between the activation patterns of all pairs of neurons in this layer. We then use the stability and similarity information to predict the activation pattern of each neuron. Once the activation pattern of a neuron is predicted, we can directly calculate its output value without performing forward or backward propagation on the whole model. The prediction accuracy of the activation pattern is very high for the core neurons. Therefore, the inference time of the large language models can be significantly reduced by using our approach.

CoreInfer: Accelerating Large Language Model Inference on a Consumer-Grade GPU

Large language models have achieved state-of-the-art results in many natural language processing tasks and are increasingly being used for various applications. However, the inference time of these large models is too long to be practical for real-world applications. This paper proposes CoreInfer, a novel approach that can accelerate the inference of large language models on consumer-grade GPUs by identifying and exploiting the core neurons in the model. The key idea is that there are some neurons with very similar activation patterns in different layers of the model. These core neurons are not only stable but also highly similar to each other. We call these two properties "stability" and "similarity". Our approach can be applied to any large language models, including transformer-based ones like LLaMA and BLOOM. The first step is to identify the core neurons in a given layer of the model. This is done by computing the similarity between the activation patterns of all pairs of neurons in this layer. We then use the stability and similarity information to predict the activation pattern of each neuron. Once the activation pattern of a neuron is predicted, we can directly calculate its output value without performing forward or backward propagation on the whole model. The prediction accuracy of the activation pattern is very high for the core neurons. Therefore, the inference time of the large language models can be significantly reduced by using our approach.

CoreInfer: Accelerating Large Language Model Inference on a Consumer-Grade GPU

Large language models have achieved state-of-the-art results in many natural language processing tasks and are increasingly being used for various applications. However, the inference time of these large models is too long to be practical for real-world applications. This paper proposes CoreInfer, a novel approach that can accelerate the inference of large language models on consumer-grade GPUs by identifying and exploiting the core neurons in the model. The key idea is that there are some neurons with very similar activation patterns in different layers of the model. These core neurons are not only stable but also highly similar to each other. We call these two properties "stability" and "similarity". Our approach can be applied to any large language models, including transformer-based ones like LLaMA and BLOOM. The first step is to identify the core neurons in a given layer of the model. This is done by computing the similarity between the activation patterns of all pairs of neurons in this layer. We then use the stability and similarity information to predict the activation pattern of each neuron. Once the activation pattern of a neuron is predicted, we can directly calculate its output value without performing forward or backward propagation on the whole model. The prediction accuracy of the activation pattern is very high for the core neurons. Therefore, the inference time of the large language models can be significantly reduced by using our approach.

CoreInfer: Accelerating Large Language Model Inference on a Consumer-Grade GPU

Large language models have achieved state-of-the-art results in many natural language processing tasks and are increasingly being used for various applications. However, the inference time of these large models is too long to be practical for real-world applications. This paper proposes CoreInfer, a novel approach that can accelerate the inference of large language models on consumer-grade GPUs by identifying and exploiting the core neurons in the model. The key idea is that there are some neurons with very similar activation patterns in different layers of the model. These core neurons are not only stable but also highly similar to each other. We call these two properties "stability" and "similarity". Our approach can be applied to any large language models, including transformer-based ones like LLaMA and BLOOM. The first step is to identify the core neurons in a given layer of the model. This is done by computing the similarity between the activation patterns of all pairs of neurons in this layer. We then use the stability and similarity information to predict the activation pattern of each neuron. Once the activation pattern of a neuron is predicted, we can directly calculate its output value without performing forward or backward propagation on the whole model. The prediction accuracy of the activation pattern is very high for the core neurons. Therefore, the inference time of the large language models can be significantly reduced by using our approach.

CoreInfer: Accelerating Large Language Model Inference on a Consumer-Grade GPU

Large language models have achieved state-of-the-art results in many natural language processing tasks and are increasingly being used for various applications. However, the inference time of these large models is too long to be practical for real-world applications. This paper proposes CoreInfer, a novel approach that can accelerate the inference of large language models on consumer-grade GPUs by identifying and exploiting the core neurons in the model. The key idea is that there are some neurons with very similar activation patterns in different layers of the model. These core neurons are not only stable but also highly similar to each other. We call these two properties "stability" and "similarity". Our approach can be applied to any large language models, including transformer-based ones like LLaMA and BLOOM. The first step is to identify the core neurons

---

**Summary Statistics:**
- Input: 10,144 words (67,167 chars)
- Output: 1,741 words
- Compression: 0.17x
- Generation: 68.1s (25.6 words/sec)
- Quality Score: 0.40/1.0
- Attempts: 1

**Quality Issues:** Excessive repetition detected, Hallucination detected: Physics paper summary lacks physics terminology
