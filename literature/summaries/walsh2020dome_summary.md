# DOME: Recommendations for supervised machine learning validation in biology

**Authors:** Ian Walsh, Dmytro Fishman, Dario Garcia-Gasulla, Tiina Titma, Gianluca Pollastri, The ELIXIR Machine Learning focus group, Jen Harrow, Fotis E. Psomopoulos, Silvio C. E. Tosatto

**Year:** 2020

**Source:** arxiv

**Venue:** N/A

**DOI:** N/A

**PDF:** [walsh2020dome.pdf](../pdfs/walsh2020dome.pdf)

**Generated:** 2025-12-05 10:15:11

---

**Overview/Summary**

The paper "DOME: Recommendations for supervised machine learning validation in life science" is a comprehensive guide to the best practices and pitfalls of validating machine learning models in the life sciences. The authors provide recommendations based on their own experiences, as well as those from the Critical Assessment of Genome Interpretation (CAGI) challenges, which are a series of competitions that evaluate the performance of different deep learning models for predicting phenotypes from genomic data. In this paper, they discuss the importance of validation in machine learning and how it is often overlooked or done poorly. They then provide recommendations for the best practices to follow when validating machine learning models.

**Key Contributions/Findings**

The authors first highlight the need for a set of best practices for model validation. The authors point out that the CAGI challenges have been successful in bringing together researchers from different disciplines and institutions, but they have also highlighted the importance of performing well on the challenges as a way to validate one's own models. They then provide recommendations based on their own experiences, as well as those from the CAGI challenges. The authors emphasize that the best practices for validation are not unique to any particular type of model or data and that they can be applied in different contexts.

**Methodology/Approach**

The authors first discuss the importance of validation in machine learning. They then provide a set of recommendations based on their own experiences, as well as those from the CAGI challenges. The authors recommend that one should not only perform model validation but also use it to improve the performance of the models. The authors point out that the best practices for validation are not unique to any particular type of model or data and that they can be applied in different contexts.

**Results/Data**

The paper does not provide a set of results from the CAGI challenges, but rather provides a set of recommendations based on the experiences of the participants. The authors point out that the best practices for validation are not unique to any particular type of model or data and that they can be applied in different contexts.

**Limitations/Discussion**

The paper does not provide a set of results from the CAGI challenges, but rather provides a set of recommendations based on the experiences of the participants. The authors point out that the best practices for validation are not unique to any particular type of model or data and that they can be applied in different contexts.

**References**

The paper does not provide references.

---

**Summary Statistics:**
- Input: 7,800 words (60,374 chars)
- Output: 422 words
- Compression: 0.05x
- Generation: 25.6s (16.5 words/sec)
- Quality Score: 0.60/1.0
- Attempts: 1

**Quality Issues:** Hallucination detected: Physics paper summary lacks physics terminology
