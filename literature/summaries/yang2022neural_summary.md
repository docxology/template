# A Neural Active Inference Model of Perceptual-Motor Learning

**Authors:** Zhizhuo Yang, Gabriel J. Diaz, Brett R. Fajen, Reynold Bailey, Alexander Ororbia

**Year:** 2022

**Source:** arxiv

**Venue:** N/A

**DOI:** N/A

**PDF:** [yang2022neural.pdf](../pdfs/yang2022neural.pdf)

**Generated:** 2025-12-05 12:34:56

---

**Overview/Summary**

The paper presents a novel generalized active inference (AIF) model for studying online visually guided locomotion using an interception task where a moving target changes its speeds in a semi-predictable manner. The authors devise a problem-specific prior function, improving the agent's computational efficiency and interpretability. Notably, they find that their proposed AIF agent exhibits better task performance when compared to a commonly used reinforcement learning (RL) agent, i.e., the deep-Q network (DQN). The full AIF agent, containing both instrumental and epistemic components, exhibits slightly better task performance and lower variance compared to the AIF agent with only an instrumental component. Furthermore, they demonstrate behavioral differences among their full AIF agents given different discount factors $\gamma$ values as well as levels of the agent's action-to-speed responsiveness. Finally, they analyze the anticipatory behavior demonstrated by their agent and examine the differences between the agent's behavior and human behavior.

**Key Contributions/Findings**

The authors propose a novel generalized active inference (AIF) model for studying online visually guided locomotion using an interception task where a moving target changes its speeds in a semi-predictable manner. The full AIF agent, containing both instrumental and epistemic components, exhibits slightly better task performance and lower variance compared to the AIF agent with only an instrumental component. Furthermore, they demonstrate behavioral differences among their full AIF agents given different discount factors $\gamma$ values as well as levels of the agent's action-to-speed responsiveness. The authors also analyze the anticipatory behavior demonstrated by their agent and examine the differences between the agent's behavior and human behavior.

**Methodology/Approach**

The authors present a novel generalized active inference (AIF) model for studying online visually guided locomotion using an interception task where a moving target changes its speeds in a semi-predictable manner. The full AIF agent, containing both instrumental and epistemic components, exhibits slightly better task performance and lower variance compared to the AIF agent with only an instrumental component. Furthermore, they demonstrate behavioral differences among their full AIF agents given different discount factors $\gamma$ values as well as levels of the agent's action-to-speed responsiveness. Finally, they analyze the anticipatory behavior demonstrated by their agent and examine the differences between the agent's behavior and human behavior.

**Results/Data**

The authors find that the full AIF agent, containing both instrumental and epistemic components, exhibits slightly better task performance and lower variance compared to the AIF agent with only an instrumental component. Furthermore, they demonstrate behavioral differences among their full AIF agents given different discount factors $\gamma$ values as well as levels of the agent's action-to-speed responsiveness. The authors also analyze the anticipatory behavior demonstrated by their agent and examine the differences between the agent's behavior and human behavior.

**Limitations/Discussion**

The authors remark that future work should address the following limitations - first, inputs to our agent are deﬁned in a simpliﬁed vector space whereas sensory inputs to the humans that actually perform the interception task are visual in nature (i.e., the model should work directly with unstructured sensory data such as pixel values). They remark that a vision-based approach could facilitate the extraction of additional information and features that are useful for solving the interception task more reliably. Second, our simulations do not account for visuo-motor delays inherent to the human visual and motor systems, and that might be modeled using techniques like delayed Markov decision process formulations [49]. The authors also analyze the anticipatory behavior demonstrated by their agent and examine the differences between the agent's behavior and human behavior. While the authors' results are promising, future work should address the following limitations - ﬁrst, inputs to our agent are deﬁned in a simpliﬁed vector space whereas sensory inputs to the humans that actually perform the interception task are visual in nature (i.e., the model should work directly with unstructured sensory data such as pixel values). We remark that a vision-based approach could facilitate the extraction of additional information and features that are useful for solving the interception task more reliably. Second, our simulations do not account for visuo-motor delays inherent to the human visual and motor systems, and that might be modeled using techniques like delayed Markov decision process formulations [49].

---

**Summary Statistics:**
- Input: 9,156 words (57,067 chars)
- Output: 686 words
- Compression: 0.07x
- Generation: 34.6s (19.8 words/sec)
- Quality Score: 0.60/1.0
- Attempts: 1

**Quality Issues:** Hallucination detected: Physics paper summary lacks physics terminology
