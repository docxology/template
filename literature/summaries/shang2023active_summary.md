# Active Vision Reinforcement Learning under Limited Visual Observability

**Authors:** Jinghuan Shang, Michael S. Ryoo

**Year:** 2023

**Source:** arxiv

**Venue:** arXiv

**DOI:** N/A

**PDF:** [shang2023active.pdf](../pdfs/shang2023active.pdf)

**Generated:** 2025-12-03 04:59:48

---

**Overview/Summary**

The paper "Active Vision Reinforcement Learning under Limited Visual Observability" by [Authors] presents a novel approach to active vision reinforcement learning (AVRL) that can learn to manipulate objects in the world from visual observations with limited observability. The authors propose an on-policy, model-based AVRL algorithm for this problem and provide theoretical guarantees of its convergence. In the proposed method, the agent is allowed to take actions based on the current observation without knowing the future observation. This makes it more challenging than the classical RL setting where the agent can observe the full state space. The authors also propose a new model-based AVRL algorithm that can learn to manipulate objects in the world from visual observations with limited observability. They provide theoretical guarantees of its convergence and show that the proposed method is more efficient than the existing on-policy, model-free AVRL algorithms.

**Key Contributions/Findings**

The main contributions of this paper are:
1. The authors propose an on-policy, model-based AVRL algorithm for learning to manipulate objects in the world from visual observations with limited observability.
2. They provide theoretical guarantees of its convergence and show that the proposed method is more efficient than the existing on-policy, model-free AVRL algorithms.

**Methodology/Approach**

The authors first propose an on-policy, model-based AVRL algorithm for learning to manipulate objects in the world from visual observations with limited observability. The key idea of this approach is that the agent can take actions based on the current observation without knowing the future observation. This makes it more challenging than the classical RL setting where the agent can observe the full state space. The authors also propose a new model-based AVRL algorithm that can learn to manipulate objects in the world from visual observations with limited observability. They provide theoretical guarantees of its convergence and show that the proposed method is more efficient than the existing on-policy, model-free AVRL algorithms.

**Results/Data**

The authors evaluate their approach using two different settings: (1) a simple 2D environment where an object can be manipulated by moving it to one of four positions; (2) a 3D environment where an object can be manipulated by pushing or pulling. In the first setting, the proposed method is more efficient than the existing on-policy, model-free AVRL algorithms. In the second setting, the authors show that the proposed algorithm is more efficient than the existing on-policy, model-free AVRL algorithms in terms of the number of steps to learn to manipulate an object.

**Limitations/Discussion**

The limitations of this paper are:
1. The proposed method can only be used for learning to manipulate objects with limited observability.
2. The authors do not provide a detailed comparison of their approach and existing on-policy, model-free AVRL algorithms in the 3D environment.
3. The authors do not evaluate the performance of the proposed algorithm in a more complex scenario where the agent can observe the full state space but has to take actions based on the current observation.

**Additional Information**

The paper is well-organized and easy to follow. The authors provide all the relevant details that are necessary for understanding their approach. The theoretical analysis is also clear and provides useful insights into the proposed method.

---

**Summary Statistics:**
- Input: 12,916 words (79,479 chars)
- Output: 525 words
- Compression: 0.04x
- Generation: 35.7s (14.7 words/sec)
- Quality Score: 0.60/1.0
- Attempts: 1

**Quality Issues:** Hallucination detected: Physics paper summary lacks physics terminology
