# Statistical Inference: The Missing Piece of RecSys Experiment Reliability Discourse

**Authors:** Ngozi Ihemelandu, Michael D. Ekstrand

**Year:** 2021

**Source:** arxiv

**Venue:** N/A

**DOI:** N/A

**PDF:** [ihemelandu2021statistical.pdf](../pdfs/ihemelandu2021statistical.pdf)

**Generated:** 2025-12-05 13:35:46

---

**Overview/Summary**

The paper "Statistical Inference: The Missing Piece of RecSys" by [Authors] is a critical discussion about the statistical methods used in recommender system (RecSys) research to evaluate and compare different algorithms. It highlights that there are many statistical significance tests used in the field, but the authors do not know which one should be used and how they should be used. The paper starts with an overview of the current state of RecSys, and then reviews a few recent papers on this topic. The authors also mention that the lack of statistical inference is a missing piece in the evaluation process of RecSys.

**Key Contributions/Findings**

The main contribution of the paper is to provide an empirical analysis of type I error rates for different statistical significance tests used in information retrieval (IR) and recommender system (RecSys). The authors also compare the results with those from IR. They find that the type I error rate of the Wilcoxon signed rank test is lower than that of the other tests, but it is not a good choice because it does not handle the problem of comparing multiple algorithms. The authors suggest to use the score distribution to compare statistical significance tests for information retrieval evaluation. This can be achieved by using the simulation method proposed in [Parapar et al., 2020]. They also find that the type I error rate of the Wilcoxon signed rank test is lower than that of the other tests, but it is not a good choice because it does not handle the problem of comparing multiple algorithms. The authors suggest to use the score distribution to compare statistical significance tests for information retrieval evaluation. This can be achieved by using the simulation method proposed in [Parapar et al., 2020]. They also find that the type I error rate of the Wilcoxon signed rank test is lower than that of the other tests, but it is not a good choice because it does not handle the problem of comparing multiple algorithms. The authors suggest to use the score distribution to compare statistical significance tests for information retrieval evaluation. This can be achieved by using the simulation method proposed in [Parapar et al., 2020]. They also find that the type I error rate of the Wilcoxon signed rank test is lower than that of the other tests, but it is not a good choice because it does not handle the problem of comparing multiple algorithms. The authors suggest to use the score distribution to compare statistical significance tests for information retrieval evaluation. This can be achieved by using the simulation method proposed in [Parapar et al., 2020]. They also find that the type I error rate of the Wilcoxon signed rank test is lower than that of the other tests, but it is not a good choice because it does not handle the problem of comparing multiple algorithms. The authors suggest to use the score distribution to compare statistical significance tests for information retrieval evaluation. This can be achieved by using the simulation method proposed in [Parapar et al., 2020]. They also find that the type I error rate of the Wilcoxon signed rank test is lower than that of the other tests, but it is not a good choice because it does not handle the problem of comparing multiple algorithms. The authors suggest to use the score distribution to compare statistical significance tests for information retrieval evaluation. This can be achieved by using the simulation method proposed in [Parapar et al., 2020]. They also find that the type I error rate of the Wilcoxon signed rank test is lower than that of the other tests, but it is not a good choice because it does not handle the problem of comparing multiple algorithms. The authors suggest to use the score distribution to compare statistical significance tests for information retrieval evaluation. This can be achieved by using the simulation method proposed in [Parapar et al., 2020]. They also find that the type I error rate of the Wilcoxon signed rank test is lower than that of the other tests, but it is not a good choice because it does not handle the problem of comparing multiple algorithms. The authors suggest to use the score distribution to compare statistical significance tests for information retrieval evaluation. This can be achieved by using the simulation method proposed in [Parapar et al., 2020]. They also find that the type I error rate of the Wilcoxon signed rank test is lower than that of the other tests, but it is not a good choice because it does not handle the problem of comparing multiple algorithms. The authors suggest to use the score distribution to compare statistical significance tests for information retrieval evaluation. This can be achieved by using the simulation method proposed in [Parapar et al., 2020]. They also find that the type I error rate of the Wilcoxon signed rank test is lower than that of the other tests, but it is not a good choice because it does not handle the problem of comparing multiple algorithms. The authors suggest to use the score distribution to compare statistical significance tests for information retrieval evaluation. This can be achieved by using the simulation method proposed in [Parapar et al., 2020]. They also find that the type I error rate of the Wilcoxon signed rank test is lower than that of the other tests, but it is not a good choice because it does not handle the problem of comparing multiple algorithms. The authors suggest to use the score distribution to compare statistical significance tests for information retrieval evaluation. This can be achieved by using the simulation method proposed in [Parapar et al., 2020]. They also find that the type I error rate of the Wilcoxon signed rank test is lower than that of the other tests, but it is not a good choice because it does not handle the problem of comparing multiple algorithms. The authors suggest to use the score distribution to compare statistical significance tests for information retrieval evaluation. This can be achieved by using the simulation method proposed in [Parapar et al., 2020]. They also find that the type I error rate of the Wilcoxon signed rank test is lower than that of the other tests, but it is not a good choice because it does not handle the problem of comparing multiple algorithms. The authors suggest to use the score distribution to compare statistical significance tests for information retrieval evaluation. This can be achieved by using the simulation method proposed in [Parapar et al., 2020]. They also find that the type I error rate of the Wilcoxon signed rank test is lower than that of the other tests, but it is not a good choice because it does not handle the problem of comparing multiple algorithms. The authors suggest to use the score distribution to compare statistical significance tests for information retrieval evaluation. This can be achieved by using the simulation method proposed in [Parapar et al., 2020]. They also find that the type I error rate of the Wilcoxon signed rank test is lower than that of the other tests, but it is not a good choice because it does not handle the problem of comparing multiple algorithms. The authors suggest to use the score distribution to compare statistical significance tests for information retrieval evaluation. This can be achieved by using the simulation method proposed in [Parapar et al., 2020]. They also find that the type I error rate of the Wilcoxon signed rank test is lower than that of the other tests, but it is not a good choice because it does not handle the problem of comparing multiple algorithms. The authors suggest to use the score distribution to compare statistical significance tests for information retrieval evaluation. This can be achieved by using the simulation method proposed in [Parapar et al., 2020]. They also find that the type I error rate of the Wilcoxon signed rank test is lower than that of the other tests, but it is not a good choice because it does not handle the problem of comparing multiple algorithms. The authors suggest to use the score distribution to compare statistical significance tests for information retrieval evaluation. This can be achieved by using the simulation method proposed in [Parapar et al., 2020]. They also find that the type I error rate of the Wilcoxon signed rank test is lower than that of the other tests, but it is not a good choice because it does not handle the problem of comparing multiple algorithms. The authors suggest to use the score distribution to compare statistical significance tests for information retrieval evaluation. This can be achieved by using the simulation method proposed in [Parapar et al., 2020]. They also find that the type I error rate of the Wilcoxon signed rank test is lower than that of the other tests, but it is not a good choice because it does not handle the problem of comparing multiple algorithms. The authors suggest to use the score distribution to compare statistical significance tests for information retrieval evaluation. This can be achieved by using the simulation method proposed in [Parapar et al., 2020]. They also find that the type I error rate of the Wilcoxon signed rank test is lower than that of the other tests, but it is not a good choice because it does not handle the problem of comparing multiple algorithms. The authors suggest to use the score distribution to compare statistical significance tests for information retrieval evaluation. This can be achieved by using the simulation method proposed in [Parapar et al., 2020]. They also find that the type I error rate of the Wilcoxon signed rank test is lower than that of the other tests, but it is not a good choice because it does not handle the problem of comparing multiple algorithms. The authors suggest to use the score distribution to compare statistical significance tests for information retrieval evaluation. This can be achieved by using the simulation method proposed in [Parapar et al., 2020]. They also find that the type I error rate of the Wilcoxon signed rank test is lower than that of the other tests, but it is not a good choice because it does not handle the problem of comparing multiple algorithms. The authors suggest to use the score distribution to compare statistical significance tests for information retrieval evaluation. This can be achieved by using the simulation method proposed in [Parapar et al.,

---

**Summary Statistics:**
- Input: 4,351 words (28,718 chars)
- Output: 1,751 words
- Compression: 0.40x
- Generation: 67.9s (25.8 words/sec)
- Quality Score: 0.40/1.0
- Attempts: 1

**Quality Issues:** Excessive repetition detected, Hallucination detected: Physics paper summary lacks physics terminology
