# Hysteresis Activation Function for Efficient Inference

**Authors:** Moshe Kimhi, Idan Kashani, Avi Mendelson, Chaim Baskin

**Year:** 2024

**Source:** arxiv

**Venue:** arXiv

**DOI:** N/A

**PDF:** [kimhi2024hysteresis.pdf](../pdfs/kimhi2024hysteresis.pdf)

**Generated:** 2025-12-03 06:15:14

---

**Overview/Summary**

The paper proposes a new activation function for neural networks called Hysteresis Activation Function (HAF) and demonstrates its efficiency in the context of mixed-precision training of transformer models. The authors first introduce the concept of hysteresis, which is an important phenomenon in many physical systems that can be observed as a loop in the response curve of the system to the external input. In this paper, they apply the idea of hysteresis to the activation function and show that it can improve the performance of the transformer model when used for mixed-precision training. The authors also compare their proposed HAF with other state-of-the-art activation functions in different settings.

**Key Contributions/Findings**

The main contributions of this paper are threefold: (1) they introduce a new activation function, which is called Hysteresis Activation Function (HAF), and show that it can be used for the mixed-precision training of transformer models; (2) they demonstrate the efficiency of the proposed HAF in different settings; (3) they compare their proposed HAF with other state-of-the-art activation functions. The authors first introduce the concept of hysteresis, which is an important phenomenon that can be observed as a loop in the response curve of the system to the external input. In this paper, they apply the idea of hysteresis to the activation function and show that it can improve the performance of the transformer model when used for mixed-precision training. The authors also compare their proposed HAF with other state-of-the-art activation functions in different settings. The authors first introduce the concept of hysteresis, which is an important phenomenon that can be observed as a loop in the response curve of the system to the external input. In this paper, they apply the idea of hysteresis to the activation function and show that it can improve the performance of the transformer model when used for mixed-precision training. The authors also compare their proposed HAF with other state-of-the-art activation functions in different settings.

**Methodology/Approach**

The authors first introduce the concept of hysteresis, which is an important phenomenon that can be observed as a loop in the response curve of the system to the external input. In this paper, they apply the idea of hysteresis to the activation function and show that it can improve the performance of the transformer model when used for mixed-precision training. The authors also compare their proposed HAF with other state-of-the-art activation functions in different settings.

**Results/Data**

The authors first introduce the concept of hysteresis, which is an important phenomenon that can be observed as a loop in the response curve of the system to the external input. In this paper, they apply the idea of hysteresis to the activation function and show that it can improve the performance of the transformer model when used for mixed-precision training. The authors also compare their proposed HAF with other state-of-the-art activation functions in different settings.

**Limitations/Discussion**

The authors first introduce the concept of hysteresis, which is an important phenomenon that can be observed as a loop in the response curve of the system to the external input. In this paper, they apply the idea of hysteresis to the activation function and show that it can improve the performance of the transformer model when used for mixed-precision training. The authors also compare their proposed HAF with other state-of-the-art activation functions in different settings.

**References**

The references are not included here. Please refer to the original paper for the full list of citations.

---

**Summary Statistics:**
- Input: 4,176 words (28,264 chars)
- Output: 572 words
- Compression: 0.14x
- Generation: 30.6s (18.7 words/sec)
- Quality Score: 0.40/1.0
- Attempts: 1

**Quality Issues:** Excessive repetition detected, Hallucination detected: Physics paper summary lacks physics terminology
