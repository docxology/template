# Approximate Decentralized Bayesian Inference

**Authors:** Trevor Campbell, Jonathan P. How

**Year:** 2014

**Source:** arxiv

**Venue:** arXiv

**DOI:** N/A

**PDF:** [campbell2014approximate.pdf](../pdfs/campbell2014approximate.pdf)

**Generated:** 2025-12-03 05:38:57

---

**Overview/Summary**

The paper introduces a new algorithm called Approximate Merging of Posters (AMPS) for approximate decentralized variational inference in distributed Bayesian networks. The authors describe the AMPS algorithm and demonstrate its advantages with respect to batch learning, distributed learning, and nonparametric models. The main contributions are the modeling and computational advantages of the AMPS algorithm.

**Key Contributions/Findings**

The key findings of this paper are the following: (1) the AMPS algorithm is a new approximate decentralized variational inference method that can be used in ad-hoc, asynchronous, and dynamic networks; (2) the authors demonstrate the modeling and computational advantages of the AMPS algorithm with respect to batch and distributed learning; and (3) there is certainly room for improvement of the AMPS algorithm. The paper also motivates future work on finding bounds on the performance of such algorithms with respect to the true AMPS optimal solution.

**Methodology/Approach**

The authors describe the AMPS algorithm, which may be used in ad-hoc, asynchronous, and dynamic networks. Experiments demonstrate the modeling and computational advantages of the AMMS algorithm with respect to batch and distributed learning. Motivated by the examples in Section 3, there is certainly room for improvement of the AMMS algorithm. The authors also motivate future work on finding bounds on the performance of such algorithms with respect to the true AMMS optimal solution.

**Results/Data**

The results of this paper are the following: (1) the approximate optimization algorithms presented herein work well in practice; and (2) the approximate log likelihoods on the held-out test set by a small margin. The authors also motivate future work on extending the AMPS for use with Bayesian nonparametric models, which is of interest for cases when the number of latent parameters is unknown a priori, or when there is the possibility that agents learn disparate sets of latent parameters that are not well-combined by optimizing over permutations.

**Limitations/Discussion**

The main weaknesses and future work of this paper are: (1) it may be possible to reduce the computational cost of AMMS by using a hierarchical optimization scheme, rather than the monolithic approach used in most of the examples presented in the paper; and (2) extending AMPS for use with Bayesian nonparametric models is of interest for cases when the number of latent parameters is unknown a priori, or when there is the possibility that agents learn disparate sets of latent parameters that are not well-combined by optimizing over permutations. The authors also motivate future work on finding bounds on the performance of such algorithms with respect to the true AMMS optimal solution.

**References**

The references for this paper are: (1) A. Asuncion, P. Smyth, and M. Welling. Asynchronous distributed learning of topic models. In Advances in Neural Information Processing Systems 21, 2008; (2) P. Belhumeur, J. Hespanha, and D. Kriegman. Eigenfaces vs. ﬁsh-feraces: Recognition using class speciﬁc linear projection. IEEE Trans. on Pattern Analysis in Machine Intelligence , 19:711–720, 1997; (3) D. M. Blei, A. Y  Ng, and M. I. Jordan. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993–1022, 2003; (4) T. Broderick, N. Boyd, A. Wibisono, A. C. Wilson, and M. I. Jordan. Streaming variational bayes. In Advances in Neural Information Procesing Systems 26, 2013; (5) J. Dean and S. Ghemawat. Mapreduce: Simpliﬁed data process- ing on large clusters. In 6th Symposium on Operating Systems Design and Implementation, 2004; (6) F. Nielsen and V  Garcia. Statistical exponential families: A digest with ﬂash cards. arXiv:0911.4853v2, 2011; (7) F. Niu, B. Recht, C. R ´e, and S. J. Wright. Hogwild!: A lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Procesing Systems 26, 2011; (8) X. Pan, J. Gonzalez, S. Jegelka, T. Broderick, and M. I. Jordan. Optimistic concurrency control for distributed unsuper-vised learning. In Advances in Neural Information Procesing Systems 26, 2013; (9) M. Rosencrantz, G. Gordon, and S. Thrun. Decentralized sensor fusion with distributed particle ﬁlters. In Proceedings of the 19th Conference on Uncertainty in Artiﬁcial Intelligence , 2003; (10) M. Stephens. Dealing with label switching in mixture models. Journal of the Royal Statistical Society: Series B , 62(4):795–809, 2000; and (11) C. Wang, J. Paisley, and D. M. Blei. Online variational inference for the hierarchical dirichlet process. In Proceedings of the 11th International Conference on Artiﬁcial Intelligence and Statistics, 2011.

**END OF PAPER SUMMARY**

Please let me know if this meets your requirements!

---

**Summary Statistics:**
- Input: 6,220 words (37,918 chars)
- Output: 718 words
- Compression: 0.12x
- Generation: 44.8s (16.0 words/sec)
- Quality Score: 0.60/1.0
- Attempts: 1

**Quality Issues:** Hallucination detected: Physics paper summary lacks physics terminology
