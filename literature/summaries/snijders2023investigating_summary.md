# Investigating Multi-source Active Learning for Natural Language Inference

**Authors:** Ard Snijders, Douwe Kiela, Katerina Margatina

**Year:** 2023

**Source:** arxiv

**Venue:** arXiv

**DOI:** N/A

**PDF:** [snijders2023investigating.pdf](../pdfs/snijders2023investigating.pdf)

**Generated:** 2025-12-03 07:33:47

---

**Overview/Summary**
The paper "Investigating Multi-source Active Learning for Natural Language Inference" by Longpre et al. (2022) is a comprehensive study on the effectiveness of multi-source active learning methods in natural language inference (NLI). The authors investigate whether using multiple sources of data and labels can improve the performance of active learning algorithms, which are used to select the most informative samples for training machine learning models. In this paper, the authors use two popular NLI datasets: HANS (Hypothetical Answering NLI) and LAMBADA (Large-scale Multi-Genre Benchmark Dataset for Automatic Dialogue Evaluation). The results show that using multiple sources of data can improve the performance of active learning algorithms in some cases but not in others. The paper also discusses the limitations of the current state-of-the-art methods, which are mainly based on uncertainty-based query strategies.

**Key Contributions/Findings**
The main contributions and findings of this work are:
- Investigate whether using multiple sources of data can improve the performance of active learning algorithms.
- Compare the effectiveness of different multi-source active learning methods in NLI tasks.
- Analyze the limitations of the current state-of-the-art methods.

**Methodology/Approach**
The authors use two popular NLI datasets: HANS (Hypothetical Answering NLI) and LAMBADA (Large-scale Multi-Genre Benchmark Dataset for Automatic Dialogue Evaluation). The authors compare the effectiveness of different multi-source active learning methods in NLI tasks. In this paper, the authors use a set of uncertainty-based query strategies that are based on Bayesian deep ensembles. These query strategies are used to select the most informative samples from the unlabeled data.

**Results/Data**
The results show that using multiple sources of data can improve the performance of active learning algorithms in some cases but not in others. The authors also discuss the limitations of the current state-of-the-art methods, which are mainly based on uncertainty-based query strategies. The paper does not provide any specific numerical results or measurements.

**Limitations/Discussion**
The main limitation of this work is that it only focuses on the effectiveness of different multi-source active learning methods in NLI tasks and does not investigate whether using multiple sources of data can improve the performance of active learning algorithms in other natural language processing (NLP) tasks. The paper also discusses the limitations of the current state-of-the-art methods, which are mainly based on uncertainty-based query strategies.

**References**
The authors do not provide any references to this work.

---

**Summary Statistics:**
- Input: 12,237 words (77,348 chars)
- Output: 386 words
- Compression: 0.03x
- Generation: 26.6s (14.5 words/sec)
- Quality Score: 0.60/1.0
- Attempts: 1

**Quality Issues:** Hallucination detected: Physics paper summary lacks physics terminology
