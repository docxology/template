# Active Regression by Stratification

**Authors:** Sivan Sabato, Remi Munos

**Year:** 2014

**Source:** arxiv

**Venue:** arXiv

**DOI:** N/A

**PDF:** [sabato2014active.pdf](../pdfs/sabato2014active.pdf)

**Generated:** 2025-12-03 05:27:23

---

**Overview/Summary**

The paper "Active Regression by Stratification" presents a new active regression algorithm for learning an unknown linear transformation from a high-dimensional input space to a lower-dimensional output space. The authors consider the problem of estimating the best possible linear predictor when the data is not available, but only samples are available. In this case, the goal is to find the best possible linear predictor based on the distribution of the data. This problem is called active regression and it has been studied in many papers. However, existing methods for active regression require that the input space is low-dimensional or the noise is sub-Gaussian. The authors' main contribution is a new algorithm for active regression when the high-dimensional input space is not necessarily low-dimensional and the noise is not necessarily sub-Gaussian. In this case, the distribution of the data is only available through samples. The paper also provides an upper bound on the excess risk of the proposed algorithm.

**Key Contributions/Findings**

The authors provide a new active regression algorithm for learning an unknown linear transformation from a high-dimensional input space to a lower-dimensional output space. The main contribution of this paper is that the authors' algorithm does not require any condition on the dimensionality of the input space or the distribution of the noise. The authors also provide an upper bound on the excess risk of their algorithm, which shows that the proposed algorithm can achieve the best possible linear predictor with a sample size that depends only on the target class and the number of samples. This is different from existing methods for active regression.

**Methodology/Approach**

The paper first introduces the problem of active regression. The authors then compare the existing algorithms for active regression, which are all based on the following two assumptions: 1) the input space is low-dimensional; and 2) the noise is sub-Gaussian. The authors show that these two assumptions are necessary to obtain a good upper bound on the excess risk of the algorithm. Then the paper proposes a new algorithm for active regression, which does not require any condition on the dimensionality of the input space or the distribution of the noise. The proposed algorithm can be seen as an extension of the existing algorithms. The authors also provide an upper bound on the excess risk of their algorithm, which shows that the proposed algorithm can achieve the best possible linear predictor with a sample size that depends only on the target class and the number of samples.

**Results/Data**

The paper provides an upper bound on the excess risk of the proposed algorithm. This is different from existing methods for active regression. The authors' main contribution is that the authors' algorithm does not require any condition on the dimensionality of the input space or the distribution of the noise. The authors also provide an upper bound on the excess risk of their algorithm, which shows that the proposed algorithm can achieve the best possible linear predictor with a sample size that depends only on the target class and the number of samples.

**Limitations/Discussion**

The paper does not discuss any limitations or future work.

---

**Summary Statistics:**
- Input: 5,580 words (32,450 chars)
- Output: 519 words
- Compression: 0.09x
- Generation: 28.8s (18.0 words/sec)
- Quality Score: 0.60/1.0
- Attempts: 1

**Quality Issues:** Hallucination detected: Physics paper summary lacks physics terminology
