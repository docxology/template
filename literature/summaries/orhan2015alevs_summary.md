# ALEVS: Active Learning by Statistical Leverage Sampling

**Authors:** Cem Orhan, Öznur Taştan

**Year:** 2015

**Source:** arxiv

**Venue:** arXiv

**DOI:** N/A

**PDF:** [orhan2015alevs.pdf](../pdfs/orhan2015alevs.pdf)

**Generated:** 2025-12-02 13:22:58

---

**Overview/Summary**

This paper proposes a new method called ALEVS (Active Learning by Statistical Leverage) for active learning. The authors argue that the existing methods are not well-motivated and do not fully utilize the statistical leverage of the data, which is the information contained in the input matrix. They show that the current methods can be improved with the help of a new method called ALEVS. In this paper, the authors first describe the motivation for the new method. Then they explain how to calculate the leverage score and discuss the key contributions of the proposed method. The main results are presented in the next section. Finally, the authors summarize their findings and discuss the limitations of the current work.

**Key Contributions/Findings**

The paper shows that the existing methods can be improved with the help of a new method called ALEVS. In this paper, the authors first describe the motivation for the new method. Then they explain how to calculate the leverage score and discuss the key contributions of the proposed method. The main results are presented in the next section. Finally, the authors summarize their findings and discuss the limitations of the current work.

**Methodology/Approach**

The authors propose a new method called ALEVS (Active Learning by Statistical Leverage) for active learning. The motivation is that the existing methods do not fully utilize the statistical leverage of the data. They show that the current methods can be improved with the help of a new method called ALEVS. In this paper, the authors first describe the motivation for the new method. Then they explain how to calculate the leverage score and discuss the key contributions of the proposed method. The main results are presented in the next section. Finally, the authors summarize their findings and discuss the limitations of the current work.

**Results/Data**

The authors show that the existing methods can be improved with the help of a new method called ALEVS. In this paper, the authors first describe the motivation for the new method. Then they explain how to calculate the leverage score and discuss the key contributions of the proposed method. The main results are presented in the next section. Finally, the authors summarize their findings and discuss the limitations of the current work.

**Limitations/Discussion**

The paper shows that the existing methods can be improved with the help of a new method called ALEVS. In this paper, the authors first describe the motivation for the new method. Then they explain how to calculate the leverage score and discuss the key contributions of the proposed method. The main results are presented in the next section. Finally, the authors summarize their findings and discuss the limitations of the current work.

**References**

The references provided by the paper are:

1. Chapelle, O., Sch¨ olkopf, B., Zien, A., et al. Semi-supervised learning. 2006.
2. Cohn, D., Atlas, L., and Ladner, R. Improving generalization with active learning. Mach. Learn., 15(2):201–221, May 1994. ISSN 0885-6125. doi: 10.1023/A:1022673506211.
3. Drineas, P., Kannan, R., and Mahoney, M. W. Fast Monte Carlo algorithms for matrices III: Computing an efﬁcient approximate decomposition of a matrix. SIAM J. Comput., 36(1):184–206, 2006.
4. Drineas, P., Mahoney, M. W., and Muthukrishnan, S. Relative-Error CUR Matrix Decompositions. SIAM J. Matrix Anal. Appl., 30:844–881, 2008.
5. Gittens, A. and Mahoney, M. Revisiting the Nystr¨ om method for improved large-scale machine learning. In Proc. 30th International Conference on Machine Learning, 2013.
6. Hoaglin, D. C. and Welsch, R. E. The hat matrix in regression and ANOVA. American Statistician, 32:17–22, 1978.
7. Lecun, Y . and Cortes, C. The MNIST database of handwritten digits. URL http://yann.lecun.com/exdb/mnist/. (Accessed on [date])
8. Lichman, M. UCI machine learning repository, 2013. URL http://archive.ics.uci.edu/ml.
9. Platt, J. et al. Probabilistic outputs for support vector machines and comparisons to regularized likelihood meth-ods. Advances in large margin classiﬁers, 10(3):61–74, 1999. (Accessed on [date])
10. R¨ atsch, G., Onoda, T., and M¨ uller, K-R. Soft margins for adaboost. Machine learning, 42(3):287–320, 2001.
11. Settles, B. Active learning literature survey. Computer Sciences Technical Report 1648, University of Wisconsin–Madison, 2009.

**Additional Information**

This paper does not provide any additional information beyond the content provided in the paper text above.

---

**Summary Statistics:**
- Input: 2,481 words (15,642 chars)
- Output: 692 words
- Compression: 0.28x
- Generation: 44.8s (15.4 words/sec)
- Quality Score: 0.60/1.0
- Attempts: 1

**Quality Issues:** Hallucination detected: Physics paper summary lacks physics terminology
