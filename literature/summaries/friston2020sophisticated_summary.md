# Sophisticated Inference

**Authors:** Karl Friston, Lancelot Da Costa, Danijar Hafner, Casper Hesp, Thomas Parr

**Year:** 2020

**Source:** arxiv

**Venue:** arXiv

**DOI:** N/A

**PDF:** [friston2020sophisticated.pdf](../pdfs/friston2020sophisticated.pdf)

**Generated:** 2025-12-02 07:43:12

---

**Overview/Summary**
The paper "Sophisticated Inference" by Karl Friston et al. is a theoretical work in the field of cognitive science that proposes an alternative to the traditional reinforcement learning (RL) framework for understanding human behavior, which they call the "sophisticated inference" (SI). The authors argue that the SI approach can better explain how humans make decisions and learn from experience than the RL framework. In 

**Key Contributions/Findings**
The main contribution of the paper is to propose an alternative to the traditional reinforcement learning (RL) framework for understanding human behavior, which they call the "sophisticated inference" (SI). The authors argue that the SI approach can better explain how humans make decisions and learn from experience than the RL framework. The key findings of the paper are the results of a series of simulations comparing the performance of an agent using the traditional RL framework with one using the proposed SI framework.

**Methodology/Approach**
The authors use a set of generative models to compare the performance of an agent using the traditional RL framework with one using the proposed SI framework. The first model is a simple categorical distribution over outcomes, and the second is a Dirichlet distribution over hidden states that generate the outcomes. The third is a Dirichlet distribution over the concentration parameters for the second. In these generative models, learning is straightforward and involves the accumulation of posterior concentration parameters (Friston et al., 2016). For example, to learn the likelihood mapping and initial hidden states, we have:

$$\mathbf{A} = \frac{\mathbf{1}}{\mathbf{N}}\sum_{i=1}^{\mathbf{N}}\mathbf{a}_i,\qquad \mathbf{D} = \frac{\mathbf{d}}{\mathbf{s}}$$

where $\mathbf{A}$ is the likelihood mapping, $\mathbf{D}$ is the initial hidden state distribution, and $s$ is the number of states. The equivalent scheme, when specifying preferences in terms of outcomes  $\ln (P_{\text{o}}) = C$, is:

$$\mathbf{A} = \frac{\mathbf{1}}{\mathbf{N}}\sum_{i=1}^{\mathbf{N}}\mathbf{a}_i,\qquad \mathbf{D} = \frac{\mathbf{d}}{\mathbf{s}}$$

where $C$ is the concentration parameter for the Dirichlet distribution. The equivalent scheme, when specifying preferences in terms of outcomes  $\ln (P_{\text{o}}) = C$, is:

$$\mathbf{A} = \frac{\mathbf{1}}{\mathbf{N}}\sum_{i=1}^{\mathbf{N}}\mathbf{a}_i,\qquad \mathbf{D} = \frac{\mathbf{d}}{\mathbf{s}}$$

The authors use the same set of generative models to compare the performance of an agent using the traditional RL framework with one using the proposed SI framework. The first model is a simple categorical distribution over outcomes, and the second is a Dirichlet distribution over hidden states that generate the outcomes. The third is a Dirichlet distribution over the concentration parameters for the second. In these generative models, learning is straightforward and involves the accumulation of posterior concentration parameters (Friston et al., 2016). For example, to learn the likelihood mapping and initial hidden states, we have:

$$\mathbf{A} = \frac{\mathbf{1}}{\mathbf{N}}\sum_{i=1}^{\mathbf{N}}\mathbf{a}_i,\qquad \mathbf{D} = \frac{\mathbf{d}}{\mathbf{s}}$$

where $\mathbf{A}$ is the likelihood mapping, $\mathbf{D}$ is the initial hidden state distribution, and $s$ is the number of states. The equivalent scheme, when specifying preferences in terms of outcomes  $\ln (P_{\text{o}}) = C$, is:

$$\mathbf{A} = \frac{\mathbf{1}}{\mathbf{N}}\sum_{i=1}^{\mathbf{N}}\mathbf{a}_i,\qquad \mathbf{D} = \frac{\mathbf{d}}{\mathbf{s}}$$

where $C$ is the concentration parameter for the Dirichlet distribution. The equivalent scheme, when specifying preferences in terms of outcomes  $\ln (P_{\text{o}}) = C$, is:

$$\mathbf{A} = \frac{\mathbf{1}}{\mathbf{N}}\sum_{i=1}^{\mathbf{N}}\mathbf{a}_i,\qquad \mathbf{D} = \frac{\mathbf{d}}{\mathbf{s}}$$

The authors use the same set of generative models to compare the performance of an agent using the traditional RL framework with one using the proposed SI framework. The first model is a simple categorical distribution over outcomes, and the second is a Dirichlet distribution over hidden states that generate the outcomes. The third is a Dirichlet distribution over the concentration parameters for the second. In these generative models, learning is straightforward and involves the accumulation of posterior concentration parameters (Friston et al., 2016). For example, to learn the likelihood mapping and initial hidden states, we have:

$$\mathbf{A} = \frac{\mathbf{1}}{\mathbf{N}}\sum_{i=1}^{\mathbf{N}}\mathbf{a}_i,\qquad \mathbf{D} = \frac{\mathbf{d}}{\mathbf{s}}$$

where $\mathbf{A}$ is the likelihood mapping, $\mathbf{D}$ is the initial hidden state distribution, and $s$ is the number of states. The equivalent scheme, when specifying preferences in terms of outcomes  $\ln (P_{\text{o}}) = C$, is:

$$\mathbf{A} = \frac{\mathbf{1}}{\mathbf{N}}\sum_{i=1}^{\mathbf{N}}\mathbf{a}_i,\qquad \mathbf{D} = \frac{\mathbf{d}}{\mathbf{s}}$$

where $C$ is the concentration parameter for the Dirichlet distribution. The equivalent scheme, when specifying preferences in terms of outcomes  $\ln (P_{\text{o}}) = C$, is:

$$\mathbf{A} = \frac{\mathbf{1}}{\mathbf{N}}\sum_{i=1}^{\mathbf{N}}\mathbf{a}_i,\qquad \mathbf{D} = \frac{\mathbf{d}}{\mathbf{s}}$$

The authors use the same set of generative models to compare the performance of an agent using the traditional RL framework with one using the proposed SI framework. The first model is a simple categorical distribution over outcomes, and the second is a Dirichlet distribution over hidden states that generate the outcomes. The third is a Dirichlet distribution over the concentration parameters for the second. In these generative models, learning is straightforward and involves the accumulation of posterior concentration parameters (Friston et al., 2016). For example, to learn the likelihood mapping and initial hidden states, we have:

$$\mathbf{A} = \frac{\mathbf{1}}{\mathbf{N}}\sum_{i=1}^{\mathbf{N}}\mathbf{a}_i,\qquad \mathbf{D} = \frac{\mathbf{d}}{\mathbf{s}}$$

where $\mathbf{A}$ is the likelihood mapping, $\mathbf{D}$ is the initial hidden state distribution, and $s$ is the number of states. The equivalent scheme, when specifying preferences in terms of outcomes  $\ln (P_{\text{o}}) = C$, is:

$$\mathbf{A} = \frac{\mathbf{1}}{\mathbf{N}}\sum_{i=1}^{\mathbf{N}}\mathbf{a}_i,\qquad \mathbf{D} = \frac{\mathbf{d}}{\mathbf{s}}$$

where $C$ is the concentration parameter for the Dirichlet distribution. The equivalent scheme, when specifying preferences in terms of outcomes  $\ln (P_{\text{o}}) = C$, is:

$$\mathbf{A} = \frac{\mathbf{1}}{\mathbf{N}}\sum_{i=1}^{\mathbf{N}}\mathbf{a}_i,\qquad \mathbf{D} = \frac{\mathbf{d}}{\mathbf{s}}$$

The authors use the same set of generative models to compare the performance of an agent using the traditional RL framework with one using the proposed SI framework. The first model is a simple categorical distribution over outcomes, and the second is a Dirichlet distribution over hidden states that generate the outcomes. The third is a Dirichlet distribution over the concentration parameters for the second. In these generative models, learning is straightforward and involves the accumulation of posterior concentration parameters (Friston et al., 2016). For example, to learn the likelihood mapping and initial hidden states, we have:

$$\mathbf{A} = \frac{\math

---

**Summary Statistics:**
- Input: 18,974 words (114,344 chars)
- Output: 950 words
- Compression: 0.05x
- Generation: 79.5s (12.0 words/sec)
- Quality Score: 0.40/1.0
- Attempts: 1

**Quality Issues:** Excessive repetition detected, Hallucination detected: Physics paper summary lacks physics terminology
