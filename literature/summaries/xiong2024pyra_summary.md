# PYRA: Parallel Yielding Re-Activation for Training-Inference Efficient Task Adaptation

**Authors:** Yizhe Xiong, Hui Chen, Tianxiang Hao, Zijia Lin, Jungong Han, Yuesong Zhang, Guoxin Wang, Yongjun Bao, Guiguang Ding

**Year:** 2024

**Source:** arxiv

**Venue:** N/A

**DOI:** N/A

**PDF:** [xiong2024pyra.pdf](../pdfs/xiong2024pyra.pdf)

**Generated:** 2025-12-05 13:18:54

---

Overview/Summary:
The paper presents a novel method for efficient generalization of pre-trained vision-language (VL) models on new datasets with minimal additional parameters. The authors propose an adapter, called Parallel Yielding Re-Activation (PYRA), which is trained in parallel to the model and can be used to adapt any VL model to a target dataset without requiring any additional training data or fine-tuning. They also provide a set of prompts that are designed to be easily compressible for efficient generalization. The authors show that their method can achieve better performance than previous state-of-the-art methods on several benchmark datasets.

Key Contributions/Findings:
The main contributions of the paper are the design and evaluation of PYRA, an adapter that is trained in parallel to the model and can be used to adapt any VL model to a target dataset without requiring any additional training data or fine-tuning. The authors also provide a set of prompts that are designed to be easily compressible for efficient generalization. They show that their method can achieve better performance than previous state-of-the-art methods on several benchmark datasets.

Methodology/Approach:
The authors first discuss the challenges in existing prompt-based adaptation methods, and then present PYRA, which is trained in parallel to the model and can be used to adapt any VL model to a target dataset without requiring any additional training data or fine-tuning. They also provide a set of prompts that are designed to be easily compressible for efficient generalization. The authors show that their method can achieve better performance than previous state-of-the-art methods on several benchmark datasets.

Results/Data:
The authors evaluate the proposed method on six benchmark datasets, including VQA, CLEVR, GQA, and two datasets from the EuroSAT dataset. They compare the proposed method with several previous state-of-the-art methods, including LLaMA, LORA, and FATE. The results show that the proposed method can achieve better performance than previous state-of-the-art methods on these benchmark datasets.

Limitations/Discussion:
The authors discuss the limitations of their work, which is the lack of a unified evaluation protocol for prompt-based adaptation. They also suggest future research directions to improve the efficiency and effectiveness of the proposed method.

---

**Summary Statistics:**
- Input: 11,770 words (79,346 chars)
- Output: 350 words
- Compression: 0.03x
- Generation: 23.3s (15.0 words/sec)
- Quality Score: 1.00/1.0
- Attempts: 1

**Quality Check:** Passed
