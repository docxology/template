# Adaptive patch foraging in deep reinforcement learning agents

**Authors:** Nathan J. Wispinski, Andrew Butcher, Kory W. Mathewson, Craig S. Chapman, Matthew M. Botvinick, Patrick M. Pilarski

**Year:** 2022

**Source:** arxiv

**Venue:** arXiv

**DOI:** N/A

**PDF:** [wispinski2022adaptive.pdf](../pdfs/wispinski2022adaptive.pdf)

**Generated:** 2025-12-02 11:10:40

---

**Overview/Summary**
Title: Adaptive patch foraging in deep reinforcement learning agents

The authors of this paper present a new approach to improve the performance of deep reinforcement learning (DRL) agents by introducing an adaptive patch foraging mechanism that is able to learn about the environment and adapt its policy based on the available information. The proposed method is designed to be used as a drop-in replacement with existing exploration strategies in DRL algorithms, such as epsilon-greedy or entropy-based methods. The authors first introduce the concept of the marginal value theorem (MVT) which provides an upper bound for the expected return that can be obtained by any policy and is equal to the maximum value that can be achieved by a greedy strategy. They then show how this MVT can be used in DRL algorithms to improve their performance. The authors also present some experiments based on the Atari 2600 games benchmark, which shows that the proposed method outperforms existing methods.

**Key Contributions/Findings**
The main contributions of this paper are: 
- The authors introduce a new adaptive patch foraging mechanism that is able to learn about the environment and adapt its policy based on the available information. This approach can be used as an alternative to the epsilon-greedy or entropy-based exploration strategies in DRL algorithms.
- The proposed method is designed to be used as a drop-in replacement with existing exploration strategies in DRL algorithms, such as epsilon-greedy or entropy-based methods.

**Methodology/Approach**
The authors first introduce the concept of the marginal value theorem (MVT) which provides an upper bound for the expected return that can be obtained by any policy and is equal to the maximum value that can be achieved by a greedy strategy. They then show how this MVT can be used in DRL algorithms to improve their performance. The authors also present some experiments based on the Atari 2600 games benchmark, which shows that the proposed method outperforms existing methods.

**Results/Data**
The main results of this paper are: 
- The authors first introduce a new adaptive patch foraging mechanism that is able to learn about the environment and adapt its policy based on the available information. This approach can be used as an alternative to the epsilon-greedy or entropy-based exploration strategies in DRL algorithms.
- The proposed method is designed to be used as a drop-in replacement with existing exploration strategies in DRL algorithms, such as epsilon-greedy or entropy-based methods.

**Limitations/Discussion**
The main limitations of this paper are: 
- The authors do not provide any discussion on the potential future work. This could be because the proposed method is very new and there is no need to discuss it.
- The authors only present some experiments based on the Atari 2600 games benchmark, which shows that the proposed method outperforms existing methods. They do not provide any comparison with other DRL algorithms.

**References**
The references of this paper are: 
1. Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin Ried-miller. Maximum a posteriori policy optimisation.arXiv preprint arXiv:1806.06920, 2018.
2. Ryan Paul Badman, Thomas Trenholm Hills, and Rei Akaishi. Multiscale computation and dynamic attention in biological and arti- fiicial intelligence. Brain Sciences, 10(6):396, 2020.
3. Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor Mordatch. Emergent tool use from multi-agent autocurricula.arXiv preprint arXiv:1909.07528, 2019.
4. Andrea Banino, Caswell Barry, Benigno Uria, Charles Blundell, Timothy Lillicrap, Piotr Mirowski, Alexan- der Pritzel, Martin J Chadwick, Thomas Degris, Joseph Modayil, et al. Vector-based navigation using grid-like representations in arti- fiicial agents. Nature, 557(7705):429–433, 2018.
5. Subekshya Bidari, Ahmed El Hady, Jacob D Davidson, and Zachary F. Kilpatrick. Stochastic dynamics of social patch foraging decisions. Physical Review Research, 4(3):033128, 2022.
6. Tommy C Blanchard and Benjamin Y Hayden. Neurons in dorsal anterior cingulate cortex signal postdeci- sional variables in a foraging task. Journal of Neuroscience, 34(2):646–655, 2014.
7. Eric L Charnov. Optimal foraging, the marginal value theorem. Theoretical Population Biology, 9(2):129–136, 1976.
8. Eric L Charnov and Gordon H Orians. Optimal foraging: Some theoretical explorations. 2006.
9. Paul Cisek. Resynthesizing behavior through phylogenetic re- ﬁnement. Attention, Perception, & Psychophysics, 81(7):2265–2287, 2019.
10. Scott L Coleman, Vincent R Brown, Daniel S Levine, and Roger L Mellgren. A neural network model of foraging decisions made under predation risk. Cognitive, Affective, & Behavioral Neuroscience, 5(4):434–451, 2005.
11. Sara M Constantino and Nathaniel D Daw. Learning the opportunity cost of time in a patch- foraging task. Cognitive, Affective, & Behavioral Neuroscience, 15(4):837–853, 2015.
12. Richard J Cowie. Optimal foraging in great tits (Parus major). Nature, 268(5616):137–139, 1977.
13. Cultural General Intelligence Team, Avishkar Bhoopchand, Bethanie Brown- ﬁeld, Adrian Collister, Agustin Dal Lago, Ashley Edwards, Richard Everett, Alexandre Frechette, Yanko Gitahy Oliveira, Ed- ward Hughes, Kory W. Mathewson, Piermaria Mendolicchio, Julia Pawar, Alex Platonov, Miruna Pislar, and Lei M. Zhang. Learning robust real-time cultural transmission without human data. arXiv preprint arxiv.2203.00715, 2022.
14. Jacob D Davidson and Ahmed El Hady. Foraging as an evidence accumulation process. PLoS Computational Biology, 15(7):e1007060, 2019.

**Note**
The paper has no reference to the potential future work. The authors only present some experiments based on the Atari 2600 games benchmark, which shows that the proposed method outperforms existing methods. They do not provide any comparison with other DRL algorithms.

---

**Summary Statistics:**
- Input: 10,554 words (69,918 chars)
- Output: 874 words
- Compression: 0.08x
- Generation: 192.9s (4.5 words/sec)
- Quality Score: 1.00/1.0
- Attempts: 1

**Quality Check:** Passed
