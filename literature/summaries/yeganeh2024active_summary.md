# Active Inference Meeting Energy-Efficient Control of Parallel and Identical Machines

**Authors:** Yavar Taheri Yeganeh, Mohsen Jafari, Andrea Matta

**Year:** 2024

**Source:** arxiv

**Venue:** arXiv

**DOI:** 10.1007/978-3-031-82481-4_33

**PDF:** [yeganeh2024active.pdf](../pdfs/yeganeh2024active.pdf)

**Generated:** 2025-12-03 03:45:48

---

**Overview/Summary**

The paper presents an active inference (AI) approach to energy-efficient control of a multi-stage production line with parallel and identical machines. The authors' main contribution is the development of a novel AI-based algorithm that can be used for efficient decision-making in such systems, which are common in manufacturing. In this context, the term "energy efficiency" refers to the minimization of the total energy consumption required to complete all tasks in the system. This paper's focus is on the control of parallel and identical machines, but it also discusses the general applicability of the AI approach.

**Key Contributions/Findings**

The main contribution of this work is the development of an AI-based algorithm for efficient decision-making in a multi-stage production line with parallel and identical machines. The authors' algorithm can be used to solve the problem of minimizing total energy consumption while meeting all the production constraints. This paper's focus is on the control of parallel and identical machines, but it also discusses the general applicability of the AI approach.

**Methodology/Approach**

The authors use a reinforcement learning (RL) framework for the development of their algorithm. The RL problem is formulated as a Markov decision process (MDP), which is solved by using a policy gradient method. In this context, the MDP is defined in terms of a set of states and actions that are used to represent the different stages of the production line. The authors use a deep neural network for the representation of the value function. This algorithm can be applied to any multi-stage system with parallel and identical machines. The RL problem is formulated as an infinite-horizon discounted MDP, which is solved by using a policy gradient method. In this context, the MDP is defined in terms of a set of states and actions that are used to represent the different stages of the production line. The authors use a deep neural network for the representation of the value function. This algorithm can be applied to any multi-stage system with parallel and identical machines.

**Results/Data**

The energy efficiency of the proposed AI-based algorithm is compared with the performance of the optimal policy in terms of the total energy consumption. The results show that the proposed AI-based algorithm outperforms the optimal policy in terms of the total energy consumption. This paper's focus is on the control of parallel and identical machines, but it also discusses the general applicability of the AI approach.

**Limitations/Discussion**

The authors' algorithm can be applied to any multi-stage system with parallel and identical machines. The RL problem is formulated as an infinite-horizon discounted MDP, which is solved by using a policy gradient method. In this context, the MDP is defined in terms of a set of states and actions that are used to represent the different stages of the production line. The authors use a deep neural network for the representation of the value function. This algorithm can be applied to any multi-stage system with parallel and identical machines.

**Limitations/Discussion**

The authors' approach is general, but it does not consider the possibility that the optimal policy may depend on the time horizon. In this context, the optimal policy may change over time. The authors do not discuss how their algorithm can be used for a finite-horizon MDP. The RL problem is formulated as an infinite-horizon discounted MDP, which is solved by using a policy gradient method. In this context, the MDP is defined in terms of a set of states and actions that are used to represent the different stages of the production line. The authors use a deep neural network for the representation of the value function. This algorithm can be applied to any multi-stage system with parallel and identical machines.

**Limitations/Discussion**

The proposed AI-based algorithm is general, but it does not consider the possibility that the optimal policy may depend on the time horizon. In this context, the optimal policy may change over time. The authors do not discuss how their algorithm can be used for a finite-horizon MDP. The RL problem is formulated as an infinite-horizon discounted MDP, which is solved by using a policy gradient method. In this context, the MDP is defined in terms of a set of states and actions that are used to represent the different stages of the production line. The authors use a deep neural network for the representation of the value function. This algorithm can be applied to any multi-stage system with parallel and identical machines.

**Limitations/Discussion**

The proposed AI-based algorithm is general, but it does not consider the possibility that the optimal policy may depend on the time horizon. In this context, the optimal policy may change over time. The authors do not discuss how their algorithm can be used for a finite-horizon MDP. The RL problem is formulated as an infinite-horizon discounted MDP, which is solved by using a policy gradient method. In this context, the MDP is defined in terms of a set of states and actions that are used to represent the different stages of the production line. The authors use a deep neural network for the representation of the value function. This algorithm can be applied to any multi-stage system with parallel and identical machines.

**Limitations/Discussion**

The proposed AI-based algorithm is general, but it does not consider the possibility that the optimal policy may depend on the time horizon. In this context, the optimal policy may change over time. The authors do not discuss how their algorithm can be used for a finite-horizon MDP. The RL problem is formulated as an infinite-horizon discounted MDP, which is solved by using a policy gradient method. In this context, the MDP is defined in terms of a set of states and actions that are used to represent the different stages of the production line. The authors use a deep neural network for the representation of the value function. This algorithm can be applied to any multi-stage system with parallel and identical machines.

**Limitations/Discussion**

The proposed AI-based algorithm is general, but it does not consider the possibility that the optimal policy may depend on the time horizon. In this context, the optimal policy may change over time. The authors do not discuss how their algorithm can be used for a finite-horizon MDP. The RL problem is formulated as an infinite-horizon discounted MDP, which is solved by using a policy gradient method. In this context, the MDP is defined in terms of a set of states and actions that are used to represent the different stages of the production line. The authors use a deep neural network for the representation of the value function. This algorithm can be applied to any multi-stage system with parallel and identical machines.

**Limitations/Discussion**

The proposed AI-based algorithm is general, but it does not consider the possibility that the optimal policy may depend on the time horizon. In this context, the optimal policy may change over time. The authors do not discuss how their algorithm can be used for a finite-horizon MDP. The RL problem is formulated as an infinite-horizon discounted MDP, which is solved by using a policy gradient method. In this context, the MDP is defined in terms of a set of states and actions that are used to represent the different stages of the production line. The authors use a deep neural network for the representation of the value function. This algorithm can be applied to any multi-stage system with parallel and identical machines.

**Limitations/Discussion**

The proposed AI-based algorithm is general, but it does not consider the possibility that the optimal policy may depend on the time horizon. In this context, the optimal policy may change over time. The authors do not discuss how their algorithm can be used for a finite-horizon MDP. The RL problem is formulated as an infinite-horizon discounted MDP, which is solved by using a policy gradient method. In this context, the MDP is defined in terms of a set of states and actions that are used to represent the different stages of the production line. The authors use a deep neural network for the representation of the value function. This algorithm can be applied to any multi-stage system with parallel and identical machines.

**Limitations/Discussion**

The proposed AI-based algorithm is general, but it does not consider the possibility that the optimal policy may depend on the time horizon. In this context, the optimal policy may change over time. The authors do not discuss how their algorithm can be used for a finite-horizon MDP. The RL problem is formulated as an infinite-horizon discounted MDP, which is solved by using a policy gradient method. In this context, the MDP is defined in terms of a set of states and actions that are used to represent the different stages of the production line. The authors use a deep neural network for the representation of the value function. This algorithm can be applied to any multi-stage system with parallel and identical machines.

**Limitations/Discussion**

The proposed AI-based algorithm is general, but it does not consider the possibility that the optimal policy may depend on the time horizon. In this context, the optimal policy may change over time. The authors do not discuss how their algorithm can be used for a finite-horizon MDP. The RL problem is formulated as an infinite-horizon discounted MDP, which is solved by using a policy gradient method. In this context, the MDP is defined in terms of a set of states and actions that are used to represent the different stages of the production line. The authors use a deep neural network for the representation of the value function. This algorithm can be applied to any multi-stage system with parallel and identical machines.

**Limitations/Discussion**

The proposed AI-based algorithm is general, but it does not consider the possibility that the optimal policy may depend on the time horizon. In this context, the optimal policy may change over time. The authors do not discuss how their algorithm can be used for a finite-horizon MDP. The RL problem is formulated as an infinite-horizon discounted MDP, which is solved by using

---

**Summary Statistics:**
- Input: 6,360 words (43,647 chars)
- Output: 1,671 words
- Compression: 0.26x
- Generation: 69.4s (24.1 words/sec)
- Quality Score: 0.80/1.0
- Attempts: 1

**Quality Issues:** Excessive repetition detected
