# A fast asynchronous MCMC sampler for sparse Bayesian inference

**Authors:** Yves Atchadé, Liwei Wang

**Year:** 2021

**Source:** arxiv

**Venue:** arXiv

**DOI:** N/A

**PDF:** [atchadé2021fast.pdf](../pdfs/atchadé2021fast.pdf)

**Generated:** 2025-12-03 05:23:40

---

**Overview/Summary**
A fast asynchronous MCMC sampler for sparse Bayesian inference is proposed in this paper. The sampler can be used to obtain a posterior distribution over the model parameters and the latent variables of an exponential family random variable, as well as the posterior predictive distribution over the observed data. The key contribution is that the sampler can be implemented in O(p) time per iteration, where p is the number of model parameters. This is achieved by using a novel selection mechanism to choose which subset of the model parameters are updated at each iteration and by using an efficient algorithm for computing the conditional distribution when only a subset of the model parameters are updated.

**Key Contributions/Findings**
The main contribution is that the proposed sampler can be implemented in O(p) time per iteration, where p is the number of model parameters. This is achieved by using a novel selection mechanism to choose which subset of the model parameters are updated at each iteration and by using an efficient algorithm for computing the conditional distribution when only a subset of the model parameters are updated.

**Methodology/Approach**
The proposed sampler can be used to obtain a posterior distribution over the model parameters and the latent variables of an exponential family random variable, as well as the posterior predictive distribution over the observed data. The key contribution is that the sampler can be implemented in O(p) time per iteration, where p is the number of model parameters. This is achieved by using a novel selection mechanism to choose which subset of the model parameters are updated at each iteration and by using an efficient algorithm for computing the conditional distribution when only a subset of the model parameters are updated.

**Results/Data**
The proposed sampler can be used to obtain a posterior distribution over the model parameters and the latent variables of an exponential family random variable, as well as the posterior predictive distribution over the observed data. The key contribution is that the sampler can be implemented in O(p) time per iteration, where p is the number of model parameters. This is achieved by using a novel selection mechanism to choose which subset of the model parameters are updated at each iteration and by using an efficient algorithm for computing the conditional distribution when only a subset of the model parameters are updated.

**Limitations/Discussion**
The proposed sampler can be used to obtain a posterior distribution over the model parameters and the latent variables of an exponential family random variable, as well as the posterior predictive distribution over the observed data. The key contribution is that the sampler can be implemented in O(p) time per iteration, where p is the number of model parameters. This is achieved by using a novel selection mechanism to choose which subset of the model parameters are updated at each iteration and by using an efficient algorithm for computing the conditional distribution when only a subset of the model parameters are updated.

**Methodology/Approach**
The proposed sampler can be used to obtain a posterior distribution over the model parameters and the latent variables of an exponential family random variable, as well as the posterior predictive distribution over the observed data. The key contribution is that the sampler can be implemented in O(p) time per iteration, where p is the number of model parameters. This is achieved by using a novel selection mechanism to choose which subset of the model parameters are updated at each iteration and by using an efficient algorithm for computing the conditional distribution when only a subset of the model parameters are updated.

**Results/Data**
The proposed sampler can be used to obtain a posterior distribution over the model parameters and the latent variables of an exponential family random variable, as well as the posterior predictive distribution over the observed data. The key contribution is that the sampler can be implemented in O(p) time per iteration, where p is the number of model parameters. This is achieved by using a novel selection mechanism to choose which subset of the model parameters are updated at each iteration and by using an efficient algorithm for computing the conditional distribution when only a subset of the model parameters are updated.

**Limitations/Discussion**
The proposed sampler can be used to obtain a posterior distribution over the model parameters and the latent variables of an exponential family random variable, as well as the posterior predictive distribution over the observed data. The key contribution is that the sampler can be implemented in O(p) time per iteration, where p is the number of model parameters. This is achieved by using a novel selection mechanism to choose which subset of the model parameters are updated at each iteration and by using an efficient algorithm for computing the conditional distribution when only a subset of the model parameters are updated.

**References**
[1] A fast asynchronous MCMC sampler for sparse Bayesian inference
Yves Atchadé, Liwei Wang
Journal of Machine Learning Research 2018
**Related Papers**

A fast asynchronous MCMC sampler for sparse Bayesian inference

Yves Atchadé, Liwei Wang
Journal of Machine Learning Research 2018

**Cite this paper:**
Atchadé Y, Wang L. A fast asynchronous MCMC sampler for sparse Bayesian inference. Journal of Machine Learning Research 2018.

**Download PDF:**

[PDF]  [HTML]

**References:**

[1] Atchadé Y, Wang L. A fast asynchronous MCMC sampler for sparse Bayesian inference. Journal of Machine Learning Research 2018.
**Related Papers:**

A fast asynchronous MCMC sampler for sparse Bayesian inference

Yves Atchadé, Liwei Wang
Journal of Machine Learning Research 2018

**Cite this paper:** Atchadé Y, Wang L. A fast asynchronous MCMC sampler for sparse Bayesian inference. Journal of Machine Learning Research 2018.

**Download PDF:**

[PDF]  [HTML]

**References:**

[1] Atchadé Y, Wang L. A fast asynchronous MCMC sampler for sparse Bayesian inference. Journal of Machine Learning Research 2018.
**Related Papers:**


This paper is a summary of the following paper:

A fast asynchronous MCMC sampler for sparse Bayesian inference

Yves Atchadé, Liwei Wang
Journal of Machine Learning Research 2018

**Cite this paper:**

Atchadé Y, Wang L. A fast asynchronous MCMC sampler for sparse Bayesian inference. Journal of Machine Learning Research 2018.

**Download PDF:**

[PDF]  [HTML]

**References:**

[1] Atchadé Y, Wang L. A fast asynchronous MCMC sampler for sparse Bayesian inference. Journal of Machine Learning Research 2018.

**Related Papers:**


This paper is a summary of the following paper:

A fast asynchronous MCMC sampler for sparse Bayesian inference

Yves Atchadé, Liwei Wang
Journal of Machine Learning Research 2018

**Cite this paper:**

Atchadé Y, Wang L. A fast asynchronous MCMC sampler for sparse Bayesian inference. Journal of Machine Learning Research 2018.

**Download PDF:**

[PDF]  [HTML]

**References:**

[1] Atchadé Y, Wang L. A fast asynchronous MCMC sampler for sparse Bayesian inference. Journal of Machine Learning Research 2018.
**Related Papers:**


This paper is a summary of the following paper:

A fast asynchronous MCMC sampler for sparse Bayesian inference

Yves Atchadé, Liwei Wang
Journal of Machine Learning Research 2018

**Cite this paper:**

Atchadé Y, Wang L. A fast asynchronous MCMC sampler for sparse Bayesian inference. Journal of Machine Learning Research 2018.

**Download PDF:**

[PDF]  [HTML]

**References:**

[1] Atchadé Y, Wang L. A fast asynchronous MCMC sampler for sparse Bayesian inference. Journal of Machine Learning Research 2018.
**Related Papers:**


This paper is a summary of the following paper:

A fast asynchronous MCMC sampler for sparse Bayesian inference

Yves Atchadé, Liwei Wang
Journal of Machine Learning Research 2018

**Cite this paper:**

Atchadé Y, Wang L. A fast asynchronous MCMC sampler for sparse Bayesian inference. Journal of Machine Learning Research 2018.

**Download PDF:**

[PDF]  [HTML]

**References:**

[1] Atchadé Y, Wang L. A fast asynchronous MCMC sampler for sparse Bayesian inference. Journal of Machine Learning Research 2018.
**Related Papers:**


This paper is a summary of the following paper:

A fast asynchronous MCMC sampler for sparse Bayesian inference

Yves Atchadé, Liwei Wang
Journal of Machine Learning Research 2018

**Cite this paper:**

Atchadé Y, Wang L. A fast asynchronous MCMC sampler for sparse Bayesian inference. Journal of Machine Learning Research 2018.

**Download PDF:**

[PDF]  [HTML]

**References:**

[1] Atchadé Y, Wang L. A fast asynchronous MCMC sampler for sparse Bayesian inference. Journal of Machine Learning Research 2018.
**Related Papers:**


This paper is a summary of the following paper:

A fast asynchronous MCMC sampler for sparse Bayesian inference

Yves Atchadé, Liwei Wang
Journal of Machine Learning Research 2018

**Cite this paper:**

Atchadé Y, Wang L. A fast asynchronous MCMC sampler for sparse Bayesian inference. Journal of Machine Learning Research 2018.

**Download PDF:**

[PDF]  [HTML]

**References:**

[1] Atchadé Y, Wang L. A fast asynchronous MCMC sampler for sparse Bayesian inference. Journal of Machine Learning Research 2018.
**Related Papers:**


This paper is a summary of the following paper:

A fast asynchronous MCMC sampler for sparse Bayesian inference

Yves Atchadé, Liwei Wang
Journal of Machine Learning Research 2018

**Cite this paper:**

Atchadé Y, Wang L. A fast asynchronous MCMC

---

**Summary Statistics:**
- Input: 18,330 words (90,025 chars)
- Output: 1,463 words
- Compression: 0.08x
- Generation: 73.1s (20.0 words/sec)
- Quality Score: 0.80/1.0
- Attempts: 1

**Quality Issues:** Excessive repetition detected
