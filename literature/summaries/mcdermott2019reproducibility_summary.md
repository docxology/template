# Reproducibility in Machine Learning for Health

**Authors:** Matthew B. A. McDermott, Shirly Wang, Nikki Marinsek, Rajesh Ranganath, Marzyeh Ghassemi, Luca Foschini

**Year:** 2019

**Source:** arxiv

**Venue:** N/A

**DOI:** N/A

**PDF:** [mcdermott2019reproducibility.pdf](../pdfs/mcdermott2019reproducibility.pdf)

**Generated:** 2025-12-05 10:27:27

---

**Overview/Summary**
Reproducibility in Machine Learning for Health is a comprehensive review of 150 papers from various machine learning and health venues that were published in 2018. The authors focus on the reproducibility of results, which they define as the ability to reproduce the results with minimal human intervention and without having access to the original data or code. They also consider whether the datasets used are publicly available. The authors find that about half of the papers do not release any code, while about 40% of them use publically available datasets. Only a quarter of the papers report any kind of statistical analysis to assess their results or comparisons to baselines in a statistically robust fashion.

**Key Contributions/Findings**
The authors' main finding is that only 25% of the papers release any code, and about 40% of them use publically available datasets. The remaining 35% do not report any kind of statistical analysis to assess their results or comparisons to baselines in a statistically robust fashion. They also found that the reproducibility of results from machine learning for health is lower than that reported in other areas, such as computer vision.

**Methodology/Approach**
The authors' methodology was based on a manual literature review of 150 papers from various machine learning and health venues published in 2018. The authors selected these papers at random to ensure an unbiased sample. They used the same selection criteria for all the papers they analyzed, which were: (1) Was code released?; (2) What datasets were used?; (3) Are these datasets publicly available (modulo data use agreements)?; and (4) Do the authors report any notion of variance around their results or assess their comparisons to baselines in a statistically robust fashion?

**Results/Data**
The authors' main findings are that about half of the papers do not release code, while about 40% of them use publically available datasets. Only a quarter of the papers report any kind of statistical analysis to assess their results or comparisons to baselines in a statistically robust fashion.

**Limitations/Discussion**
The authors' limitations and discussion are that only 25% of the papers release code, while about 40% of them use publically available datasets. Only a quarter of the papers report any kind of statistical analysis to assess their results or comparisons to baselines in a statistically robust fashion. The authors also found that the reproducibility of results from machine learning for health is lower than that reported in other areas, such as computer vision.

**References**
The references are not provided here. Please refer to the original paper above.

---

**Summary Statistics:**
- Input: 5,208 words (35,797 chars)
- Output: 422 words
- Compression: 0.08x
- Generation: 26.2s (16.1 words/sec)
- Quality Score: 0.60/1.0
- Attempts: 1

**Quality Issues:** Hallucination detected: Physics paper summary lacks physics terminology
