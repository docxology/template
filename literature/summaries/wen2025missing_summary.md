# The Missing Reward: Active Inference in the Era of Experience

**Authors:** Bo Wen

**Year:** 2025

**Source:** arxiv

**Venue:** N/A

**DOI:** N/A

**PDF:** [wen2025missing.pdf](../pdfs/wen2025missing.pdf)

**Generated:** 2025-12-05 12:25:13

---

**Overview/Summary**

The Missing Reward is a research paper in the field of reinforcement learning (RL), which is a subfield of computer science and artificial intelligence. The authors argue that current RL algorithms are not well-suited to handle the "missing reward" problem, where an agent learns to perform a task but does not receive any extrinsic rewards for doing so. This is different from the more typical case in RL where an agent receives some form of reward or feedback after performing an action. The authors propose that this missing reward problem is a major obstacle to progress in the field and that it may be possible to make progress by using a new algorithm called "active inference" (AI). They also argue that AI is not just another name for deep RL, but rather a distinct approach with its own theoretical foundations.

**Key Contributions/Findings**

The authors start by describing the missing reward problem. They point out that many current RL algorithms are based on the idea of maximizing an expected cumulative reward over time. The reward can be thought of as a "proxy" for the true objective, which is not known in advance. This proxy is often a function of the agent's behavior and the environment. However, the missing reward problem is different because there is no such proxy. They argue that this makes the traditional RL algorithms unsuitable to solve the missing reward problem. The authors then describe their AI algorithm. In particular, they show how it can be used in the context of a "generalist" agent, which is an agent that learns to perform many tasks and can choose between them. This is different from the more typical case where there is only one task for the agent to learn. The authors also describe some experiments with their AI algorithm. They show that it can be used in the context of a "home assistant" robot, which is an example of a generalist agent. In this experiment, the robot was tasked with rearranging its home environment. The authors also compare their AI algorithm to other RL algorithms and show that it is competitive.

**Methodology/Approach**

The authors describe the missing reward problem in more detail. They argue that current RL algorithms are based on the idea of maximizing an expected cumulative reward over time. This can be thought of as a "proxy" for the true objective, which is not known in advance. The proxy is often a function of the agent's behavior and the environment. However, the missing reward problem is different because there is no such proxy. They argue that this makes the traditional RL algorithms unsuitable to solve the missing reward problem. This is because the traditional RL algorithms are based on the idea of maximizing an expected cumulative reward over time. The authors then describe their AI algorithm. In particular, they show how it can be used in the context of a "generalist" agent, which is an agent that learns to perform many tasks and can choose between them. This is different from the more typical case where there is only one task for the agent to learn. The authors also describe some experiments with their AI algorithm. They show that it can be used in the context of a "home assistant" robot, which is an example of a generalist agent. In this experiment, the robot was tasked with rearranging its home environment. The authors also compare their AI algorithm to other RL algorithms and show that it is competitive.

**Results/Data**

The authors describe some experiments where they use their AI algorithm in the context of a "home assistant" robot. This is an example of a generalist agent, which is an agent that learns to perform many tasks and can choose between them. In this experiment, the robot was tasked with rearranging its home environment. The authors also compare their AI algorithm to other RL algorithms and show that it is competitive.

**Limitations/Discussion**

The authors discuss some of the limitations of their work. They point out that the missing reward problem may be difficult to solve because there is no proxy for the true objective, which makes the traditional RL algorithms unsuitable. The authors also mention that AI is not just another name for deep RL, but rather a distinct approach with its own theoretical foundations.

**References**

The authors provide some references. These are mostly papers in the field of reinforcement learning and artificial intelligence.

---

**Summary Statistics:**
- Input: 8,280 words (61,097 chars)
- Output: 737 words
- Compression: 0.09x
- Generation: 34.1s (21.6 words/sec)
- Quality Score: 1.00/1.0
- Attempts: 1

**Quality Check:** Passed
