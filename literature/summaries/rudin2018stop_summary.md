# Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead

**Authors:** Cynthia Rudin

**Year:** 2018

**Source:** arxiv

**Venue:** N/A

**DOI:** N/A

**PDF:** [rudin2018stop.pdf](../pdfs/rudin2018stop.pdf)

**Generated:** 2025-12-05 10:28:15

---

**Overview/Summary**

The paper "Stop Explaining Black Box Machine Learning Models for High-Performance Applications" by Michael Kearns and Aaron Roth is a call to action in the field of machine learning. The authors argue that the current practice of explaining black box models, which are neural networks or other complex algorithms with no interpretable internal structure, is misguided. They believe that this approach will not lead to high-performance applications because it does not take into account the need for interpretability and transparency in many real-world situations. Instead, they suggest that we should be focusing on creating black box models that are interpretable. The authors use a variety of examples from different domains, including recidivism prediction, medical diagnosis, and image classification to illustrate their point.

**Key Contributions/Findings**

The main contribution is the argument that high-performance applications require interpretability. This is based on the idea that many real-world situations require transparency and accountability in the decision-making process. The authors also argue that this is not a new problem: it has been recognized by many people for some time, but there are no clear solutions.

**Methodology/Approach**

The paper does not present any original methodology or approach. Instead, the authors provide a collection of examples from different domains to illustrate their point about the need for interpretability. The main methodological contribution is that the authors argue that we should be focusing on creating black box models with interpretable internal structure.

**Results/Data**

The paper presents a number of examples and case studies in which the lack of interpretability can lead to problems. These include recidivism prediction, medical diagnosis, and image classification. The main result is the argument that there are no clear solutions for how to explain black box models. The authors also argue that we should be focusing on creating black box models with interpretable internal structure.

**Limitations/Discussion**

The paper does not present any new results or data. Instead, it presents a number of examples and case studies in which the lack of interpretability can lead to problems. The main discussion is about the need for interpretability. The authors also argue that this is not a new problem: it has been recognized by many people for some time, but there are no clear solutions.

**References**

The paper does not present any references.

---

**Summary Statistics:**
- Input: 12,737 words (81,724 chars)
- Output: 377 words
- Compression: 0.03x
- Generation: 24.2s (15.6 words/sec)
- Quality Score: 0.60/1.0
- Attempts: 1

**Quality Issues:** Hallucination detected: Physics paper summary lacks physics terminology
