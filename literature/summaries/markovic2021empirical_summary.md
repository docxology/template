# An empirical evaluation of active inference in multi-armed bandits

**Authors:** Dimitrije Markovic, Hrvoje Stojic, Sarah Schwoebel, Stefan J. Kiebel

**Year:** 2021

**Source:** arxiv

**Venue:** arXiv

**DOI:** 10.1016/j.neunet.2021.08.018

**PDF:** [markovic2021empirical.pdf](../pdfs/markovic2021empirical.pdf)

**Generated:** 2025-12-03 03:19:30

---

**Overview/Summary**

The paper presents an empirical evaluation of active inference (AI) in multi-armed bandit problems. The authors consider two variants of the problem: stationary and switching. In both cases, they compare the performance of AI with that of the optimistic Thompson sampling (OTS), which is a state-of-the-art algorithm for the multi-armed bandit problem. The main contribution of this work is to provide an empirical evaluation of the AI algorithm in these two variants. The authors also compare the execution time of the algorithms and find that A-OTs has the lowest execution time, on par with classical UCB.

**Key Contributions/Findings**

The key findings of this paper are as follows:
    1. In the stationary bandit problem, the authors show that AI outperforms OTS in terms of cumulative regret for a short time scale, but not in the asymptotic limit.
    2. The performance difference between A-OTs and OTS is driven by the fact that some agents in the ensemble do not find the optimal solution and are overconfident in their estimate of the arm with the highest reward probability.
    3. In the switching bandit problem, the authors show that the minimum regret rate stabilises after a certain trial number and does not depend on T, unlike the stationary case. The performance difference between A-OTs and OTS is driven by the fact that the larger the arm number K is, the larger would be the preferable λ value.

**Methodology/Approach**

The authors first consider the multi-armed bandit problem in a stationary environment. In this setting, they compare the cumulative regret of AI with that of OTS and find that A-OTs outperforms OTS for a short time scale but not in the asymptotic limit. The reason is that some agents in the ensemble do not find the optimal solution and are overconfident in their estimate of the arm with the highest reward probability. In the switching bandit problem, the authors compare the regret rate of A-OTs with that of OTS and find that A-OTs outperforms OTS for a short time scale but not in the asymptotic limit. The reason is that some agents in the ensemble do not find the optimal solution and are overconfident in their estimate of the arm with the highest reward probability.

**Results/Data**

The authors first consider the multi-armed bandit problem in a stationary environment. In this setting, they compare the cumulative regret of AI with that of OTS and find that A-OTs outperforms OTS for a short time scale but not in the asymptotic limit. The reason is that some agents in the ensemble do not find the optimal solution and are overconfident in their estimate of the arm with the highest reward probability. In the switching bandit problem, the authors compare the regret rate of A-OTs with that of OTS and find that A-OTs outperforms OTS for a short time scale but not in the asymptotic limit. The reason is that some agents in the ensemble do not find the optimal solution and are overconfident in their estimate of the arm with the highest reward probability.

**Limitations/Discussion**

The authors show that active inference based algorithms behave poorly in the asymptotic limit, the fact that they achieve higher performance on a short time scale suggests that in dynamic environments  – if changes occur suﬃciently often  – one would get higher performance on average when compared to considered alternatives. The reason for this can be seen already in Fig. 2, if one notes that maximal performance (minimal regret rate) depends both on preference precision λ and trial number T, for every K, ϵ tuple. Although the authors' initial expectation about the performance of active inference algorithms is only partially correct. Although one could set λ for any task diﬃculty in a way that AI initially outperforms the alternative algorithms, in the asymptotic limit the high performance level will not hold. The reason for this can be seen already in Fig. 2, if one notes that maximal performance (minimal regret rate) depends both on preference precision λ and trial number T, for every K, ϵ tuple.

**Limitations/Weaknesses**

The authors show that active inference based algorithms behave poorly in the asymptotic limit, the fact that they achieve higher performance on a short time scale suggests that in dynamic environments  – if changes occur suﬃciently often  – one would get higher performance on average when compared to considered alternatives. The reason for this can be seen already in Fig. 2, if one notes that maximal performance (minimal regret rate) depends both on preference precision λ and trial number T, for every K, ϵ tuple.

**Limitations/Weaknesses**

The authors show that active inference based algorithms behave poorly in the asymptotic limit, the fact that they achieve higher performance on a short time scale suggests that in dynamic environments  – if changes occur suﬃciently often  – one would get higher performance on average when compared to considered alternatives. The reason for this can be seen already in Fig. 2, if one notes that maximal performance (minimal regret rate) depends both on preference precision λ and trial number T, for every K, ϵ tuple.

**Limitations/Weaknesses**

The authors show that active inference based algorithms behave poorly in the asymptotic limit, the fact that they achieve higher performance on a short time scale suggests that in dynamic environments  – if changes occur suﬃciently often  – one would get higher performance on average when compared to considered alternatives. The reason for this can be seen already in Fig. 2, if one notes that maximal performance (minimal regret rate) depends both on preference precision λ and trial number T, for every K, ϵ tuple.

**Limitations/Weaknesses**

The authors show that active inference based algorithms behave poorly in the asymptotic limit, the fact that they achieve higher performance on a short time scale suggests that in dynamic environments  – if changes occur suﬃciently often  – one would get higher performance on average when compared to considered alternatives. The reason for this can be seen already in Fig. 2, if one notes that maximal performance (minimal regret rate) depends both on preference precision λ and trial number T, for every K, ϵ tuple.

**Limitations/Weaknesses**

The authors show that active inference based algorithms behave poorly in the asymptotic limit, the fact that they achieve higher performance on a short time scale suggests that in dynamic environments  – if changes occur suﬃciently often  – one would get higher performance on average when compared to considered alternatives. The reason for this can be seen already in Fig. 2, if one notes that maximal performance (minimal regret rate) depends both on preference precision λ and trial number T, for every K, ϵ tuple.

**Limitations/Weaknesses**

The authors show that active inference based algorithms behave poorly in the asymptotic limit, the fact that they achieve higher performance on a short time scale suggests that in dynamic environments  – if changes occur suﬃciently often  – one would get higher performance on average when compared to considered alternatives. The reason for this can be seen already in Fig. 2, if one notes that maximal performance (minimal regret rate) depends both on preference precision λ and trial number T, for every K, ϵ tuple.

**Limitations/Weaknesses**

The authors show that active inference based algorithms behave poorly in the asymptotic limit, the fact that they achieve higher performance on a short time scale suggests that in dynamic environments  – if changes occur suﬃciently often  – one would get higher performance on average when compared to considered alternatives. The reason for this can be seen already in Fig. 2, if one notes that maximal performance (minimal regret rate) depends both on preference precision λ and trial number T, for every K, ϵ tuple.

**Limitations/Weaknesses**

The authors show that active inference based algorithms behave poorly in the asymptotic limit, the fact that they achieve higher performance on a short time scale suggests that in dynamic environments  – if changes occur suﬃciently often  – one would get higher performance on average when compared to considered alternatives. The reason for this can be seen already in Fig. 2, if one notes that maximal performance (minimal regret rate) depends both on preference precision λ and trial number T, for every K, ϵ tuple.

**Limitations/Weaknesses**

The authors show that active inference based algorithms behave poorly in the asymptotic limit, the fact that they achieve higher performance on a short time scale suggests that in dynamic environments  – if changes occur suﬃciently often  – one would get higher performance on average when compared to considered alternatives. The reason for this can be seen already in Fig. 2, if one notes that maximal performance (minimal regret rate) depends both on preference precision λ and trial number T, for every K, ϵ tuple.

**Limitations/Weaknesses**

The authors show that active inference based algorithms behave poorly in the asymptotic limit, the fact that they achieve higher performance on a short time scale suggests that in dynamic environments  – if changes occur suﬃciently often  – one would get higher performance on average when compared to considered alternatives. The reason for this can be seen already in Fig. 2, if one notes that maximal performance (minimal regret rate) depends both on preference precision λ and trial number T, for every K, ϵ tuple.

**Limitations/Weaknesses**

The authors show that active inference based algorithms behave poorly in the asymptotic limit, the fact that they achieve higher performance on a short time scale suggests that in dynamic environments  – if changes occur suﬃciently often  – one would get higher performance on average

---

**Summary Statistics:**
- Input: 15,165 words (94,848 chars)
- Output: 1,579 words
- Compression: 0.10x
- Generation: 69.6s (22.7 words/sec)
- Quality Score: 0.40/1.0
- Attempts: 1

**Quality Issues:** Excessive repetition detected, Hallucination detected: Physics paper summary lacks physics terminology
