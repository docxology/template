# Bidirectional Interaction between Visual and Motor Generative Models using Predictive Coding and Active Inference

**Authors:** Louis Annabi, Alexandre Pitti, Mathias Quoy

**Year:** 2021

**Source:** arxiv

**Venue:** N/A

**DOI:** N/A

**PDF:** [annabi2021bidirectional.pdf](../pdfs/annabi2021bidirectional.pdf)

**Generated:** 2025-12-05 12:21:42

---

**Overview/Summary**

The paper presents a novel approach to learning motor trajectories from an indirect supervision in the visual space. The authors propose a bidirectional interaction between two generative models for vision and action that is able to dynamically control the motor trajectory according to the visual prediction. This work is motivated by the fact that the traditional approaches to learn the motor trajectory are based on the direct observation of the motor signal, which can be difficult in some situations. The proposed approach learns the motor trajectories from an indirect supervision in the visual space and the bidirectional interaction between the two generative models gives the ability to dynamically control the motor trajectory according to the visual prediction. This work is motivated by the fact that the traditional approaches to learn the motor trajectory are based on the direct observation of the motor signal, which can be difficult in some situations.

**Key Contributions/Findings**

The authors propose a novel approach to learning the motor trajectories from an indirect supervision in the visual space. The bidirectional interaction between two generative models for vision and action that is able to dynamically control the motor trajectory according to the visual prediction. This work is motivated by the fact that the traditional approaches to learn the motor trajectory are based on the direct observation of the motor signal, which can be difficult in some situations. The authors also show how an architecture can learn motor trajectories from an indirect supervision in the visual space and the bidirectional interactions between the vision prediction RNN and the motor RNN give our model the ability to dynamically control motor trajectories according to the visual predictions. The authors evaluate their model in different experimental setups highlighting interesting features such as intermittent control, robustness to perturbation, and adaptation to scaling and rotation.

**Methodology/Approach**

The proposed approach is based on a bidirectional interaction between two generative models for vision and action that is able to dynamically control the motor trajectory according to the visual prediction. The authors first propose an architecture with two RNNs (Recurrent Neural Networks) for the vision and the motor. The authors then show how this architecture can be used to learn motor trajectories from an indirect supervision in the visual space. The bidirectional interaction between the two RNN models gives the ability to dynamically control the motor trajectory according to the visual prediction. The authors also evaluate their model in different experimental setups highlighting interesting features such as intermittent control, robustness to perturbation, and adaptation to scaling and rotation.

**Results/Data**

The proposed approach is able to learn the motor trajectories from an indirect supervision in the visual space. The bidirectional interaction between the two RNN models gives the ability to dynamically control the motor trajectory according to the visual prediction. The authors also show how their model can be used to evaluate the intermittent control, robustness to perturbation, and adaptation to scaling and rotation.

**Limitations/Discussion**

The proposed approach learns the motor trajectories from an indirect supervision in the visual space. The bidirectional interaction between the two RNN models gives the ability to dynamically control the motor trajectory according to the visual prediction. The authors also show how their model can be used to evaluate the intermittent control, robustness to perturbation, and adaptation to scaling and rotation. The proposed approach is based on a bidirectional interaction between two generative models for vision and action that is able to dynamically control the motor trajectory according to the visual prediction. This work is motivated by the fact that the traditional approaches to learn the motor trajectory are based on the direct observation of the motor signal, which can be difficult in some situations. The authors also show how an architecture can learn the motor trajectories from an indirect supervision in the visual space and the bidirectional interactions between the two RNN models give our model the ability to dynamically control the motor trajectory according to the visual predictions. The proposed approach is based on a bidirectional interaction between two generative models for vision and action that is able to dynamically control the motor trajectory according to the visual prediction. This work is motivated by the fact that the traditional approaches to learn the motor trajectory are based on the direct observation of the motor signal, which can be difficult in some situations.

**References**

[1] [2] [3] [4] [5] [6] [7] [8]

Please let me know if you would like me to add more details or modify anything.

---

**Summary Statistics:**
- Input: 10,243 words (66,166 chars)
- Output: 747 words
- Compression: 0.07x
- Generation: 34.3s (21.8 words/sec)
- Quality Score: 0.60/1.0
- Attempts: 1

**Quality Issues:** Hallucination detected: Physics paper summary lacks physics terminology
