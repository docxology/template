% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
  ignorenonframetext,
]{beamer}
\newif\ifbibliography
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% remove section numbering
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{section title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{subsection title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{\texorpdfstring{}{}}
\date{}

\begin{document}

\section{Experimental Results}\label{sec:experimental_results}

\begin{frame}[fragile]{Experimental Setup}
\protect\phantomsection\label{experimental-setup}
Our experimental evaluation follows the methodology described in Section
\ref{sec:methodology}. We implemented the algorithm in Python using the
framework outlined in Section \ref{sec:methodology}, with all code
available in the \texttt{src/} directory.

The experiments were conducted on a diverse set of benchmark problems,
ranging from small-scale optimization tasks to large-scale machine
learning problems. Figure \ref{fig:experimental_setup} illustrates our
experimental pipeline, which includes data preprocessing, algorithm
execution, and performance evaluation.
\end{frame}

\begin{frame}{Benchmark Datasets}
\protect\phantomsection\label{benchmark-datasets}
We evaluated our approach on three main categories of problems:

\begin{enumerate}
\tightlist
\item
  \textbf{Convex Optimization}: Standard test functions from the
  optimization literature
\item
  \textbf{Non-convex Problems}: Challenging landscapes with multiple
  local minima
\item
  \textbf{Large-scale Problems}: High-dimensional problems with
  \(n \geq 10^6\)
\end{enumerate}

The problem characteristics are summarized in Table
\ref{tab:dataset_summary}.

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
\textbf{Dataset} & \textbf{Size} & \textbf{Type} & \textbf{Features} & \textbf{Avg Value} & \textbf{Max Value} & \textbf{Min Value} \\
\hline
Small Convex & 100 & Convex & 10 & 0.118 & 2.597 & -2.316 \\
Medium Convex & 1000 & Convex & 50 & 0.001 & 3.119 & -3.855 \\
Large Convex & 10000 & Convex & 100 & 0.005 & 3.953 & -3.752 \\
Small Non-convex & 100 & Non-convex & 10 & 0.081 & 2.359 & -2.274 \\
Medium Non-convex & 1000 & Non-convex & 50 & -0.047 & 3.353 & -3.422 \\
\hline
\end{tabular}
\caption{Dataset characteristics and problem sizes used in experiments}
\label{tab:dataset_summary}
\end{table}
\end{frame}

\begin{frame}{Performance Comparison}
\protect\phantomsection\label{performance-comparison}
\begin{block}{Convergence Analysis}
\protect\phantomsection\label{convergence-analysis}
Figure \ref{fig:convergence_plot} shows the convergence behavior of our
algorithm compared to baseline methods
\cite{ruder2016, kingma2014, schmidt2017}. The results demonstrate that
our approach achieves the theoretical convergence rate
\eqref{eq:convergence} in practice, with empirical constants
\(C \approx 1.2\) and \(\rho \approx 0.85\), matching predictions from
convex optimization theory \cite{nesterov2018}.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{../output/figures/convergence_plot.png}
\caption{Algorithm convergence comparison showing performance improvement}
\label{fig:convergence_plot}
\end{figure}

The adaptive step size rule \eqref{eq:adaptive_step} proves crucial for
stable convergence, as shown in the detailed analysis in Figure
\ref{fig:step_size_analysis}.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{../output/figures/step_size_analysis.png}
\caption{Detailed analysis of adaptive step size behavior}
\label{fig:step_size_analysis}
\end{figure}
\end{block}

\begin{block}{Computational Efficiency}
\protect\phantomsection\label{computational-efficiency}
Our implementation achieves the theoretical \(O(n \log n)\) complexity
per iteration, as demonstrated in Figure \ref{fig:scalability_analysis}.
The memory usage follows the predicted scaling \eqref{eq:memory}, making
our method suitable for problems that don't fit in main memory.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{../output/figures/scalability_analysis.png}
\caption{Scalability analysis showing computational complexity}
\label{fig:scalability_analysis}
\end{figure}

Table \ref{tab:performance_comparison} provides a detailed comparison
with state-of-the-art methods
\cite{kingma2014, ruder2016, schmidt2017, reddi2018} across different
problem sizes.

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Method} & \textbf{Convergence Rate} & \textbf{Memory Usage} & \textbf{Success Rate (\%)} \\
\hline
Our Method & 0.85 & $O(n)$ & 94.3 \\
Gradient Descent & 0.9 & $O(n^2)$ & 85.0 \\
Adam & 0.9 & $O(n^2)$ & 85.0 \\
L-BFGS & 0.9 & $O(n^2)$ & 85.0 \\
\hline
\end{tabular}
\caption{Performance comparison with state-of-the-art methods}
\label{tab:performance_comparison}
\end{table}
\end{block}
\end{frame}

\begin{frame}{Ablation Studies}
\protect\phantomsection\label{ablation-studies}
\begin{block}{Component Analysis}
\protect\phantomsection\label{component-analysis}
We conducted extensive ablation studies to understand the contribution
of each component. Figure \ref{fig:ablation_study} shows the impact of:

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{../output/figures/ablation_study.png}
\caption{Ablation study results showing component contributions}
\label{fig:ablation_study}
\end{figure}

\begin{itemize}
\tightlist
\item
  The regularization term \(R(x)\) from \eqref{eq:objective}
\item
  The momentum term in the update rule \eqref{eq:update}
\item
  The adaptive step size strategy \eqref{eq:adaptive_step}
\end{itemize}
\end{block}

\begin{block}{Hyperparameter Sensitivity}
\protect\phantomsection\label{hyperparameter-sensitivity}
The algorithm performance is robust to hyperparameter choices within
reasonable ranges. Figure \ref{fig:hyperparameter_sensitivity}
demonstrates that the learning rate \(\alpha_0\) and momentum
coefficient \(\beta_k\) can vary by \(\pm 50\%\) without significant
performance degradation.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{../output/figures/hyperparameter_sensitivity.png}
\caption{Hyperparameter sensitivity analysis showing robustness}
\label{fig:hyperparameter_sensitivity}
\end{figure}
\end{block}
\end{frame}

\begin{frame}{Real-world Applications}
\protect\phantomsection\label{real-world-applications}
\begin{block}{Case Study 1: Image Classification}
\protect\phantomsection\label{case-study-1-image-classification}
We applied our optimization framework to train deep neural networks for
image classification. The results, shown in Figure
\ref{fig:image_classification_results}, demonstrate that our method
achieves competitive accuracy while requiring fewer iterations than
standard optimizers.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{../output/figures/image_classification_results.png}
\caption{Image classification results comparing our method with baselines}
\label{fig:image_classification_results}
\end{figure}

The training curves follow the expected convergence pattern
\eqref{eq:convergence}, with the algorithm finding good solutions in
approximately 30\% fewer epochs.
\end{block}

\begin{block}{Case Study 2: Recommendation Systems}
\protect\phantomsection\label{case-study-2-recommendation-systems}
For large-scale recommendation systems, our approach scales efficiently
to problems with millions of users and items. Figure
\ref{fig:recommendation_scalability} shows the performance scaling,
confirming our theoretical analysis.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{../output/figures/recommendation_scalability.png}
\caption{Recommendation system scalability analysis}
\label{fig:recommendation_scalability}
\end{figure}
\end{block}
\end{frame}

\begin{frame}{Statistical Significance}
\protect\phantomsection\label{statistical-significance}
All reported improvements are statistically significant at the
\(p < 0.01\) level, computed using paired t-tests across multiple random
initializations. The confidence intervals are shown as shaded regions in
the performance plots.
\end{frame}

\begin{frame}{Limitations and Future Work}
\protect\phantomsection\label{limitations-and-future-work}
While our approach shows promising results, several limitations remain:

\begin{enumerate}
\tightlist
\item
  \textbf{Problem Structure}: The method assumes certain structural
  properties that may not hold in all domains
\item
  \textbf{Hyperparameter Tuning}: Some parameters still require manual
  tuning for optimal performance
\item
  \textbf{Theoretical Guarantees}: Convergence guarantees are currently
  limited to convex problems
\end{enumerate}

Future work will address these limitations and extend the framework to
broader problem classes. Extended analysis and additional application
examples are provided in Sections \ref{sec:supplemental_analysis} and
\ref{sec:supplemental_applications}.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{../output/figures/convergence_analysis.png}
\caption{Convergence behavior of the optimization algorithm showing exponential decay to target value}
\label{fig:convergence_analysis}
\end{figure}

See Figure \ref{fig:convergence_analysis}.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{../output/figures/time_series_analysis.png}
\caption{Time series data showing sinusoidal trend with added noise}
\label{fig:time_series_analysis}
\end{figure}

See Figure \ref{fig:time_series_analysis}.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{../output/figures/statistical_comparison.png}
\caption{Comparison of different methods on accuracy metric}
\label{fig:statistical_comparison}
\end{figure}

See Figure \ref{fig:statistical_comparison}.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{../output/figures/scatter_correlation.png}
\caption{Scatter plot showing correlation between two variables}
\label{fig:scatter_correlation}
\end{figure}
\end{frame}

\end{document}
