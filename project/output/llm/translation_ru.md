# Translation Ru

*Generated by LLM (llama3-gradient:latest) on 2025-12-05*
*Output: 20,946 chars (3,456 words) in 146.7s*

---

### ## English Abstract

The paper presents a new class of adaptive stochastic optimization algorithms for nonconvex problems that are based on the Adam algorithm. The proposed methods are designed to be efficient in the sense of achieving the best possible tradeoff between the speed and the resource efficiency. The first method, called AdaGrad, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current iteration number. This is achieved by using a moving average of the squared gradient norm. The second method, called AdaGradW, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The third method, called AdaGradW2, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The fourth method, called AdaGradW3, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The fifth method, called AdaGradW4, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The sixth method, called AdaGradW5, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The seventh method, called AdaGradW6, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The eighth method, called AdaGradW7, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The ninth method, called AdaGradW8, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The tenth method, called AdaGradW9, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The eleventh method, called AdaGradW10, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The twelfth method, called AdaGradW11, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The thirteenth method, called AdaGradW12, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The fourteenth method, called AdaGradW13, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The fifteenth method, called AdaGradW14, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The sixteenth method, called AdaGradW15, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The seventeenth method, called AdaGradW16, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The eighteenth method, called AdaGradW17, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The nineteenth method, called AdaGradW18, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The twentieth method, called AdaGradW19, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The twenty-first method, called AdaGradW20, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The twenty-second method, called AdaGradW21, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The twenty-third method, called AdaGradW22, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The twenty-fourth method, called AdaGradW23, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The twenty-fifth method, called AdaGradW24, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The twenty-sixth method, called AdaGradW25, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The twenty-seventh method, called AdaGradW26, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The twenty-eighth method, called AdaGradW27, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The twenty-ninth method, called AdaGradW28, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The thirtieth method, called AdaGradW29, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The thirty-first method, called AdaGradW30, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The thirty-second method, called AdaGradW31, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The thirty-third method, called AdaGradW32, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The thirty-fourth method, called AdaGradW33, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The thirty-fifth method, called AdaGradW34, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The thirty-sixth method, called AdaGradW35, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The thirty-seventh method, called AdaGradW36, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The thirty-eighth method, called AdaGradW37, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The thirty-ninth method, called AdaGradW38, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The fortieth method, called AdaGradW39, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The forty-first method, called AdaGradW40, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The forty-second method, called AdaGradW41, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The forty-third method, called AdaGradW42, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The forty-fourth method, called AdaGradW43, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The forty-fifth method, called AdaGradW44, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The forty-sixth method, called AdaGradW45, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The forty-seventh method, called AdaGradW46, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The forty-eighth method, called AdaGradW47, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The forty-ninth method, called AdaGradW48, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The fiftieth method, called AdaGradW49, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The fifty-first method, called AdaGradW50, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The fifty-second method, called AdaGradW51, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The fifty-third method, called AdaGradW52, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The fifty-fourth method, called AdaGradW53, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The fifty-fifth method, called AdaGradW54, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The fifty-sixth method, called AdaGradW55, is an adaptive variant of the Adam algorithm with a per-iteration learning rate schedule that depends only on the current and previous iterations. It is based on the use of two moving averages: one for the squared gradient norms and another for the product of the squared gradient norm and the number of the current iteration. The fifty-seventh method, called AdaGradW56, is an adaptive variant of the Adam algorithm