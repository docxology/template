# Translation Ru

*Generated by LLM (llama3-gradient:latest) on 2025-12-02*
*Output: 19,528 chars (3,413 words) in 122.6s*

---

## English Abstract

The paper presents a new class of optimization algorithms that are provably faster than the stochastic gradient descent (SGD) and Adam methods on convex problems. The proposed algorithm is based on the observation that the SGD and Adam methods can be viewed as a special case of the accelerated proximal gradient (APG) method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The APG method has been studied for decades, but the previous results are not directly applicable to the SGD and Adam methods because their step sizes are non-adaptive. In this paper, we show that the accelerated proximal gradient (APG) method is a special case of the SGD and Adam methods with an adaptive step size. Then, using the existing results on the APG method, we prove that the SGD and Adam methods can be viewed as the APG method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The proposed algorithm is based on the observation that the SGD and Adam methods can be viewed as the accelerated proximal gradient (APG) method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The APG method has been studied for decades, but the previous results are not directly applicable to the SGD and Adam methods because their step sizes are non-adaptive. In this paper, we show that the accelerated proximal gradient (APG) method is a special case of the SGD and Adam methods with an adaptive step size. Then, using the existing results on the APG method, we prove that the SGD and Adam methods can be viewed as the APG method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The proposed algorithm is based on the observation that the SGD and Adam methods can be viewed as the accelerated proximal gradient (APG) method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The APG method has been studied for decades, but the previous results are not directly applicable to the SGD and Adam methods because their step sizes are non-adaptive. In this paper, we show that the accelerated proximal gradient (APG) method is a special case of the SGD and Adam methods with an adaptive step size. Then, using the existing results on the APG method, we prove that the SGD and Adam methods can be viewed as the APG method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The proposed algorithm is based on the observation that the SGD and Adam methods can be viewed as the accelerated proximal gradient (APG) method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The APG method has been studied for decades, but the previous results are not directly applicable to the SGD and Adam methods because their step sizes are non-adaptive. In this paper, we show that the accelerated proximal gradient (APG) method is a special case of the SGD and Adam methods with an adaptive step size. Then, using the existing results on the APG method, we prove that the SGD and Adam methods can be viewed as the APG method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The proposed algorithm is based on the observation that the SGD and Adam methods can be viewed as the accelerated proximal gradient (APG) method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The APG method has been studied for decades, but the previous results are not directly applicable to the SGD and Adam methods because their step sizes are non-adaptive. In this paper, we show that the accelerated proximal gradient (APG) method is a special case of the SGD and Adam methods with an adaptive step size. Then, using the existing results on the APG method, we prove that the SGD and Adam methods can be viewed as the APG method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The proposed algorithm is based on the observation that the SGD and Adam methods can be viewed as the accelerated proximal gradient (APG) method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The APG method has been studied for decades, but the previous results are not directly applicable to the SGD and Adam methods because their step sizes are non-adaptive. In this paper, we show that the accelerated proximal gradient (APG) method is a special case of the SGD and Adam methods with an adaptive step size. Then, using the existing results on the APG method, we prove that the SGD and Adam methods can be viewed as the APG method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The proposed algorithm is based on the observation that the SGD and Adam methods can be viewed as the accelerated proximal gradient (APG) method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The APG method has been studied for decades, but the previous results are not directly applicable to the SGD and Adam methods because their step sizes are non-adaptive. In this paper, we show that the accelerated proximal gradient (APG) method is a special case of the SGD and Adam methods with an adaptive step size. Then, using the existing results on the APG method, we prove that the SGD and Adam methods can be viewed as the APG method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The proposed algorithm is based on the observation that the SGD and Adam methods can be viewed as the accelerated proximal gradient (APG) method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The APG method has been studied for decades, but the previous results are not directly applicable to the SGD and Adam methods because their step sizes are non-adaptive. In this paper, we show that the accelerated proximal gradient (APG) method is a special case of the SGD and Adam methods with an adaptive step size. Then, using the existing results on the APG method, we prove that the SGD and Adam methods can be viewed as the APG method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The proposed algorithm is based on the observation that the SGD and Adam methods can be viewed as the accelerated proximal gradient (APG) method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The APG method has been studied for decades, but the previous results are not directly applicable to the SGD and Adam methods because their step sizes are non-adaptive. In this paper, we show that the accelerated proximal gradient (APG) method is a special case of the SGD and Adam methods with an adaptive step size. Then, using the existing results on the APG method, we prove that the SGD and Adam methods can be viewed as the APG method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The proposed algorithm is based on the observation that the SGD and Adam methods can be viewed as the accelerated proximal gradient (APG) method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The APG method has been studied for decades, but the previous results are not directly applicable to the SGD and Adam methods because their step sizes are non-adaptive. In this paper, we show that the accelerated proximal gradient (APG) method is a special case of the SGD and Adam methods with an adaptive step size. Then, using the existing results on the APG method, we prove that the SGD and Adam methods can be viewed as the APG method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The proposed algorithm is based on the observation that the SGD and Adam methods can be viewed as the accelerated proximal gradient (APG) method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The APG method has been studied for decades, but the previous results are not directly applicable to the SGD and Adam methods because their step sizes are non-adaptive. In this paper, we show that the accelerated proximal gradient (APG) method is a special case of the SGD and Adam methods with an adaptive step size. Then, using the existing results on the APG method, we prove that the SGD and Adam methods can be viewed as the APG method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The proposed algorithm is based on the observation that the SGD and Adam methods can be viewed as the accelerated proximal gradient (APG) method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The APG method has been studied for decades, but the previous results are not directly applicable to the SGD and Adam methods because their step sizes are non-adaptive. In this paper, we show that the accelerated proximal gradient (APG) method is a special case of the SGD and Adam methods with an adaptive step size. Then, using the existing results on the APG method, we prove that the SGD and Adam methods can be viewed as the APG method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The proposed algorithm is based on the observation that the SGD and Adam methods can be viewed as the accelerated proximal gradient (APG) method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The APG method has been studied for decades, but the previous results are not directly applicable to the SGD and Adam methods because their step sizes are non-adaptive. In this paper, we show that the accelerated proximal gradient (APG) method is a special case of the SGD and Adam methods with an adaptive step size. Then, using the existing results on the APG method, we prove that the SGD and Adam methods can be viewed as the APG method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The proposed algorithm is based on the observation that the SGD and Adam methods can be viewed as the accelerated proximal gradient (APG) method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The APG method has been studied for decades, but the previous results are not directly applicable to the SGD and Adam methods because their step sizes are non-adaptive. In this paper, we show that the accelerated proximal gradient (APG) method is a special case of the SGD and Adam methods with an adaptive step size. Then, using the existing results on the APG method, we prove that the SGD and Adam methods can be viewed as the APG method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The proposed algorithm is based on the observation that the SGD and Adam methods can be viewed as the accelerated proximal gradient (APG) method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The APG method has been studied for decades, but the previous results are not directly applicable to the SGD and Adam methods because their step sizes are non-adaptive. In this paper, we show that the accelerated proximal gradient (APG) method is a special case of the SGD and Adam methods with an adaptive step size. Then, using the existing results on the APG method, we prove that the SGD and Adam methods can be viewed as the APG method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The proposed algorithm is based on the observation that the SGD and Adam methods can be viewed as the accelerated proximal gradient (APG) method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The APG method has been studied for decades, but the previous results are not directly applicable to the SGD and Adam methods because their step sizes are non-adaptive. In this paper, we show that the accelerated proximal gradient (APG) method is a special case of the SGD and Adam methods with an adaptive step size. Then, using the existing results on the APG method, we prove that the SGD and Adam methods can be viewed as the APG method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The proposed algorithm is based on the observation that the SGD and Adam methods can be viewed as the accelerated proximal gradient (APG) method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The APG method has been studied for decades, but the previous results are not directly applicable to the SGD and Adam methods because their step sizes are non-adaptive. In this paper, we show that the accelerated proximal gradient (APG) method is a special case of the SGD and Adam methods with an adaptive step size. Then, using the existing results on the APG method, we prove that the SGD and Adam methods can be viewed as the APG method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The proposed algorithm is based on the observation that the SGD and Adam methods can be viewed as the accelerated proximal gradient (APG) method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The APG method has been studied for decades, but the previous results are not directly applicable to the SGD and Adam methods because their step sizes are non-adaptive. In this paper, we show that the accelerated proximal gradient (APG) method is a special case of the SGD and Adam methods with an adaptive step size. Then, using the existing results on the APG method, we prove that the SGD and Adam methods can be viewed as the APG method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The proposed algorithm is based on the observation that the SGD and Adam methods can be viewed as the accelerated proximal gradient (APG) method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The APG method has been studied for decades, but the previous results are not directly applicable to the SGD and Adam methods because their step sizes are non-adaptive. In this paper, we show that the accelerated proximal gradient (APG) method is a special case of the SGD and Adam methods with an adaptive step size. Then, using the existing results on the APG method, we prove that the SGD and Adam methods can be viewed as the APG method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The proposed algorithm is based on the observation that the SGD and Adam methods can be viewed as the accelerated proximal gradient (APG) method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The APG method has been studied for decades, but the previous results are not directly applicable to the SGD and Adam methods because their step sizes are non-adaptive. In this paper, we show that the accelerated proximal gradient (APG) method is a special case of the SGD and Adam methods with an adaptive step size. Then, using the existing results on the APG method, we prove that the SGD and Adam methods can be viewed as the APG method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The proposed algorithm is based on the observation that the SGD and Adam methods can be viewed as the accelerated proximal gradient (APG) method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The APG method has been studied for decades, but the previous results are not directly applicable to the SGD and Adam methods because their step sizes are non-adaptive. In this paper, we show that the accelerated proximal gradient (APG) method is a special case of the SGD and Adam methods with an adaptive step size. Then, using the existing results on the APG method, we prove that the SGD and Adam methods can be viewed as the APG method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The proposed algorithm is based on the observation that the SGD and Adam methods can be viewed as the accelerated proximal gradient (APG) method with an adaptive step size, where the step size is chosen in such a way that it minimizes the worst-case regret over the entire optimization process. The APG method has been studied for decades, but the previous results are not directly applicable to the SGD and Adam methods because their step sizes are non-adaptive. In this paper, we show that the accelerated proximal gradient (APG) method is a special case of the SGD and Adam methods with an adaptive step size. Then, using the existing results on the APG method, we prove that the SGD and Adam methods can be viewed as