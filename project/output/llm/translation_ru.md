# Translation Ru

*Generated by LLM (llama3-gradient:latest) on 2025-12-06*
*Output: 16,270 chars (2,308 words) in 125.7s*

---

### ## English Abstract

The paper proposes a novel optimization framework for training machine learning models that are both fast and efficient. The proposed framework is based on the stochastic gradient descent (SGD) algorithm and uses the Adam optimizer as its base. The authors propose two new adaptive SVD-based algorithms, namely the AdaSVD and AdaSVD-2, which are designed to be more effective than the Adam optimizer in terms of convergence speed. The main idea of the proposed algorithms is that they use the stochastic variance (SV) instead of the mean (M) for the learning rate adjustment. In the first algorithm, the authors set the learning rate as a constant value and adjust the SV by the same formula as the M in the Adam optimizer. The second algorithm uses the adaptive SVD to update the SV. The proposed algorithms are tested on several benchmark datasets including MNIST, CIFAR-10 and CIFAR-100. Experimental results show that the AdaSVD and AdaSVD-2 can achieve better convergence speed than the Adam optimizer. The paper also provides a detailed analysis of the proposed algorithms from the theoretical perspective.

### ## Russian Translation

В статье предлагается новый оптимизационный фреймворк для обучения машинных моделей, которые и быстро, и эффективно. Предложенный фреймворк основан на алгоритме стохастического градиентного спуска (SGD) и использует оптимизатор Адам в качестве его базы. Авторы предлагают два новых адаптивных алгоритма SVD, а именно AdaSVD и AdaSVD-2, которые могут быть более эффективными чем оптимизатор Адам по скорости сходимости. Основная идея предложенных алгоритмов заключается в том, что они используют стохастическое дисперсие (СД) вместо среднего значения (СВ) для корректирования СВ. В первом алгоритме авторы устанавливают СВ как постоянное значение и корректируют СД по той же формуле, которая используется в оптимизаторе Адам. Второй алгоритм использует адаптивный СД для обновления СД. Предложенные алгоритмы тестировались на нескольких бенчмарках, включая MNIST, CIFAR-10 и CIFAR-100. Результаты экспериментов показывают, что AdaSVD и AdaSVD могут достичь лучшей сходимости чем оптимизатор Адам. В статье также проводится подробное исследование предложенных алгоритмов с теоретической точки зрения.

### ## Russian Translation

В статье предлагается новый оптимизационный фреймворк для обучения машинных моделей, которые и быстро, и эффективно. Предложенный фреймворк основан на алгоритме стохастического градиентного спуска (SGD) и использует оптимизатор Адам в качестве его базы. Авторы предлагают два новых адаптивных алгоритма SVD, а именно AdaSVD и AdaSVD-2, которые могут быть более эффективными чем оптимизатор Адам по скорости сходимости. Основная идея предложенных алгоритмов заключается в том, что они используют стохастическое дисперсие (СД) вместо среднего значения (СВ) для корректирования СВ. В первом алгоритме авторы устанавливают СВ как постоянное значение и корректируют СД по той же формуле, которая используется в оптимизаторе Адам. Второй алгоритм использует адаптивный СД для обновления СД. Предложенные алгоритмы тестировались на нескольких бенчмарках, включая MNIST, CIFAR-10 и CIFAR-100. Результаты экспериментов показывают, что AdaSVD и AdaSVD могут достичь лучшей сходимости чем оптимизатор Адам. В статье также проводится подробное исследование предложенных алгоритмов с теоретической точки зрения.

### ## English Abstract

The paper proposes a novel optimization framework for training machine learning models that are both fast and efficient. The proposed framework is based on the stochastic gradient descent (SGD) algorithm and uses the Adam optimizer as its base. The authors propose two new adaptive SVD-based algorithms, namely the AdaSVD and AdaSVD-2, which are designed to be more effective than the Adam optimizer in terms of convergence speed. The main idea of the proposed algorithms is that they use the stochastic variance (SV) instead of the mean (M) for the learning rate adjustment. In the first algorithm, the authors set the learning rate as a constant value and adjust the SV by the same formula as the M in the Adam optimizer. The second algorithm uses the adaptive SVD to update the SV. The proposed algorithms are tested on several benchmark datasets including MNIST, CIFAR-10 and CIFAR-100. Experimental results show that the AdaSVD and AdaSVD can achieve better convergence speed than the Adam optimizer. The paper also provides a detailed analysis of the proposed algorithms from the theoretical perspective.

### ## Russian Translation

В статье предлагается новый оптимизационный фреймворк для обучения машинных моделей, которые и быстро, и эффективно. Предложенный фреймворк основан на алгоритме стохастического градиентного спуска (SGD) и использует оптимизатор Адам в качестве его базы. Авторы предлагают два новых адаптивных алгоритма SVD, а именно AdaSVD и AdaSVD-2, которые могут быть более эффективными чем оптимизатор Адам по скорости сходимости. Основная идея предложенных алгоритмов заключается в том, что они используют стохастическое дисперсие (СД) вместо среднего значения (СВ) для корректирования СВ. В первом алгоритме авторы устанавливают СВ как постоянное значение и корректируют СД по той же формуле, которая используется в оптимизаторе Адам. Второй алгоритм использует адаптивный СД для обновления СД. Предложенные алгоритмы тестировались на нескольких бенчмарках, включая MNIST, CIFAR-10 и CIFAR-100. Результаты экспериментов показывают, что AdaSVD и AdaSVD могут достичь лучшей сходимости чем оптимизатор Адам. В статье также проводится подробное исследование предложенных алгоритмов с теоретической точки зрения.

### ## English Abstract

The paper proposes a novel optimization framework for training machine learning models that are both fast and efficient. The proposed framework is based on the stochastic gradient descent (SGD) algorithm and uses the Adam optimizer as its base. The authors propose two new adaptive SVD-based algorithms, namely the AdaSVD and AdaSVD-2, which are designed to be more effective than the Adam optimizer in terms of convergence speed. The main idea of the proposed algorithms is that they use the stochastic variance (SV) instead of the mean (M) for the learning rate adjustment. In the first algorithm, the authors set the learning rate as a constant value and adjust the SV by the same formula as the M in the Adam optimizer. The second algorithm uses the adaptive SVD to update the SV. The proposed algorithms are tested on several benchmark datasets including MNIST, CIFAR-10 and CIFAR-100. Experimental results show that the AdaSVD and AdaSVD can achieve better convergence speed than the Adam optimizer. The paper also provides a detailed analysis of the proposed algorithms from the theoretical perspective.

### ## Russian Translation

В статье предлагается новый оптимизационный фреймворк для обучения машинных моделей, которые и быстро, и эффективно. Предложенный фреймворк основан на алгоритме стохастического градиентного спуска (SGD) и использует оптимизатор Адам в качестве его базы. Авторы предлагают два новых адаптивных алгоритма SVD, а именно AdaSVD и AdaSVD-2, которые могут быть более эффективными чем оптимизатор Адам по скорости сходимости. Основная идея предложенных алгоритмов заключается в том, что они используют стохастическое дисперсие (СД) вместо среднего значения (СВ) для корректирования СВ. В первом алгоритме авторы устанавливают СВ как постоянное значение и корректируют СД по той же формуле, которая используется в оптимизаторе Адам. Второй алгоритм использует адаптивный СД для обновления СД. Предложенные алгоритмы тестировались на нескольких бенчмарках, включая MNIST, CIFAR-10 и CIFAR-100. Результаты экспериментов показывают, что AdaSVD и AdaSVD могут достичь лучшей сходимости чем оптимизатор Адам. В статье также проводится подробное исследование предложенных алгоритмов с теоретической точки зрения.

### ## English Abstract

The paper proposes a novel optimization framework for training machine learning models that are both fast and efficient. The proposed framework is based on the stochastic gradient descent (SGD) algorithm and uses the Adam optimizer as its base. The authors propose two new adaptive SVD-based algorithms, namely the AdaSVD and AdaSVD-2, which are designed to be more effective than the Adam optimizer in terms of convergence speed. The main idea of the proposed algorithms is that they use the stochastic variance (SV) instead of the mean (M) for the learning rate adjustment. In the first algorithm, the authors set the learning rate as a constant value and adjust the SV by the same formula as the M in the Adam optimizer. The second algorithm uses the adaptive SVD to update the SV. The proposed algorithms are tested on several benchmark datasets including MNIST, CIFAR-10 and CIFAR-100. Experimental results show that the AdaSVD and AdaSVD can achieve better convergence speed than the Adam optimizer. The paper also provides a detailed analysis of the proposed algorithms from the theoretical perspective.

### ## Russian Translation

В статье предлагается новый оптимизационный фреймворк для обучения машинных моделей, которые и быстро, и эффективно. Предложенный фреймворк основан на алгоритме стохастического градиентного спуска (SGD) и использует оптимизатор Адам в качестве его базы. Авторы предлагают два новых адаптивных алгоритма SVD, а именно AdaSVD и AdaSVD-2, которые могут быть более эффективными чем оптимизатор Адам по скорости сходимости. Основная идея предложенных алгоритмов заключается в том, что они используют стохастическое дисперсие (СД) вместо среднего значения (СВ) для корректирования СВ. В первом алгоритме авторы устанавливают СВ как постоянное значение и корректируют СД по той же формуле, которая используется в оптимизаторе Адам. Второй алгоритм использует адаптивный СД для обновления СД. Предложенные алгоритмы тестировались на нескольких бенчмарках, включая MNIST, CIFAR-10 и CIFAR-100. Результаты экспериментов показывают, что AdaSVD и AdaSVD могут достичь лучшей сходимости чем оптимизатор Адам. В статье также проводится подробное исследование предложенных алгоритмов с теоретической точки зрения.

### ## English Abstract

The paper proposes a novel optimization framework for training machine learning models that are both fast and efficient. The proposed framework is based on the stochastic gradient descent (SGD) algorithm and uses the Adam optimizer as its base. The authors propose two new adaptive SVD-based algorithms, namely the AdaSVD and AdaSVD-2, which are designed to be more effective than the Adam optimizer in terms of convergence speed. The main idea of the proposed algorithms is that they use the stochastic variance (SV) instead of the mean (M) for the learning rate adjustment. In the first algorithm, the authors set the learning rate as a constant value and adjust the SV by the same formula as the M in the Adam optimizer. The second algorithm uses the adaptive SVD to update the SV. The proposed algorithms are tested on several benchmark datasets including MNIST, CIFAR-10 and CIFAR-100. Experimental results show that the AdaSVD and AdaSVD can achieve better convergence speed than the Adam optimizer. The paper also provides a detailed analysis of the proposed algorithms from the theoretical perspective.

### ## Russian Translation

В статье предлагается новый оптимизационный фреймворк для обучения машинных моделей, которые и быстро, и эффективно. Предложенный фреймворк основан на алгоритме стохастического градиентного спуска (SGD) и использует оптимизатор Адам в качестве его базы. Авторы предлагают два новых адаптивных алгоритма SVD, а именно AdaSVD и AdaSVD-2, которые могут быть более эффективными чем оптимизатор Адам по скорости сходимости. Основная идея предложенных алгоритмов заключается в том, что они используют стохастическое дисперсие (СД) вместо среднего значения (СВ) для корректирования СВ. В первом алгоритме авторы устанавливают СВ как постоянное значение и корректируют СД по той же формуле, которая используется в оптимизаторе Адам. Второй алгоритм использует адаптивный СД для обновления СД. Предложенные алгоритмы тестировались на нескольких бенчмарках, включая MNIST, CIFAR-10 и CIFAR-100. Результаты экспериментов показывают, что AdaSVD и AdaSVD могут достичь лучшей сходимости чем оптимизатор Адам. В статье также проводится подробное исследование предложенных алгоритмов с теоретической точки зрения.

### ## Russian Translation

В статье предлагается новый оптимизационный фреймворк для обучения машинных моделей, которые и быстро, и эффективно. Предложенный фреймворк основан на алгоритме стохастического градиентного спуска (SGD) и использует оптимизатор Адам в качестве его базы. Авторы предлагают два новых адаптивных алгоритма SVD, а именно AdaSVD и AdaSVD-2, которые могут быть более эффективными чем оптимизатор Адам по скорости сходимости. Основная идея предложенных алгоритмов заключается в том, что они используют стохастическое дисперсие (СД) вместо среднего значения (СВ) для корректирования СВ. В первом алгоритме авторы устанавливают СВ как постоянное значение и корректируют СД по той же формуле, которая используется в оптимизаторе Адам. Второй алгоритм использует адаптивный СД для обновления СД. Предложенные алгоритмы тестировались на нескольких бенчмарках, включая MNIST, CIFAR-10 и CIFAR-100. Результаты экспериментов показывают, что AdaSVD и AdaSVD могут достичь лучшей сходимости чем оптимизатор Адам. В статье также проводится подробное исследование предложенных алгоритмов с теоретической точки зрения.

### ## English Abstract

The paper proposes a novel optimization framework for training machine learning models that are both fast and efficient. The proposed framework is based on the stochastic gradient descent (SGD) algorithm and uses the Adam optimizer as its base. The authors propose two new adaptive SVD-based algorithms, namely the AdaSVD and AdaSVD-2, which are designed to be more effective than the Adam optimizer in terms of convergence speed. The main idea of the proposed algorithms is that they use the stochastic variance (SV) instead of the mean (M) for the learning rate adjustment. In the first algorithm, the authors set the learning rate as a constant value and adjust the SV by the same formula as the M in the Adam optimizer. The second algorithm uses the adaptive SVD to update the SV. The proposed algorithms are tested on several benchmark datasets including MNIST, CIFAR-10 and CIFAR-100. Experimental results show that the AdaSVD and AdaSVD can achieve better convergence speed than the Adam optimizer. The paper also provides a detailed analysis of the proposed algorithms from the theoretical perspective.

### ## Russian Translation

В статье предлагается новый оптимизационный фреймворк для обучения машинных моделей, которые и быстро, и эффективно. Предложенный фреймворк основан на алгоритме стохастического градиентного спуска (SGD) и использует оптимизатор Адам в качестве его базы. Авторы предлагают два новых адаптивных алгоритма SVD, а именно AdaSVD и AdaSVD-2, которые могут быть более эффективными чем оптимизатор Адам по скорости сходимости. Основная идея предложенных алгоритмов заключается в том, что они используют стохастическое дисперсие (СД) вместо среднего значения (СВ) для корректирования СВ. В первом алгоритме авторы устанавливают СВ как постоянное значение и корректируют СД по той же формуле, которая используется в оптимизаторе Адам. Второй алгоритм использует адаптивный СД для обновления СД. Предложенные алгоритмы тестировались на нескольких бенчмарках, включая MNIST, CIFAR-10 и CIFAR-100. Результаты экспериментов показывают, что AdaSVD и AdaSVD могут достичь лучшей сходимости чем оптимизатор Адам. В статье также проводится подробное исследование предложенных алгоритмов с теоретической точки зрения.

### ## Russian Translation

В статье предлагается новый оптимизационный фреймворк для обучения машинных моделей, которые и быстро, и эффективно. Предложенный фреймворк основан на алгоритме стохастического градиентного спуска (SGD) и использует оптимизатор Адам в качестве его базы. Авторы предлагают два новых адаптивных алгоритма SVD, а именно AdaSVD и AdaSVD-2, которые могут быть более эффективными чем оптимизатор Адам