# Translation Hi

*Generated by LLM (llama3-gradient:latest) on 2025-12-08*
*Output: 22,999 chars (3,538 words) in 123.8s*

---

### ## English Abstract


This paper presents a novel approach to the problem of accelerating the convergence of stochastic gradient descent (SGD) for training deep neural networks. The key idea is to use an adaptive learning rate that depends on the magnitude of the gradient, and which is independent of the iteration number. This is in contrast to the standard practice where the learning rate is fixed or decreased with the iteration number. In this paper, we show that such a strategy can be implemented by using the stochastic average gradient (SAG) algorithm, and that it has the same convergence guarantee as SAG for strongly convex objectives. The SAG algorithm is an adaptive version of SGD which uses the moving average to update the learning rate. This is in contrast to Adam, which also uses the moving average but with a different strategy. We provide the first non-asymptotic analysis of the SAG algorithm and show that it has the same convergence guarantee as SAG for strongly convex objectives. The main contribution of this paper is the new analysis of the SAG algorithm, and the key finding is that the SAG algorithm can be used to accelerate the training of deep neural networks with a strong adaptive learning rate. We provide an extensive experimental study on the effectiveness of the proposed approach, which shows that it can achieve better performance than Adam for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives. This is in contrast to Adam, which has a different convergence guarantee. Secondly, we provide an extensive experimental study on the effectiveness of the proposed approach. We compare the performance of the proposed adaptive learning rate with the standard fixed and decreasing strategies, and show that it can achieve better performance than these two baselines for many problems. The main contributions are as follows.
Firstly, we show that the SAG algorithm has the same convergence guarantee as SAG for strongly convex objectives.