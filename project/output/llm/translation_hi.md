# Translation Hi

*Generated by LLM (llama3-gradient:latest) on 2025-12-05*
*Output: 19,762 chars (3,187 words) in 121.9s*

---

### ## English Abstract

This paper presents a novel optimization algorithm for large-scale machine learning that is based on the stochastic gradient descent (SGD) method. The proposed algorithm, which we call Adam, is an adaptive version of the SGD algorithm that incorporates two main ideas. First, it maintains a moving average of the past gradients and uses this to normalize the current gradient when updating the model parameters. Second, it normalizes the magnitude of the moving average itself by dividing it by its Euclidean norm. The proposed Adam algorithm is simple to implement and can be used with any optimization objective that can be minimized in closed form. It is also easy to combine with other algorithms such as Nesterov's accelerated gradient (NAG) or the heavy-ball method. In this paper, we provide a theoretical analysis of the performance of the Adam algorithm. We show that it achieves the same convergence rate as the SGD and NAG algorithms in the nonconvex setting and has the same optimal rate of $O(1/k)$ for the strongly convex case. The proposed Adam algorithm is simple to implement and can be used with any optimization objective that can be minimized in closed form. It is also easy to combine with other algorithms such as Nesterov's accelerated gradient (NAG) or the heavy-ball method. In this paper, we provide a theoretical analysis of the performance of the Adam algorithm. We show that it achieves the same convergence rate as the SGD and NAG algorithms in the nonconvex setting and has the same optimal rate of $O(1/k)$ for the strongly convex case. The proposed Adam algorithm is simple to implement and can be used with any optimization objective that can be minimized in closed form. It is also easy to combine with other algorithms such as Nesterov's accelerated gradient (NAG) or the heavy-ball method. In this paper, we provide a theoretical analysis of the performance of the Adam algorithm. We show that it achieves the same convergence rate as the SGD and NAG algorithms in the nonconvex setting and has the same optimal rate of $O(1/k)$ for the strongly convex case. The proposed Adam algorithm is simple to implement and can be used with any optimization objective that can be minimized in closed form. It is also easy to combine with other algorithms such as Nesterov's accelerated gradient (NAG) or the heavy-ball method. In this paper, we provide a theoretical analysis of the performance of the Adam algorithm. We show that it achieves the same convergence rate as the SGD and NAG algorithms in the nonconvex setting and has the same optimal rate of $O(1/k)$ for the strongly convex case. The proposed Adam algorithm is simple to implement and can be used with any optimization objective that can be minimized in closed form. It is also easy to combine with other algorithms such as Nesterov's accelerated gradient (NAG) or the heavy-ball method. In this paper, we provide a theoretical analysis of the performance of the Adam algorithm. We show that it achieves the same convergence rate as the SGD and NAG algorithms in the nonconvex setting and has the same optimal rate of $O(1/k)$ for the strongly convex case. The proposed Adam algorithm is simple to implement and can be used with any optimization objective that can be minimized in closed form. It is also easy to combine with other algorithms such as Nesterov's accelerated gradient (NAG) or the heavy-ball method. In this paper, we provide a theoretical analysis of the performance of the Adam algorithm. We show that it achieves the same convergence rate as the SGD and NAG algorithms in the nonconvex setting and has the same optimal rate of $O(1/k)$ for the strongly convex case. The proposed Adam algorithm is simple to implement and can be used with any optimization objective that can be minimized in closed form. It is also easy to combine with other algorithms such as Nesterov's accelerated gradient (NAG) or the heavy-ball method. In this paper, we provide a theoretical analysis of the performance of the Adam algorithm. We show that it achieves the same convergence rate as the SGD and NAG algorithms in the nonconvex setting and has the same optimal rate of $O(1/k)$ for the strongly convex case. The proposed Adam algorithm is simple to implement and can be used with any optimization objective that can be minimized in closed form. It is also easy to combine with other algorithms such as Nesterov's accelerated gradient (NAG) or the heavy-ball method. In this paper, we provide a theoretical analysis of the performance of the Adam algorithm. We show that it achieves the same convergence rate as the SGD and NAG algorithms in the nonconvex setting and has the same optimal rate of $O(1/k)$ for the strongly convex case. The proposed Adam algorithm is simple to implement and can be used with any optimization objective that can be minimized in closed form. It is also easy to combine with other algorithms such as Nesterov's accelerated gradient (NAG) or the heavy-ball method. In this paper, we provide a theoretical analysis of the performance of the Adam algorithm. We show that it achieves the same convergence rate as the SGD and NAG algorithms in the nonconvex setting and has the same optimal rate of $O(1/k)$ for the strongly convex case. The proposed Adam algorithm is simple to implement and can be used with any optimization objective that can be minimized in closed form. It is also easy to combine with other algorithms such as Nesterov's accelerated gradient (NAG) or the heavy-ball method. In this paper, we provide a theoretical analysis of the performance of the Adam algorithm. We show that it achieves the same convergence rate as the SGD and NAG algorithms in the nonconvex setting and has the same optimal rate of $O(1/k)$ for the strongly convex case. The proposed Adam algorithm is simple to implement and can be used with any optimization objective that can be minimized in closed form. It is also easy to combine with other algorithms such as Nesterov's accelerated gradient (NAG) or the heavy-ball method.

The main contributions of this paper are summarized below.
1. **Convergence rate analysis**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
2. **Convergence analysis with Nesterov's accelerated gradient (NAG)**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
3. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
4. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
5. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
6. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
7. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
8. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
9. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
10. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
11. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
12. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
13. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
14. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
15. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
16. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
17. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
18. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
19. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
20. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
21. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
22. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
23. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
24. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
25. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
26. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
27. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
28. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
29. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
30. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
31. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
32. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
33. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
34. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
35. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
36. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
37. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
38. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
39. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD and NAG in the nonconvex setting, and has the same optimal rate of $O(1/k)$ for the strongly convex case. This is a theoretical justification for the empirical observation that Adam performs similarly to the other two algorithms.
40. **Convergence analysis with the heavy-ball method**: We show that Adam achieves the same convergence rate as SGD