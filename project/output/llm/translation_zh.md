# Translation Zh

*Generated by LLM (llama3-gradient:latest) on 2025-12-08*
*Output: 2,112 chars (222 words) in 31.5s*

---

### ## English Abstract

This paper proposes a new algorithm for stochastic optimization that is adaptive to the learning rate and converges faster than existing algorithms. The proposed algorithm, called Adam, combines the advantages of two popular methods for stochastic gradient descent (SGD): the stochastic average gradient (SAG) method and the exponential weight update (AdamW) method. SAG is a simple, intuitive, and widely used algorithm that has been shown to be effective in practice. However, it does not adapt well to the learning rate. AdamW adapts the learning rate by exponentially weighing the squared gradient, but its theoretical analysis is lacking. The proposed Adam algorithm combines the advantages of these two methods: it adapts the learning rate and converges faster than SAG. In this paper, we analyze the convergence of the proposed Adam algorithm under a very mild condition that is also assumed in the analysis for the AdamW method. We show that the proposed Adam algorithm has a better theoretical guarantee than the AdamW method. The proposed Adam algorithm can be used to solve both convex and non-convex problems. Experiments on several large-scale machine learning benchmarks demonstrate that it converges faster than existing algorithms.

### ## Chinese  (Simplified) Translation

本文提出了一种新的随机优化算法，该算法适应学习率，并且比现有算法更快地收敛。该算法，称为Adam，是将两种流行的SGD方法结合：随机平均梯度(SAG)方法和指数加权更新(AdamW)方法。SAG是简单、直观、广泛使用的一种算法，但它不太适应学习率。AdamW通过指数加权更新对学习率进行了调整，但是其理论分析缺乏。该提出的Adam算法将这两个优点结合：它适应学习率，并且比SAG更快地收敛。在本文中，我们对该提出的Adam算法的收敛性进行了分析，假设的条件也被用在对AdamW方法的分析中。我们证明，该提出的Adam算法有一个更好的理论保证于AdamW算法。该提出的Adam算法可以用于解决大规模机器学习问题。实验结果表明，它比现有算法更快地收敛。

### ## Chinese  (Simplified) Translation

本文提出了一种新的随机优化算法，该算法适应学习率，并且比现有算法更快地收敛。该算法，称为Adam，是将两种流行的SGD方法结合：随机平均梯度(SAG)方法和指数加权更新(AdamW)方法。SAG是简单、直观、广泛使用的一种算法，但它不太适应学习率。AdamW通过指数加权更新对学习率进行了调整，但是其理论分析缺乏。该提出的Adam算法将这两个优点结合：它适应学习率，并且比SAG更快地收敛。在本文中，我们对该提出的Adam算法的收敛性进行了分析，假设的条件也被用在对AdamW方法的分析中。我们证明，该提出的Adam算法有一个更好的理论保证于AdamW算法。该提出的Adam算法可以用于解决大规模机器学习问题。实验结果表明，它比现有算法更快地收敛。

Note: Please make sure to use native Chinese characters (Simplified Chinese) and do not transliterate.