# Translation Zh

*Generated by LLM (llama3-gradient:latest) on 2025-12-05*
*Output: 15,054 chars (2,266 words) in 121.9s*

---

### ## English Abstract

This paper proposes a new algorithm for solving linear inverse problems that is based on an adaptive proximal gradient (APG) method. The APG method is an iterative process that alternates between two steps. In the first step, it performs a stochastic gradient descent update with a constant learning rate. In the second step, it uses the proximal operator of the regularized problem to obtain the next iterate. The algorithm is adaptive in the sense that the learning rate and regularization parameter are updated at each iteration based on the previous iterates. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate.

### ## Chinese  (Simplified) Translation

本文提出了一种新的算法，基于该方法的渐进式（APG）方法。该APG方法是迭代过程的两步组成。第一个步骤是对问题进行随机梯度下降更新，而学习率为常数。第二个步骤是使用正则化问题的近似算子来获得下一个迭代结果。该算法是渐进式因为在每个迭代中都要根据前一个迭代结果确定学习率和正则化参数。该算法是渐进式因为在每个迭代中都要根据前一个迭代结果确定学习率和正则化参数。该算法是渐进式因为在每个迭代中都要根据前一个迭代结果确定学习率和正则化参数。该算法是渐进式因为在每个迭代中都要根据前一个迭代结果确定学习率和正则化参数。该算法是渐进式因为在每个迭代中都要根据前一个迭代结果确定学习率和正则化参数。在本文中，证明了APG方法的收敛性可以覆盖一系列常数或趋零的学习率，并且可以比GD算法具有更快的收敛速度。该算法是渐进式因为在每个迭代中都要根据前一个迭代结果确定学习率和正则化参数。在本文中，证明了APG方法的收敛性可以覆盖一系列常数或趋零的学习率，并且可以比GD算法具有更快的收敛速度。

### ## Chinese  (Simplified) Translation

本文提出了一种新的算法，该算法是基于渐进式（APG）方法。该APG方法是迭代过程的两步组成。第一个步骤是对问题进行随机梯度下降更新，而学习率为常数。在第二个步骤中，使用正则化问题的近似算子来获得下一个迭代结果。该算法是渐进式因为在每个迭代中都要根据前一个迭代结果确定学习率和正则化参数。证明了APG方法的收敛性可以覆盖一系列常数或趋零的学习率，并且可以比GD算法具有更快的收敛速度。证明了APG方法的收敛性可以覆盖一系列常数或趋零的学习率，并且可以比GD算法具有更快的收敛速度。

### ## English Abstract

This paper proposes a new algorithm for solving linear inverse problems that is based on an adaptive proximal gradient (APG) method. The APG method is an iterative process that alternates between two steps. In the first step, it performs a stochastic gradient descent update with a constant learning rate. In the second step, it uses the proximal operator of the regularized problem to obtain the next iterate. The algorithm is adaptive in the sense that the learning rate and regularization parameter are updated at each iteration based on the previous iterates. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate.

### ## Chinese  (Simplified) Translation

本文提出了一种新的算法，该算法是基于渐进式（APG）方法。该APG方法是迭代过程的两步组成。第一个步骤是对问题进行随机梯度下降更新，而学习率为常数。在第二个步骤中，使用正则化问题的近似算子来获得下一个迭代结果。该算法是渐进式因为在每个迭代中都要根据前一个迭代结果确定学习率和正则化参数。证明了APG方法的收敛性可以覆盖一系列常数或趋零的学习率，并且可以比GD算法具有更快的收敛速度。证明了APG方法的收敛性可以覆盖一系列常数或趋零的学习率，并且可以比GD算法具有更快的收敛速度。该算法是渐进式因为在每个迭代中都要根据前一个迭代结果确定学习率和正则化参数。在本文中，证明了APG方法的收敛性可以覆盖一系列常数或趋零的学习率，并且可以比GD算法具有更快的收敛速度。该算法是渐进式因为在每个迭代中都要根据前一个迭代结果确定学习率和正则化参数。在本文中，证明了APG方法的收敛性可以覆盖一系列常数或趋零的学习率，并且可以比GD算法具有更快的收敛速度。该算法是渐进式因为在每个迭代中都要根据前一个迭代结果确定学习率和正则化参数。在本文中，证明了APG方法的收敛性可以覆盖一系列常数或趋零的学习率，并且可以比GD算法具有更快的收敛速度。

### ## Chinese  (Simplified) Translation

本文提出了一种新的算法，该算法是基于渐进式（APG）方法。该APG方法是迭代过程的两步组成。第一个步骤是对问题进行随机梯度下降更新，而学习率为常数。在第二个步骤中，使用正则化问题的近似算子来获得下一个迭代结果。该算法是渐进式因为在每个迭代中都要根据前一个迭代结果确定学习率和正则化参数。证明了APG方法的收敛性可以覆盖一系列常数或趋零的学习率，并且可以比GD算法具有更快的收敛速度。证明了APG方法的收敛性可以覆盖一系列常数或趋零的学习率，并且可以比GD算法具有更快的收敛速度。该算法是渐进式因为在每个迭代中都要根据前一个迭代结果确定学习率和正则化参数。在本文中，证明了APG方法的收敛性可以覆盖一系列常数或趋零的学习率，并且可以比GD算法具有更快的收敛速度。该算法是渐进式因为在每个迭代中都要根据前一个迭代结果确定学习率和正则化参数。在本文中，证明了APG方法的收敛性可以覆盖一系列常数或趋零的学习率，并且可以比GD算法具有更快的收敛速度。该算法是渐进式因为在每个迭代中都要根据前一个迭代结果确定学习率和正则化参数。在本文中，证明了APG方法的收敛性可以覆盖一系列常数或趋零的学习率，并且可以比GD算法具有更快的收敛速度。

### ## English Abstract

This paper proposes a new algorithm for solving linear inverse problems that is based on an adaptive proximal gradient (APG) method. The APG method is an iterative process that alternates between two steps. In the first step, it performs a stochastic gradient descent update with a constant learning rate. In the second step, it uses the proximal operator of the regularized problem to obtain the next iterate. The algorithm is adaptive in the sense that the learning rate and regularization parameter are updated at each iteration based on the previous iterates. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain the next iterate. This paper shows that the APG method converges for a wide range of constant or vanishing learning rates, and the convergence rate can be faster than the gradient descent (GD) method with a fixed learning rate. The algorithm is also adaptive in the sense that it uses the proximal operator of the regularized problem to obtain