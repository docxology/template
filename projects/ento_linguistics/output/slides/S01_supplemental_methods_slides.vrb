\frametitle{Supplemental Methods}
\protect\phantomsection\label{sec:supplemental_methods}
This section provides detailed methodological information supplementing
Section \ref{sec:methodology}, focusing on the computational
implementation of Ento-Linguistic analysis.

\begin{block}{S1.1 Text Processing Pipeline Implementation}
\protect\phantomsection\label{s1.1-text-processing-pipeline-implementation}
\begin{block}{S1.1.1 Multi-Stage Text Normalization}
\protect\phantomsection\label{s1.1.1-multi-stage-text-normalization}
Our text processing pipeline implements systematic normalization to
ensure reliable pattern detection:

\begin{equation}\label{eq:text_normalization}
T_{\text{normalized}} = \text{lowercase}(\text{strip_punct}(\text{unicode_normalize}(T)))
\end{equation}

where \(T\) represents raw text input and each transformation step
standardizes linguistic variation while preserving semantic content.

\textbf{Tokenization Strategy}: We employ domain-aware tokenization that
recognizes scientific terminology:

\begin{equation}\label{eq:domain_tokenization}
\tau(T) = \bigcup_{t \in T} \begin{cases}
t & \text{if } t \in \mathcal{T}_{\text{scientific}} \\
\text{word_tokenize}(t) & \text{otherwise}
\end{cases}
\end{equation}

where \(\mathcal{T}_{\text{scientific}}\) contains curated scientific
terminology that should not be further subdivided.
\end{block}

\begin{block}{S1.1.2 Linguistic Preprocessing Pipeline}
\protect\phantomsection\label{s1.1.2-linguistic-preprocessing-pipeline}
The complete preprocessing pipeline includes:

\begin{enumerate}
\tightlist
\item
  \textbf{Unicode Normalization}: Standardizing character encodings
\item
  \textbf{Case Folding}: Converting to lowercase for consistency
\item
  \textbf{Punctuation Handling}: Removing or preserving scientific
  notation
\item
  \textbf{Number Normalization}: Standardizing numerical expressions
\item
  \textbf{Stop Word Filtering}: Domain-aware removal of non-informative
  terms
\item
  \textbf{Lemmatization}: Reducing words to base forms using scientific
  dictionaries
\end{enumerate}
\end{block}
\end{block}

\begin{block}{S1.2 Terminology Extraction Algorithms}
\protect\phantomsection\label{s1.2-terminology-extraction-algorithms}
\begin{block}{S1.2.1 Domain-Specific Term Identification}
\protect\phantomsection\label{s1.2.1-domain-specific-term-identification}
Terminology extraction uses a multi-criteria approach combining
statistical and linguistic features:

\begin{equation}\label{eq:term_extraction_score}
S(t) = \alpha \cdot \text{TF-IDF}(t) + \beta \cdot \text{domain_relevance}(t) + \gamma \cdot \text{linguistic_features}(t)
\end{equation}

where weights \(\alpha, \beta, \gamma\) are calibrated for each
Ento-Linguistic domain.

\textbf{Domain Relevance Scoring}: Terms are scored for relevance to
specific domains using:

\begin{itemize}
\tightlist
\item
  \textbf{Co-occurrence Patterns}: Terms frequently appearing with
  domain indicators
\item
  \textbf{Semantic Similarity}: Vector similarity to domain seed terms
\item
  \textbf{Contextual Features}: Syntactic patterns characteristic of
  domain usage
\end{itemize}
\end{block}

\begin{block}{S1.2.2 Ambiguity Detection Framework}
\protect\phantomsection\label{s1.2.2-ambiguity-detection-framework}
Ambiguity detection identifies terms with context-dependent meanings:

\begin{equation}\label{eq:ambiguity_score}
A(t) = \frac{H(\text{contexts}(t))}{\log |\text{contexts}(t)|} \cdot \frac{|\text{meanings}(t)|}{\text{frequency}(t)}
\end{equation}

where \(H(\text{contexts}(t))\) is the entropy of contextual usage
patterns, measuring dispersion across different research contexts.
\end{block}
\end{block}

\begin{block}{S1.3 Network Construction and Analysis}
\protect\phantomsection\label{s1.3-network-construction-and-analysis}
\begin{block}{S1.3.1 Edge Weight Calculation}
\protect\phantomsection\label{s1.3.1-edge-weight-calculation}
Network edges are weighted using multiple co-occurrence measures:

\begin{equation}\label{eq:edge_weight_computation}
w(u,v) = \frac{1}{3} \left[ \frac{\text{co-occurrence}(u,v)}{\max(\text{freq}(u), \text{freq}(v))} + \text{Jaccard}(u,v) + \text{cosine}(\vec{u}, \vec{v}) \right]
\end{equation}

where co-occurrence is measured within sliding windows, Jaccard
similarity captures set overlap, and cosine similarity measures semantic
relatedness.
\end{block}

\begin{block}{S1.3.2 Community Detection Algorithms}
\protect\phantomsection\label{s1.3.2-community-detection-algorithms}
We implement multiple community detection approaches:

\textbf{Modularity Optimization}: \begin{equation}\label{eq:modularity}
Q = \frac{1}{2m} \sum_{ij} \left[ A_{ij} - \frac{k_i k_j}{2m} \right] \delta(c_i, c_j)
\end{equation}

\textbf{Domain-Aware Clustering}: Communities are constrained to respect
Ento-Linguistic domain boundaries while allowing cross-domain bridging
terms.
\end{block}

\begin{block}{S1.3.3 Network Validation Metrics}
\protect\phantomsection\label{s1.3.3-network-validation-metrics}
Network quality is assessed using:

\begin{equation}\label{eq:network_validation}
V(G) = \alpha \cdot \text{modularity}(G) + \beta \cdot \text{conductance}(G) + \gamma \cdot \text{domain_purity}(G)
\end{equation}

where domain purity measures the extent to which communities correspond
to Ento-Linguistic domains.
\end{block}
\end{block}

\begin{block}{S1.4 Framing Analysis Implementation}
\protect\phantomsection\label{s1.4-framing-analysis-implementation}
\begin{block}{S1.4.1 Anthropomorphic Framing Detection}
\protect\phantomsection\label{s1.4.1-anthropomorphic-framing-detection}
Anthropomorphic language is detected through:

\textbf{Lexical Indicators}: Terms suggesting human-like agency or
intentionality \textbf{Syntactic Patterns}: Sentence structures implying
human-like behavior \textbf{Semantic Fields}: Clusters of terms drawing
from human social domains

\textbf{Detection Algorithm}:
\begin{equation}\label{eq:anthropomorphic_score}
A_{\text{anthro}}(t) = \sum_{f \in F_{\text{human}}} \text{similarity}(t, f) \cdot w_f
\end{equation}

where \(F_{\text{human}}\) contains human social concept features and
\(w_f\) are calibrated weights.
\end{block}

\begin{block}{S1.4.2 Hierarchical Framing Analysis}
\protect\phantomsection\label{s1.4.2-hierarchical-framing-analysis}
Hierarchical structures are identified by:

\textbf{Term Relationship Patterns}: Chains of subordination (superior →
subordinate) \textbf{Power Dynamic Indicators}: Terms implying
authority, control, or submission \textbf{Organizational Metaphors}:
Language drawing from human institutional structures
\end{block}
\end{block}

\begin{block}{S1.5 Validation Framework Implementation}
\protect\phantomsection\label{s1.5-validation-framework-implementation}
\begin{block}{S1.5.1 Computational Validation Procedures}
\protect\phantomsection\label{s1.5.1-computational-validation-procedures}
\textbf{Terminology Extraction Validation}: - \textbf{Precision}: Manual
verification of extracted terms against expert-curated lists -
\textbf{Recall}: Coverage assessment against comprehensive domain
glossaries - \textbf{Domain Accuracy}: Correct classification into
Ento-Linguistic domains

\textbf{Network Validation}: - \textbf{Structural Validity}: Comparison
against null models - \textbf{Domain Correspondence}: Alignment with
theoretical domain boundaries - \textbf{Stability Analysis}: Consistency
across subsampling procedures
\end{block}

\begin{block}{S1.5.2 Theoretical Validation Methods}
\protect\phantomsection\label{s1.5.2-theoretical-validation-methods}
\textbf{Inter-coder Agreement}: Multiple researchers code ambiguous
passages to assess consistency.

\textbf{Theoretical Saturation}: Iterative analysis until theoretical
categories are fully developed.

\textbf{Member Checking}: Expert review of interpretations and
categorizations.
\end{block}
\end{block}

\begin{block}{S1.6 Implementation Architecture}
\protect\phantomsection\label{s1.6-implementation-architecture}
\begin{block}{S1.6.1 Modular Software Design}
\protect\phantomsection\label{s1.6.1-modular-software-design}
The implementation follows a modular architecture:

\begin{verbatim}
entolinguistic/
├── text_processing/     # Text normalization and tokenization
├── terminology/         # Term extraction and classification
├── networks/           # Graph construction and analysis
├── framing/            # Framing analysis algorithms
├── validation/         # Validation and quality assurance
└── visualization/      # Result visualization
\end{verbatim}
\end{block}

\begin{block}{S1.6.2 Data Structures and Formats}
\protect\phantomsection\label{s1.6.2-data-structures-and-formats}
\textbf{Terminology Database}:

\begin{Shaded}
\begin{Highlighting}[]
\AttributeTok{@dataclass}
\KeywordTok{class}\NormalTok{ TerminologyEntry:}
\NormalTok{    term: }\BuiltInTok{str}
\NormalTok{    domains: List[}\BuiltInTok{str}\NormalTok{]}
\NormalTok{    contexts: List[}\BuiltInTok{str}\NormalTok{]}
\NormalTok{    frequencies: Dict[}\BuiltInTok{str}\NormalTok{, }\BuiltInTok{int}\NormalTok{]}
\NormalTok{    ambiguities: List[}\BuiltInTok{str}\NormalTok{]}
\NormalTok{    framings: List[}\BuiltInTok{str}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\textbf{Network Representation}:

\begin{Shaded}
\begin{Highlighting}[]
\AttributeTok{@dataclass}
\KeywordTok{class}\NormalTok{ TerminologyNetwork:}
\NormalTok{    nodes: Dict[}\BuiltInTok{str}\NormalTok{, TerminologyEntry]}
\NormalTok{    edges: Dict[Tuple[}\BuiltInTok{str}\NormalTok{, }\BuiltInTok{str}\NormalTok{], }\BuiltInTok{float}\NormalTok{]}
\NormalTok{    communities: Dict[}\BuiltInTok{str}\NormalTok{, List[}\BuiltInTok{str}\NormalTok{]]}
\NormalTok{    domain\_mappings: Dict[}\BuiltInTok{str}\NormalTok{, }\BuiltInTok{str}\NormalTok{]}
\end{Highlighting}
\end{Shaded}
\end{block}

\begin{block}{S1.6.3 Performance Optimization}
\protect\phantomsection\label{s1.6.3-performance-optimization}
\textbf{Scalability Considerations}: - Streaming processing for large
corpora - Incremental network updates - Parallel processing for
independent analyses - Memory-efficient data structures for large
networks

\textbf{Computational Complexity}:
\begin{equation}\label{eq:method_complexity}
C(n,m,d) = O(n \log n + m \cdot d + e \cdot \log e)
\end{equation}

where \(n\) is corpus size, \(m\) is extracted terms, \(d\) is domains,
and \(e\) is network edges.
\end{block}
\end{block}

\begin{block}{S1.7 Parameter Calibration and Sensitivity}
\protect\phantomsection\label{s1.7-parameter-calibration-and-sensitivity}
\begin{block}{S1.7.1 Algorithm Parameters}
\protect\phantomsection\label{s1.7.1-algorithm-parameters}
Critical parameters and their calibration:

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Parameter} & \textbf{Default} & \textbf{Range} & \textbf{Impact} & \textbf{Calibration Method} \\
\hline
Window Size & 50 & [20, 100] & High & Cross-validation \\
Similarity Threshold & 0.3 & [0.1, 0.8] & High & Domain expert review \\
Minimum Frequency & 5 & [1, 50] & Medium & Statistical significance \\
Ambiguity Threshold & 0.7 & [0.5, 0.9] & Medium & Manual validation \\
\hline
\end{tabular}
\caption{Algorithm parameter calibration and sensitivity analysis}
\label{tab:parameter_calibration}
\end{table}
\end{block}

\begin{block}{S1.7.2 Sensitivity Analysis Results}
\protect\phantomsection\label{s1.7.2-sensitivity-analysis-results}
Parameter sensitivity testing revealed:

\textbf{Window Size}: Optimal at 50 words; smaller windows miss
long-range relationships, larger windows introduce noise.

\textbf{Similarity Threshold}: 0.3 provides balance between precision
and recall; lower values increase false positives, higher values miss
subtle relationships.

\textbf{Frequency Threshold}: 5 occurrences ensures statistical
reliability while maintaining coverage.
\end{block}
\end{block}

\begin{block}{S1.8 Quality Assurance and Reproducibility}
\protect\phantomsection\label{s1.8-quality-assurance-and-reproducibility}
\begin{block}{S1.8.1 Automated Quality Checks}
\protect\phantomsection\label{s1.8.1-automated-quality-checks}
\textbf{Data Quality Validation}: - Text encoding verification - Corpus
completeness checks - Metadata consistency validation

\textbf{Algorithmic Validation}: - Deterministic output verification -
Cross-platform compatibility testing - Performance regression monitoring
\end{block}

\begin{block}{S1.8.2 Reproducibility Framework}
\protect\phantomsection\label{s1.8.2-reproducibility-framework}
\textbf{Version Control}: All code, data, and parameters are version
controlled with DOI minting for long-term access.

\textbf{Containerization}: Analysis environments are containerized for
exact reproducibility.

\textbf{Documentation}: Comprehensive documentation of all processing
steps, parameters, and decisions.
\end{block}
\end{block}

\begin{block}{S1.9 Extensions and Future Methods}
\protect\phantomsection\label{s1.9-extensions-and-future-methods}
\begin{block}{S1.9.1 Advanced Semantic Analysis}
\protect\phantomsection\label{s1.9.1-advanced-semantic-analysis}
Future extensions include:

\textbf{Transformer-based Embeddings}: Using contextual language models
for more sophisticated semantic analysis.

\textbf{Multilingual Extensions}: Cross-language terminology mapping and
comparison.

\textbf{Temporal Analysis}: Tracking terminological evolution over time
using diachronic methods.
\end{block}

\begin{block}{S1.9.2 Integration with External Resources}
\protect\phantomsection\label{s1.9.2-integration-with-external-resources}
\textbf{Ontology Integration}: Mapping to existing biological ontologies
and terminologies.

\textbf{Citation Network Analysis}: Integrating citation patterns with
terminology usage.

\textbf{Author Network Analysis}: Examining how terminology use
correlates with research communities.

This detailed methodological framework ensures rigorous, reproducible
Ento-Linguistic analysis while maintaining flexibility for
methodological refinement and extension.
\end{block}
\end{block}
