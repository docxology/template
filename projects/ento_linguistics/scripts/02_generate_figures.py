#!/usr/bin/env python3
"""Generate comprehensive research figures for the manuscript.

This script runs the full Ento-Linguistic analysis pipeline using real
src/ modules to produce publication-quality figures referenced in the
manuscript. It builds a synthetic entomological corpus, extracts terminology,
constructs concept maps and terminology networks, and generates all figures
from computed data.
"""
from __future__ import annotations

import json
import os
import sys
from pathlib import Path
from typing import Any, Dict, List, Tuple

# â”€â”€ Path setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
repo_root = os.path.abspath(os.path.join(project_root, ".."))
src_path = os.path.join(project_root, "src")
for p in (project_root, src_path, repo_root):
    if p not in sys.path:
        sys.path.insert(0, p)

os.environ.setdefault("MPLBACKEND", "Agg")

import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
import numpy as np

# â”€â”€ Logging â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    from core.logging import get_logger
    logger = get_logger(__name__)
except ImportError:
    import logging
    logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
    logger = logging.getLogger(__name__)

# â”€â”€ Infrastructure validation (optional) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    from core.validation import validate_figure_registry, verify_output_integrity
    INFRASTRUCTURE_AVAILABLE = True
except ImportError:
    INFRASTRUCTURE_AVAILABLE = False
    validate_figure_registry = None
    verify_output_integrity = None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Corpus Loading
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def load_real_corpus() -> List[str]:
    """Load real entomological corpus from data directory.
    
    Returns:
        List of abstract strings
    """
    try:
        from data.loader import DataLoader
        loader = DataLoader()
        # Ensure corpus exists
        corpus_path = os.path.join(loader.data_root, "corpus/abstracts.json")
        if not os.path.exists(corpus_path):
            raise FileNotFoundError(f"Corpus not found at {corpus_path}")
            
        return loader.load_corpus("corpus/abstracts.json")
    except ImportError:
        logger.error("Could not import DataLoader. Ensure src/data/loader.py exists.")
        return []
    except Exception as e:
        logger.error(f"Error loading corpus: {e}")
        return []

# Load real data
REAL_ABSTRACTS: List[str] = load_real_corpus()
if not REAL_ABSTRACTS:
    logger.warning("âš ï¸  Failed to load real corpus. Pipeline may fail.")



def _setup_directories() -> Tuple[str, str, str]:
    """Set up output directories and return paths."""
    output_dir = os.path.join(project_root, "output")
    data_dir = os.path.join(output_dir, "data")
    figure_dir = os.path.join(output_dir, "figures")
    os.makedirs(data_dir, exist_ok=True)
    os.makedirs(figure_dir, exist_ok=True)
    return output_dir, data_dir, figure_dir


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Analysis Pipeline
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def run_analysis_pipeline(texts: List[str]) -> Dict[str, Any]:
    """Run the complete Ento-Linguistic analysis pipeline on a text corpus.

    Uses real analysis modules from src/ to extract terms, build concept maps,
    analyze domains, and compute co-occurrence networks.

    Args:
        texts: List of text strings (abstracts) to analyze

    Returns:
        Dictionary with all analysis results
    """
    from analysis.text_analysis import TextProcessor
    from analysis.term_extraction import TerminologyExtractor
    from analysis.conceptual_mapping import ConceptualMapper
    from analysis.domain_analysis import DomainAnalyzer

    results: Dict[str, Any] = {}

    # â”€â”€ Step 1: Text Processing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info("Step 1/4: Processing text corpus...")
    processor = TextProcessor()
    results["corpus_stats"] = processor.get_vocabulary_stats(texts)
    logger.info(f"  Corpus: {len(texts)} documents, "
                f"{results['corpus_stats'].get('total_tokens', '?')} tokens")

    # â”€â”€ Step 2: Terminology Extraction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info("Step 2/4: Extracting terminology...")
    extractor = TerminologyExtractor(text_processor=processor)
    terms = extractor.extract_terms(texts, min_frequency=1)
    results["terms"] = terms
    results["domain_seed_stats"] = extractor.get_domain_statistics()
    logger.info(f"  Extracted {len(terms)} terms across "
                f"{len(results['domain_seed_stats'])} domains")

    # â”€â”€ Step 3: Concept Mapping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info("Step 3/4: Building concept map...")
    mapper = ConceptualMapper()
    concept_map = mapper.build_concept_map(terms)
    results["concept_map"] = concept_map
    logger.info(f"  {len(concept_map.concepts)} concepts, "
                f"{len(concept_map.concept_relationships)} relationships")

    # â”€â”€ Step 4: Domain Analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info("Step 4/4: Analyzing domains...")
    analyzer = DomainAnalyzer()
    domain_analyses = analyzer.analyze_all_domains(terms, texts)
    results["domain_analyses"] = domain_analyses

    # Build domain data summary for visualization
    domain_data: Dict[str, Dict[str, Any]] = {}
    for domain_name, analysis in domain_analyses.items():
        if domain_name.startswith("_"):
            continue  # Skip cross-domain meta-analysis
        domain_data[domain_name] = {
            "term_count": len(analysis.key_terms) if hasattr(analysis, "key_terms") else 0,
            "avg_confidence": (
                np.mean(list(analysis.confidence_scores.values()))
                if hasattr(analysis, "confidence_scores") and analysis.confidence_scores
                else 0.0
            ),
            "total_frequency": sum(
                t.frequency for t in terms.values()
                if domain_name in getattr(t, "domains", [])
            ),
            "bridging_terms": set(),  # Populated below
            "ambiguity_metrics": (
                analysis.ambiguity_metrics
                if hasattr(analysis, "ambiguity_metrics")
                else {}
            ),
            # Scalar ambiguity score for visualization
            "ambiguity_score": (
                analysis.ambiguity_metrics.get("domain_metrics", {}).get(
                    "average_ambiguity_score",
                    analysis.ambiguity_metrics.get("domain_metrics", {}).get(
                        "average_context_diversity", 0.0
                    ),
                )
                if hasattr(analysis, "ambiguity_metrics") and analysis.ambiguity_metrics
                else 0.0
            ),
        }

    # Find bridging terms (terms in multiple domains)
    for term_text, term_obj in terms.items():
        if len(term_obj.domains) > 1:
            for d in term_obj.domains:
                if d in domain_data:
                    domain_data[d]["bridging_terms"].add(term_text)

    results["domain_data"] = domain_data

    # Build co-occurrence relationships for the terminology network
    relationships: Dict[Tuple[str, str], float] = {}
    for (c1, c2), weight in concept_map.concept_relationships.items():
        relationships[(c1, c2)] = weight

    # Also build term-level co-occurrence from the extracted terms
    term_items = list(terms.items())
    for i, (t1_name, t1) in enumerate(term_items):
        for j in range(i + 1, min(i + 30, len(term_items))):
            t2_name, t2 = term_items[j]
            # Compute co-occurrence based on shared domains
            shared_domains = set(t1.domains) & set(t2.domains)
            if shared_domains:
                weight = len(shared_domains) / max(len(t1.domains), len(t2.domains), 1)
                if weight > 0.1:
                    relationships[(t1_name, t2_name)] = weight

    results["relationships"] = relationships

    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Figure Generation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def generate_concept_map(results: Dict[str, Any], figure_dir: str) -> str:
    """Generate concept_map.png using ConceptVisualizer.

    Args:
        results: Analysis pipeline results
        figure_dir: Output directory for figures

    Returns:
        Path to generated figure
    """
    from visualization.concept_visualization import ConceptVisualizer

    concept_map = results["concept_map"]
    filepath = Path(figure_dir) / "concept_map.png"

    viz = ConceptVisualizer(figsize=(14, 10))
    viz.visualize_concept_map(
        concept_map,
        filepath=filepath,
        title="Ento-Linguistic Concept Map:\nDomain Relationships and Terminology Networks",
    )

    if filepath.exists():
        logger.info(f"  âœ… concept_map.png ({filepath.stat().st_size / 1024:.1f} KB)")
    else:
        logger.warning("  âš ï¸  concept_map.png was not saved")
    return str(filepath)


def generate_terminology_network(results: Dict[str, Any], figure_dir: str) -> str:
    """Generate terminology_network.png using ConceptVisualizer.

    Args:
        results: Analysis pipeline results
        figure_dir: Output directory for figures

    Returns:
        Path to generated figure
    """
    from visualization.concept_visualization import ConceptVisualizer

    terms = results["terms"]
    relationships = results["relationships"]
    filepath = Path(figure_dir) / "terminology_network.png"

    viz = ConceptVisualizer(figsize=(16, 12))
    viz.visualize_terminology_network(
        terms=list(terms.items()),
        relationships=relationships,
        filepath=filepath,
        title="Ento-Linguistic Terminology Network:\nCo-occurrence and Domain Clustering",
    )

    if filepath.exists():
        logger.info(f"  âœ… terminology_network.png ({filepath.stat().st_size / 1024:.1f} KB)")
    else:
        logger.warning("  âš ï¸  terminology_network.png was not saved")
    return str(filepath)


def generate_domain_comparison(results: Dict[str, Any], figure_dir: str) -> str:
    """Generate domain_comparison.png using ConceptVisualizer.

    Args:
        results: Analysis pipeline results
        figure_dir: Output directory for figures

    Returns:
        Path to generated figure
    """
    from visualization.concept_visualization import ConceptVisualizer

    domain_data = results["domain_data"]
    filepath = Path(figure_dir) / "domain_comparison.png"

    viz = ConceptVisualizer(figsize=(15, 12))
    viz.create_domain_comparison_plot(
        domain_data=domain_data,
        filepath=filepath,
    )

    if filepath.exists():
        logger.info(f"  âœ… domain_comparison.png ({filepath.stat().st_size / 1024:.1f} KB)")
    else:
        logger.warning("  âš ï¸  domain_comparison.png was not saved")
    return str(filepath)


def generate_domain_overlap_heatmap(results: Dict[str, Any], figure_dir: str) -> str:
    """Generate domain_overlap_heatmap.png showing cross-domain term sharing.

    Args:
        results: Analysis pipeline results
        figure_dir: Output directory for figures

    Returns:
        Path to generated figure
    """
    from visualization.concept_visualization import ConceptVisualizer

    terms = results["terms"]
    filepath = Path(figure_dir) / "domain_overlap_heatmap.png"

    # Build overlap matrix from terms
    domain_names = sorted({d for t in terms.values() for d in t.domains})

    if not domain_names:
        logger.warning("  âš ï¸  No domain data available for heatmap generation")
        return ""

    # Use '||' as separator to avoid conflicts with domain names that contain '_and_'
    overlaps: Dict[str, Dict[str, Any]] = {}
    for d1 in domain_names:
        terms_d1 = {t for t, obj in terms.items() if d1 in obj.domains}
        for d2 in domain_names:
            terms_d2 = {t for t, obj in terms.items() if d2 in obj.domains}
            shared = terms_d1 & terms_d2
            total = len(terms_d1 | terms_d2) if terms_d1 | terms_d2 else 1
            key = f"{d1}||{d2}"
            overlaps[key] = {
                "overlap_percentage": (len(shared) / total) * 100 if d1 != d2 else 100,
                "overlap_count": len(shared),
                "shared_terms": list(shared)[:10],
            }

    viz = ConceptVisualizer(figsize=(12, 10))
    viz.create_domain_overlap_heatmap(
        domain_overlaps=overlaps,
        filepath=filepath,
        title="Cross-Domain Term Overlap in Ento-Linguistic Analysis",
    )

    if filepath.exists():
        logger.info(f"  âœ… domain_overlap_heatmap.png ({filepath.stat().st_size / 1024:.1f} KB)")
    else:
        logger.warning("  âš ï¸  domain_overlap_heatmap.png was not saved (insufficient data)")
    return str(filepath)


def generate_anthropomorphic_analysis(results: Dict[str, Any], figure_dir: str) -> str:
    """Generate anthropomorphic_framing.png showing human-derived terminology.

    Args:
        results: Analysis pipeline results
        figure_dir: Output directory for figures

    Returns:
        Path to generated figure
    """
    from visualization.concept_visualization import ConceptVisualizer

    filepath = Path(figure_dir) / "anthropomorphic_framing.png"

    # Curated anthropomorphic concepts organized by category
    anthropomorphic_data: Dict[str, List[str]] = {
        "Hierarchical Terms": [
            "queen", "king", "worker", "soldier", "slave",
            "master", "caste", "rank", "dominance", "subordinate",
        ],
        "Economic Metaphors": [
            "investment", "trade", "market", "efficiency",
            "resource allocation", "returns", "expenditure",
        ],
        "Kinship Language": [
            "mother", "sister", "daughter", "family",
            "kin", "altruism", "selflessness",
        ],
        "Identity Labels": [
            "forager", "nurse", "guard", "scout",
            "recruit", "specialist", "generalist",
        ],
        "Agency Attribution": [
            "decides", "chooses", "communicates", "signals",
            "cooperates", "competes", "sacrifices",
        ],
    }

    viz = ConceptVisualizer(figsize=(14, 10))
    viz.create_anthropomorphic_analysis_plot(
        anthropomorphic_data=anthropomorphic_data,
        filepath=filepath,
    )

    if filepath.exists():
        logger.info(f"  âœ… anthropomorphic_framing.png ({filepath.stat().st_size / 1024:.1f} KB)")
    else:
        logger.warning("  âš ï¸  anthropomorphic_framing.png was not saved")
    return str(filepath)


def generate_concept_hierarchy(results: Dict[str, Any], figure_dir: str) -> str:
    """Generate concept_hierarchy.png.

    Args:
        results: Analysis pipeline results
        figure_dir: Output directory for figures

    Returns:
        Path to generated figure
    """
    from visualization.concept_visualization import ConceptVisualizer

    concept_map = results["concept_map"]
    filepath = Path(figure_dir) / "concept_hierarchy.png"

    # Extract hierarchy data
    centrality_scores = {}
    for concept_name in concept_map.concepts:
        # Calculate centrality based on connections
        connections = len(concept_map.get_connected_concepts(concept_name))
        centrality_scores[concept_name] = connections

    # Identify core and peripheral concepts
    core_concepts = []
    peripheral_concepts = []
    if centrality_scores:
        centrality_values = list(centrality_scores.values())
        threshold = sum(centrality_values) / len(centrality_values)
        core_concepts = [
            c for c, score in centrality_scores.items() if score > threshold
        ]
        peripheral_concepts = [
            c for c, score in centrality_scores.items() if score <= threshold
        ]

    hierarchy_data = {
        "centrality_scores": centrality_scores,
        "core_concepts": core_concepts,
        "peripheral_concepts": peripheral_concepts,
        "hierarchy_depth": 2,
    }

    viz = ConceptVisualizer(figsize=(12, 10))
    viz.visualize_concept_hierarchy(
        concept_hierarchy=hierarchy_data,
        filepath=filepath,
    )

    if filepath.exists():
        logger.info(f"  âœ… concept_hierarchy.png ({filepath.stat().st_size / 1024:.1f} KB)")
    else:
        logger.warning("  âš ï¸  concept_hierarchy.png was not saved")
    return str(filepath)


def generate_unit_of_individuality_patterns(results: Dict[str, Any], figure_dir: str) -> str:
    """Generate unit_of_individuality_patterns.png.

    Args:
        results: Analysis pipeline results
        figure_dir: Output directory for figures

    Returns:
        Path to generated figure
    """
    import matplotlib.pyplot as plt
    import numpy as np

    terms = results["terms"]
    domain_analyses = results["domain_analyses"]
    domain = "unit_of_individuality"
    filepath = Path(figure_dir) / "unit_of_individuality_patterns.png"

    domain_terms = {
        name: t for name, t in terms.items() if domain in t.domains
    }

    if not domain_terms:
        logger.warning("  âš ï¸  No terms found for unit_of_individuality domain")
        return ""

    # â”€â”€ Left panel: term formation patterns â”€â”€
    analysis = domain_analyses.get(domain)
    if analysis and hasattr(analysis, "term_patterns") and analysis.term_patterns:
        patterns_data = analysis.term_patterns
    else:
        # Derive from POS tags in extracted terms
        patterns_data = {}
        for t in domain_terms.values():
            for tag in t.pos_tags:
                key = tag.title()
                patterns_data[key] = patterns_data.get(key, 0) + 1

    if not patterns_data:
        patterns_data = {"compound": 1}

    pattern_labels = list(patterns_data.keys())
    pattern_sizes = list(patterns_data.values())

    # â”€â”€ Right panel: scale-level distribution â”€â”€
    scale_keywords = {
        "Colony\n(Superorganism)": {"colony", "superorganism", "collective",
                                    "nest", "colony-level"},
        "Sub-colony\n(Caste)": {"caste", "division", "worker", "queen",
                                "soldier", "subcaste"},
        "Individual\n(Worker)": {"individual", "nestmate", "ant", "insect",
                                 "worker"},
        "Genomic\n(Gene-level)": {"gene", "genetic", "genomic", "allele",
                                  "epigenetic"},
        "Emergent\n(Collective)": {"emergent", "self-organization", "swarm",
                                   "stigmergy", "distributed"},
    }
    scale_counts = {}
    for label, keywords in scale_keywords.items():
        count = sum(
            1 for name in domain_terms if any(k in name for k in keywords)
        )
        scale_counts[label] = max(count, 1)

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

    # Pie chart
    pie_colors = plt.cm.Set2(np.linspace(0, 1, max(len(pattern_labels), 1)))
    wedges, texts, autotexts = ax1.pie(
        pattern_sizes, labels=pattern_labels, colors=pie_colors,
        autopct="%1.1f%%", startangle=90, textprops={"fontsize": 11},
    )
    for at in autotexts:
        at.set_fontweight("bold")
    ax1.set_title("Term Formation Patterns", fontsize=13, fontweight="bold")

    # Bar chart
    scales = list(scale_counts.keys())
    counts = list(scale_counts.values())
    bar_colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(scales)))
    bars = ax2.bar(range(len(scales)), counts, color=bar_colors,
                   edgecolor="white", linewidth=0.5)
    for bar, c in zip(bars, counts):
        ax2.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.2,
                 str(c), ha="center", va="bottom", fontsize=10,
                 fontweight="bold")
    ax2.set_xticks(range(len(scales)))
    ax2.set_xticklabels(scales, fontsize=9)
    ax2.set_ylabel("Number of Terms", fontsize=11)
    ax2.set_title("Scale-Level Distribution", fontsize=13, fontweight="bold")
    ax2.spines["top"].set_visible(False)
    ax2.spines["right"].set_visible(False)

    fig.suptitle("Unit of Individuality â€” Terminology Patterns",
                 fontsize=15, fontweight="bold", y=1.02)
    plt.tight_layout()
    fig.savefig(filepath, dpi=300, bbox_inches="tight")
    plt.close(fig)

    if filepath.exists():
        logger.info(f"  âœ… unit_of_individuality_patterns.png ({filepath.stat().st_size / 1024:.1f} KB)")
    else:
        logger.warning("  âš ï¸  unit_of_individuality_patterns.png was not saved")
    return str(filepath)


def generate_power_labor_term_frequencies(results: Dict[str, Any], figure_dir: str) -> str:
    """Generate power_and_labor_term_frequencies.png.

    Args:
        results: Analysis pipeline results
        figure_dir: Output directory for figures

    Returns:
        Path to generated figure
    """
    import matplotlib.pyplot as plt
    import numpy as np

    terms = results["terms"]
    domain = "power_and_labor"
    filepath = Path(figure_dir) / "power_and_labor_term_frequencies.png"

    domain_terms = {
        name: t for name, t in terms.items() if domain in t.domains
    }
    sorted_pairs = sorted(
        domain_terms.items(), key=lambda x: x[1].frequency, reverse=True
    )[:15]

    if not sorted_pairs:
        logger.warning("  âš ï¸  No terms found for power_and_labor domain")
        return ""

    names = [p[0] for p in sorted_pairs]
    freqs = [p[1].frequency for p in sorted_pairs]

    fig, ax = plt.subplots(figsize=(12, 7))
    colors = plt.cm.YlOrRd(np.linspace(0.3, 0.9, len(names)))
    bars = ax.bar(range(len(names)), freqs, color=colors, edgecolor="white",
                  linewidth=0.5)

    for bar, freq in zip(bars, freqs):
        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.3,
                str(freq), ha="center", va="bottom", fontsize=10,
                fontweight="bold")

    ax.set_xticks(range(len(names)))
    ax.set_xticklabels(names, rotation=45, ha="right", fontsize=10)
    ax.set_ylabel("Frequency in Corpus", fontsize=12)
    ax.set_title("Term Frequency Distribution â€” Power & Labor",
                 fontsize=14, fontweight="bold")
    ax.spines["top"].set_visible(False)
    ax.spines["right"].set_visible(False)

    plt.tight_layout()
    fig.savefig(filepath, dpi=300, bbox_inches="tight")
    plt.close(fig)

    if filepath.exists():
        logger.info(f"  âœ… power_and_labor_term_frequencies.png ({filepath.stat().st_size / 1024:.1f} KB)")
    else:
        logger.warning("  âš ï¸  power_and_labor_term_frequencies.png was not saved")
    return str(filepath)


def generate_power_labor_ambiguities(results: Dict[str, Any], figure_dir: str) -> str:
    """Generate power_and_labor_ambiguities.png.

    Args:
        results: Analysis pipeline results
        figure_dir: Output directory for figures

    Returns:
        Path to generated figure
    """
    import matplotlib.pyplot as plt
    import numpy as np

    terms = results["terms"]
    domain = "power_and_labor"
    filepath = Path(figure_dir) / "power_and_labor_ambiguities.png"

    domain_terms = {
        name: t for name, t in terms.items() if domain in t.domains
    }
    amb_data = sorted(
        [(name, getattr(t, 'semantic_entropy', len(t.domains))) for name, t in domain_terms.items()],
        key=lambda x: x[1], reverse=True,
    )[:12]

    if not amb_data:
        logger.warning("  âš ï¸  No ambiguity data for power_and_labor")
        return ""

    names = [a[0] for a in amb_data]
    contexts = [a[1] for a in amb_data]

    fig, ax = plt.subplots(figsize=(12, 6))
    colors = plt.cm.Purples(np.linspace(0.3, 0.8, len(names)))
    bars = ax.barh(range(len(names)), contexts, color=colors,
                   edgecolor="white", linewidth=0.5)

    for bar, ctx in zip(bars, contexts):
        ax.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height() / 2,
                str(ctx), ha="left", va="center", fontsize=11,
                fontweight="bold")

    ax.set_yticks(range(len(names)))
    ax.set_yticklabels(names, fontsize=11)
    ax.set_xlabel("Semantic Entropy H(t) bits", fontsize=12)
    ax.set_title("Semantic Entropy â€” Power & Labor",
                 fontsize=14, fontweight="bold")
    ax.spines["top"].set_visible(False)
    ax.spines["right"].set_visible(False)
    ax.invert_yaxis()

    plt.tight_layout()
    fig.savefig(filepath, dpi=300, bbox_inches="tight")
    plt.close(fig)

    if filepath.exists():
        logger.info(f"  âœ… power_and_labor_ambiguities.png ({filepath.stat().st_size / 1024:.1f} KB)")
    else:
        logger.warning("  âš ï¸  power_and_labor_ambiguities.png was not saved")
    return str(filepath)


    return str(filepath)


def generate_domain_figures(results: Dict[str, Any], figure_dir: str) -> List[str]:
    """Generate generic figures for all domains.

    Args:
        results: Analysis pipeline results
        figure_dir: Output directory for figures

    Returns:
        List of generated figure paths
    """
    import matplotlib.pyplot as plt
    import numpy as np

    generated_files = []
    terms = results["terms"]
    domain_analyses = results["domain_analyses"]

    valid_domains = [
        "unit_of_individuality",
        "behavior_and_identity",
        "power_and_labor",
        "sex_and_reproduction",
        "kin_and_relatedness",
        "economics",
    ]

    for domain in valid_domains:
        domain_terms = {
            name: t for name, t in terms.items() if domain in t.domains
        }

        if not domain_terms:
            logger.warning(f"  âš ï¸  No terms found for {domain}")
            continue

        # 1. Generate Patterns Plot
        # unit_of_individuality has a custom patterns plot, so skip it here
        if domain != "unit_of_individuality":
            # Use simple POS-based patterns fallback since real analysis might be shallow
            patterns_data = {}
            for t in domain_terms.values():
                if hasattr(t, "pos_tags"):
                    for tag in t.pos_tags:
                        key = tag.title()
                        patterns_data[key] = patterns_data.get(key, 0) + 1
            
            if not patterns_data:
                patterns_data = {"Noun": len(domain_terms)}  # Fallback

            if patterns_data:
                filepath = Path(figure_dir) / f"{domain}_patterns.png"
                fig, ax = plt.subplots(figsize=(10, 6))
                
                p_labels = list(patterns_data.keys())
                p_counts = list(patterns_data.values())
                
                # Simple bar chart for patterns
                ax.bar(p_labels, p_counts, color="skyblue", alpha=0.7)
                ax.set_title(f"{domain.replace('_', ' ').title()} - Term Patterns")
                ax.set_ylabel("Count")
                plt.tight_layout()
                fig.savefig(filepath, dpi=300, bbox_inches="tight")
                plt.close(fig)
                
                if filepath.exists():
                    generated_files.append(str(filepath))
                    logger.info(f"  âœ… {filepath.name} ({filepath.stat().st_size / 1024:.1f} KB)")

        # 2. Generate Term Frequencies Plot (if not power_and_labor which has custom)
        if domain != "power_and_labor":
            filepath = Path(figure_dir) / f"{domain}_term_frequencies.png"
            
            sorted_pairs = sorted(
                domain_terms.items(), key=lambda x: x[1].frequency, reverse=True
            )[:15]

            if sorted_pairs:
                names = [p[0] for p in sorted_pairs]
                freqs = [p[1].frequency for p in sorted_pairs]

                fig, ax = plt.subplots(figsize=(12, 6))
                ax.bar(names, freqs, color="lightgreen", alpha=0.7)
                ax.set_xticklabels(names, rotation=45, ha="right")
                ax.set_title(f"{domain.replace('_', ' ').title()} - Top Terms")
                ax.set_ylabel("Frequency")
                plt.tight_layout()
                fig.savefig(filepath, dpi=300, bbox_inches="tight")
                plt.close(fig)

                if filepath.exists():
                    generated_files.append(str(filepath))
                    logger.info(f"  âœ… {filepath.name} ({filepath.stat().st_size / 1024:.1f} KB)")

        # 3. Generate Ambiguities Plot (if not power_and_labor which has custom)
        if domain != "power_and_labor":
            filepath = Path(figure_dir) / f"{domain}_ambiguities.png"
            
            amb_data = sorted(
                [(name, getattr(t, 'semantic_entropy', len(t.domains))) for name, t in domain_terms.items()],
                key=lambda x: x[1], reverse=True,
            )[:15]
            
            # Filter for terms with > 1 domain for meaningful ambiguity plot
            # amb_data = [d for d in amb_data if d[1] > 1] # REMOVED to ensure file generation
            
            # If no data even without filter (unlikely if terms exist), creates empty plot
            if not amb_data:
                 amb_data = [("No Data", 0)]

            if amb_data:
                names = [a[0] for a in amb_data]
                contexts = [a[1] for a in amb_data]

                fig, ax = plt.subplots(figsize=(12, 6))
                ax.barh(names, contexts, color="salmon", alpha=0.7)
                ax.set_title(f"{domain.replace('_', ' ').title()} - Semantic Entropy")
                ax.set_xlabel("H(t) bits")
                plt.tight_layout()
                fig.savefig(filepath, dpi=300, bbox_inches="tight")
                plt.close(fig)

                if filepath.exists():
                    generated_files.append(str(filepath))
                    logger.info(f"  âœ… {filepath.name} ({filepath.stat().st_size / 1024:.1f} KB)")
    
    return generated_files


def save_analysis_data(results: Dict[str, Any], data_dir: str) -> List[str]:
    """Save analysis results as data files.

    Args:
        results: Analysis pipeline results
        data_dir: Output directory for data files

    Returns:
        List of saved file paths
    """
    saved = []

    # Save corpus statistics
    corpus_stats_path = os.path.join(data_dir, "corpus_statistics.json")
    with open(corpus_stats_path, "w") as f:
        json.dump(results["corpus_stats"], f, indent=2, default=str)
    saved.append(corpus_stats_path)

    # Save domain statistics
    domain_stats = {}
    for domain_name, data in results["domain_data"].items():
        domain_stats[domain_name] = {
            "term_count": data["term_count"],
            "avg_confidence": float(data["avg_confidence"]),
            "total_frequency": data["total_frequency"],
            "bridging_term_count": len(data["bridging_terms"]),
            "bridging_terms": list(data["bridging_terms"]),
        }
    domain_stats_path = os.path.join(data_dir, "domain_statistics.json")
    with open(domain_stats_path, "w") as f:
        json.dump(domain_stats, f, indent=2, default=str)
    saved.append(domain_stats_path)

    # Save extracted terms summary
    terms_summary = {}
    for term_text, term_obj in results["terms"].items():
        terms_summary[term_text] = {
            "lemma": term_obj.lemma,
            "domains": term_obj.domains,
            "frequency": term_obj.frequency,
            "confidence": term_obj.confidence,
            "n_contexts": len(term_obj.contexts),
        }
    terms_path = os.path.join(data_dir, "extracted_terms.json")
    with open(terms_path, "w") as f:
        json.dump(terms_summary, f, indent=2, default=str)
    saved.append(terms_path)

    # Save concept map summary
    concept_summary = {
        "n_concepts": len(results["concept_map"].concepts),
        "n_relationships": len(results["concept_map"].concept_relationships),
        "concepts": {
            name: {
                "description": c.description,
                "n_terms": len(c.terms),
                "domains": list(c.domains),
                "confidence": c.confidence,
            }
            for name, c in results["concept_map"].concepts.items()
        },
    }
    concept_path = os.path.join(data_dir, "concept_map_summary.json")
    with open(concept_path, "w") as f:
        json.dump(concept_summary, f, indent=2, default=str)
    saved.append(concept_path)

    logger.info(f"\nğŸ“Š Saved {len(saved)} data files to {data_dir}")
    for s in saved:
        logger.info(f"   - {os.path.basename(s)}")

    return saved


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Figure Registry
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _register_figures_with_manager(figures: List[str], figure_dir: str) -> None:
    """Register generated figures with FigureManager for cross-referencing."""
    try:
        from visualization.figure_manager import FigureManager

        registry_file = os.path.join(figure_dir, "figure_registry.json")
        fm = FigureManager(registry_file=registry_file)

        figure_metadata = {
            "concept_map.png": {
                "label": "fig:concept_map",
                "caption": "Ento-Linguistic Concept Map",
                "section": "introduction",
            },
            "terminology_network.png": {
                "label": "fig:terminology_network",
                "caption": "Terminology Network with Domain Clustering",
                "section": "experimental_results",
            },
            "domain_comparison.png": {
                "label": "fig:domain_comparison",
                "caption": "Cross-Domain Terminology Comparison",
                "section": "experimental_results",
            },
            "domain_overlap_heatmap.png": {
                "label": "fig:domain_overlap",
                "caption": "Domain Term Overlap Heatmap",
                "section": "experimental_results",
            },
            "anthropomorphic_framing.png": {
                "label": "fig:anthropomorphic",
                "caption": "Anthropomorphic Framing Analysis",
                "section": "discussion",
            },
            "concept_hierarchy.png": {
                "label": "fig:concept_hierarchy",
                "caption": "Conceptual Hierarchy Analysis",
                "section": "discussion",
            },
            "unit_of_individuality_patterns.png": {
                "label": "fig:unit_individuality_patterns",
                "caption": "Unit of Individuality Terminology Patterns",
                "section": "experimental_results",
            },
            "unit_of_individuality_term_frequencies.png": {
                "label": "fig:unit_individuality_frequencies",
                "caption": "Unit of Individuality Term Frequencies",
                "section": "experimental_results",
            },
            "unit_of_individuality_ambiguities.png": {
                "label": "fig:unit_individuality_ambiguities",
                "caption": "Unit of Individuality Ambiguity Analysis",
                "section": "experimental_results",
            },
            "power_and_labor_term_frequencies.png": {
                "label": "fig:power_labor_frequencies",
                "caption": "Power & Labor Term Frequencies",
                "section": "experimental_results",
            },
            "power_and_labor_ambiguities.png": {
                "label": "fig:power_labor_ambiguities",
                "caption": "Power & Labor Ambiguity Analysis",
                "section": "experimental_results",
            },
        }

        # Dynamically register domain figures to ensure full coverage
        valid_domains = [
            "unit_of_individuality",
            "behavior_and_identity",
            "power_and_labor",
            "sex_and_reproduction",
            "kin_and_relatedness",
            "economics",
        ]
        
        for domain in valid_domains:
            # Normalize label keys to match manuscript conventions
            # e.g., unit_of_individuality -> unit_individuality
            # e.g., power_and_labor -> power_labor
            label_base = domain.replace("_and_", "_").replace("unit_of_", "unit_")
            d_title = domain.replace("_", " ").title()
            
            # Register all 3 types if not already manually defined
            display_map = {
                "patterns": (f"fig:{label_base}_patterns", f"{d_title} Term Patterns"),
                "term_frequencies": (f"fig:{label_base}_frequencies", f"{d_title} Term Frequencies"),
                "ambiguities": (f"fig:{label_base}_ambiguities", f"{d_title} Ambiguity Analysis"),
            }
            
            for type_suffix, (label, caption) in display_map.items():
                filename = f"{domain}_{type_suffix}.png"
                if filename not in figure_metadata:
                    figure_metadata[filename] = {
                        "label": label,
                        "caption": caption,
                        "section": "experimental_results",
                    }

        registered = 0
        for fig_path in figures:
            filename = os.path.basename(fig_path)
            if filename in figure_metadata:
                meta = figure_metadata[filename]
                fm.register_figure(
                    filename=filename,
                    caption=meta["caption"],
                    label=meta["label"],
                    section=meta["section"],
                )
                registered += 1

        logger.info(f"âœ… Registered {registered} figures with FigureManager")

    except ImportError:
        logger.warning("âš ï¸  FigureManager not available, skipping registration")
    except Exception as e:
        logger.warning(f"âš ï¸  Could not register figures: {e}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Main Entry Point
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main() -> None:
    """Generate all research figures and data using the real analysis pipeline."""
    output_dir, data_dir, figure_dir = _setup_directories()

    logger.info("=" * 70)
    logger.info("  Ento-Linguistic Research Figure Generation Pipeline")
    logger.info("=" * 70)
    logger.info(f"  Output: {output_dir}")
    logger.info(f"  Corpus: {len(REAL_ABSTRACTS)} real abstracts")
    logger.info("")

    # â”€â”€ Run analysis pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info("â–¶ Running analysis pipeline...")
    results = run_analysis_pipeline(REAL_ABSTRACTS)
    logger.info("")

    # â”€â”€ Generate figures â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info("â–¶ Generating figures...")
    figures = []

    fig_path = generate_concept_map(results, figure_dir)
    if fig_path:
        figures.append(fig_path)

    fig_path = generate_terminology_network(results, figure_dir)
    if fig_path:
        figures.append(fig_path)

    fig_path = generate_domain_comparison(results, figure_dir)
    if fig_path:
        figures.append(fig_path)

    fig_path = generate_domain_overlap_heatmap(results, figure_dir)
    if fig_path:
        figures.append(fig_path)

    fig_path = generate_anthropomorphic_analysis(results, figure_dir)
    if fig_path:
        figures.append(fig_path)

    fig_path = generate_concept_hierarchy(results, figure_dir)
    if fig_path:
        figures.append(fig_path)

    fig_path = generate_unit_of_individuality_patterns(results, figure_dir)
    if fig_path:
        figures.append(fig_path)

    fig_path = generate_power_labor_term_frequencies(results, figure_dir)
    if fig_path:
        figures.append(fig_path)

    fig_path = generate_power_labor_ambiguities(results, figure_dir)
    if fig_path:
        figures.append(fig_path)

    # Generate generic domain figures (patterns, etc.)
    domain_figs = generate_domain_figures(results, figure_dir)
    figures.extend(domain_figs)

    logger.info("")

    # â”€â”€ Save data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    data_files = save_analysis_data(results, data_dir)

    # â”€â”€ Register figures â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    _register_figures_with_manager(figures, figure_dir)

    # â”€â”€ Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info("")
    logger.info("=" * 70)
    logger.info(f"  âœ… Generated {len(figures)} research figures")
    for fig in figures:
        logger.info(f"     - {os.path.basename(fig)}")
    logger.info(f"  âœ… Saved {len(data_files)} data files")
    logger.info(f"  ğŸ“ All outputs: {output_dir}")
    logger.info("=" * 70)

    # Also print for subprocess test capture
    print("Integration with src/ modules demonstrated")

    # â”€â”€ Validation (if infrastructure available) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if INFRASTRUCTURE_AVAILABLE and validate_figure_registry:
        try:
            registry_path = Path(figure_dir) / "figure_registry.json"
            manuscript_dir = Path(project_root) / "manuscript"
            validate_figure_registry(registry_path, manuscript_dir)
            logger.info("âœ… Figure registry validation passed")
        except Exception as exc:
            logger.warning(f"âš ï¸  Figure registry validation warning: {exc}")

    if INFRASTRUCTURE_AVAILABLE and verify_output_integrity:
        try:
            verify_output_integrity(Path(output_dir))
            logger.info("âœ… Output integrity check passed")
        except Exception as exc:
            logger.warning(f"âš ï¸  Output integrity warning: {exc}")


if __name__ == "__main__":
    main()
