% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
  ignorenonframetext,
]{beamer}
\newif\ifbibliography
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% remove section numbering
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{section title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{subsection title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{longtable,booktabs,array}
\newcounter{none} % for unnumbered tables
\usepackage{calc} % for calculating minipage widths
\usepackage{caption}
% Make caption package work with longtable
\makeatletter
\def\fnum@table{\tablename~\thetable}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{\texorpdfstring{}{}}
\date{}

\begin{document}

\begin{frame}{Results}
\protect\phantomsection\label{results}
This section presents the theoretical results and mathematical
derivations obtained through our methodological approach.

\begin{block}{Theoretical Results}
\protect\phantomsection\label{theoretical-results}
The main theoretical contribution is encapsulated in the following
proposition:

\textbf{Proposition 1.} For any continuously differentiable function
\(f: \mathbb{R}^n \rightarrow \mathbb{R}\), the gradient descent
algorithm with appropriate step sizes converges to a stationary point.
\end{block}

\begin{block}{Mathematical Derivations}
\protect\phantomsection\label{mathematical-derivations}
Consider the Taylor expansion of \(f\) around point \(x\):

\[f(x + h) = f(x) + \nabla f(x)^T h + \frac{1}{2} h^T \nabla^2 f(x) h + O(\|h\|^3)\]

For small \(h\), the dominant term is the linear term
\(\nabla f(x)^T h\).
\end{block}

\begin{block}{Algorithm Convergence}
\protect\phantomsection\label{algorithm-convergence}
The convergence rate analysis yields:

\[\lim_{k \rightarrow \infty} \|\nabla f(x_k)\| = 0\]

with convergence rate depending on the condition number of the Hessian
matrix.
\end{block}

\begin{block}{Key Findings}
\protect\phantomsection\label{key-findings}
Our theoretical analysis reveals several important findings:

\begin{enumerate}
\tightlist
\item
  \textbf{Convergence Properties}

  \begin{itemize}
  \tightlist
  \item
    Linear convergence for strongly convex functions
  \item
    Sublinear convergence for general convex functions
  \item
    No convergence guarantee for non-convex functions
  \end{itemize}
\item
  \textbf{Optimal Step Sizes}

  \begin{itemize}
  \tightlist
  \item
    Constant step size:
    \(\alpha = \frac{2}{\lambda_{\min} + \lambda_{\max}}\)
  \item
    Diminishing step size: \(\alpha_k = \frac{\alpha}{k+1}\)
  \item
    Adaptive step size based on function properties
  \end{itemize}
\item
  \textbf{Numerical Stability}

  \begin{itemize}
  \tightlist
  \item
    Condition number affects convergence speed
  \item
    Ill-conditioned problems require preconditioning
  \item
    Gradient computation accuracy impacts final precision
  \end{itemize}
\end{enumerate}
\end{block}

\begin{block}{Comparative Analysis}
\protect\phantomsection\label{comparative-analysis}
{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Method & Convergence Rate & Memory Usage & Implementation Complexity \\
\midrule\noalign{}
\endhead
Gradient Descent & Linear & O(n) & Low \\
Newton Method & Quadratic & O(n²) & High \\
Conjugate Gradient & Superlinear & O(n) & Medium \\
BFGS & Superlinear & O(n²) & High \\
\bottomrule\noalign{}
\end{longtable}
}

Table 1: Comparison of optimization methods showing trade-offs between
convergence speed, memory requirements, and implementation complexity.
\end{block}

\begin{block}{Discussion}
\protect\phantomsection\label{discussion}
The results demonstrate that:

\begin{itemize}
\tightlist
\item
  \textbf{Theoretical guarantees} exist for convex optimization problems
\item
  \textbf{Practical performance} depends on problem conditioning
\item
  \textbf{Algorithm selection} should balance convergence speed with
  computational cost
\item
  \textbf{Numerical considerations} are crucial for reliable
  implementation
\end{itemize}
\end{block}

\begin{block}{Future Directions}
\protect\phantomsection\label{future-directions}
Several avenues for future research include:

\begin{itemize}
\tightlist
\item
  Extension to constrained optimization problems
\item
  Development of adaptive step size strategies
\item
  Analysis of stochastic gradient variants
\item
  Application to large-scale machine learning problems
\end{itemize}
\end{block}
\end{frame}

\end{document}
