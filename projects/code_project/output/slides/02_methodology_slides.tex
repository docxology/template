% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
  ignorenonframetext,
]{beamer}
\newif\ifbibliography
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% remove section numbering
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{section title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{subsection title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{\texorpdfstring{}{}}
\date{}

\begin{document}

\begin{frame}[fragile]{Methodology}
\protect\phantomsection\label{methodology}
This section describes the implementation methodology and experimental
setup used in the optimization project.

\begin{block}{Algorithm Implementation}
\protect\phantomsection\label{algorithm-implementation}
\begin{block}{Gradient Descent Algorithm}
\protect\phantomsection\label{gradient-descent-algorithm}
The core algorithm implements the following iterative procedure for
unconstrained optimization:

\textbf{Input:} Initial point \(x_0 \in \mathbb{R}^d\), step size
\(\alpha > 0\), tolerance \(\epsilon > 0\), maximum iterations
\(N_{\max} \in \mathbb{N}\)

\textbf{Output:} Approximate solution \(x^* \approx \arg\min f(x)\)

\textbf{Algorithm 1: Gradient Descent}

\begin{verbatim}
Initialize: k ← 0, x_0 ∈ ℝ^d
While k < N_max do:
    Compute gradient: ∇f(x_k)
    Check convergence: if ||∇f(x_k)||_2 < ε then
        Return x_k as approximate solution
    Update: x_{k+1} ← x_k - α ∇f(x_k)
    Increment: k ← k + 1
Return x_k (maximum iterations reached)
\end{verbatim}

The algorithm follows the fundamental principle of steepest descent,
moving in the direction of the negative gradient to minimize the
objective function \(f: \mathbb{R}^d \rightarrow \mathbb{R}\).
\end{block}

\begin{block}{Test Problem: Quadratic Minimization}
\protect\phantomsection\label{test-problem-quadratic-minimization}
We use quadratic functions of the form:

\[f(x) = \frac{1}{2} x^T A x - b^T x\]

where: - \(A\) is a positive definite matrix - \(b\) is the linear term
vector - The gradient is: \(\nabla f(x) = A x - b\)

For the simple case \(A = I\) and \(b = 1\), we have:

\[f(x) = \frac{1}{2} x^2 - x\]

with gradient:

\[\nabla f(x) = x - 1\]

The analytical minimum occurs at \(x = 1\) with \(f(1) = -\frac{1}{2}\).
\end{block}
\end{block}

\begin{block}{Convergence Analysis}
\protect\phantomsection\label{convergence-analysis}
\begin{block}{Convergence Rate Theory}
\protect\phantomsection\label{convergence-rate-theory}
For strongly convex functions with condition number
\(\kappa = \frac{\lambda_{\max}}{\lambda_{\min}}\), the convergence rate
of gradient descent satisfies:

\[\frac{\|x_{k+1} - x^*\|}{\|x_k - x^*\|} \leq \sqrt{\frac{\kappa - 1}{\kappa + 1}}\]

where \(x^*\) denotes the optimal solution. This bound shows linear
convergence with rate
\(\rho = \sqrt{\frac{\kappa - 1}{\kappa + 1}} < 1\).

For quadratic functions \(f(x) = \frac{1}{2}x^T A x - b^T x\) where
\(A\) is positive definite, the convergence factor becomes:

\[\rho = \frac{|\lambda_{\max} - \alpha\lambda_{\min}|}{|\lambda_{\min} + \alpha\lambda_{\max}|}\]

where \(\alpha\) is the step size. Optimal convergence occurs when
\(\alpha = \frac{2}{\lambda_{\min} + \lambda_{\max}}\), yielding
\(\rho = \frac{\kappa - 1}{\kappa + 1}\).
\end{block}

\begin{block}{Step Size Selection Criteria}
\protect\phantomsection\label{step-size-selection-criteria}
The optimal constant step size for quadratic functions is:

\[\alpha = \frac{2}{\lambda_{\min} + \lambda_{\max}}\]

For our test problem with \(\lambda_{\min} = \lambda_{\max} = 1\), this
gives \(\alpha = 1\).
\end{block}

\begin{block}{Complexity Analysis}
\protect\phantomsection\label{complexity-analysis}
The computational complexity per iteration is: - \textbf{Time
complexity}: \(O(n)\) for gradient computation - \textbf{Space
complexity}: \(O(n)\) for storing variables

Total complexity for convergence:
\(O\left(n \cdot \log\left(\frac{1}{\epsilon}\right)\right)\)
\end{block}
\end{block}

\begin{block}{Experimental Setup}
\protect\phantomsection\label{experimental-setup}
\begin{block}{Step Size Analysis}
\protect\phantomsection\label{step-size-analysis}
We investigate the effect of different step sizes on convergence:

\begin{itemize}
\tightlist
\item
  \(\alpha = 0.01\) (conservative)
\item
  \(\alpha = 0.05\) (moderate)
\item
  \(\alpha = 0.10\) (aggressive)
\item
  \(\alpha = 0.20\) (very aggressive)
\end{itemize}
\end{block}

\begin{block}{Convergence Criteria}
\protect\phantomsection\label{convergence-criteria}
The algorithm terminates when: - Gradient norm falls below tolerance:
\(||\nabla f(x)|| < \epsilon\) - Maximum iterations reached: \(k = N\)
\end{block}

\begin{block}{Performance Metrics}
\protect\phantomsection\label{performance-metrics}
We track: - \textbf{Solution accuracy}: Distance to analytical optimum -
\textbf{Convergence speed}: Number of iterations to convergence -
\textbf{Objective value}: Function value at final solution
\end{block}
\end{block}

\begin{block}{Implementation Details}
\protect\phantomsection\label{implementation-details}
\begin{block}{Numerical Stability Considerations}
\protect\phantomsection\label{numerical-stability-considerations}
The implementation uses NumPy's vectorized operations for efficient
computation. Numerical stability is ensured through:

\begin{itemize}
\tightlist
\item
  \textbf{Gradient computation}: Analytical gradients computed using
  matrix operations
\item
  \textbf{Convergence checking}: Relative gradient norms to handle
  different scales
\item
  \textbf{Step size validation}: Bounds checking to prevent divergence
\item
  \textbf{Iteration limits}: Maximum iteration caps to prevent infinite
  loops
\end{itemize}
\end{block}

\begin{block}{Error Handling and Robustness}
\protect\phantomsection\label{error-handling-and-robustness}
Input validation ensures algorithmic reliability:

\begin{itemize}
\tightlist
\item
  \textbf{Matrix dimensions}: Compatible shapes for quadratic terms and
  linear coefficients
\item
  \textbf{Step size bounds}: \(\alpha > 0\) with upper bounds to prevent
  oscillation
\item
  \textbf{Tolerance validation}: \(\epsilon > 0\) with machine precision
  considerations
\item
  \textbf{Initial point validation}: Finite, non-NaN starting values
\end{itemize}
\end{block}

\begin{block}{Testing Strategy and Validation}
\protect\phantomsection\label{testing-strategy-and-validation}
Comprehensive test suite covers multiple dimensions:

\begin{itemize}
\tightlist
\item
  \textbf{Functional correctness}: Analytical gradient verification
  against finite differences
\item
  \textbf{Convergence behavior}: Multiple step sizes and tolerance
  levels
\item
  \textbf{Edge cases}: Pre-converged solutions, maximum iteration limits
\item
  \textbf{Numerical accuracy}: Comparison with analytical solutions for
  quadratic functions
\item
  \textbf{Robustness}: Ill-conditioned problems and numerical precision
  limits
\end{itemize}
\end{block}
\end{block}

\begin{block}{Analysis Pipeline}
\protect\phantomsection\label{analysis-pipeline}
The analysis script automatically: 1. Runs optimization experiments with
different parameters 2. Collects convergence trajectories 3. Generates
publication-quality plots 4. Saves numerical results to CSV files 5.
Registers figures for manuscript integration

This automated approach ensures reproducible research and consistent
result generation.
\end{block}
\end{frame}

\end{document}
