% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
  ignorenonframetext,
]{beamer}
\newif\ifbibliography
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% remove section numbering
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{section title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{subsection title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{\texorpdfstring{}{}}
\date{}

\begin{document}

\begin{frame}[fragile]{Methodology}
\protect\phantomsection\label{methodology}
This section describes the implementation methodology and experimental
setup used in the optimization project.

\begin{block}{Algorithm Implementation}
\protect\phantomsection\label{algorithm-implementation}
\begin{block}{Gradient Descent Algorithm}
\protect\phantomsection\label{gradient-descent-algorithm}
The core algorithm implements the following iterative procedure for
unconstrained optimization:

\textbf{Input:} Initial point \(x_0 \in \mathbb{R}^d\), step size
\(\alpha > 0\), tolerance \(\epsilon > 0\), maximum iterations
\(N_{\max} \in \mathbb{N}\)

\textbf{Output:} Approximate solution \(x^* \approx \arg\min f(x)\)

\textbf{Algorithm 1: Gradient Descent}

\begin{quote}
\textbf{Input:} Initial point \(x_0\), step size \(\alpha\), tolerance
\(\epsilon\), max iterations \(N_{\max}\)

\begin{enumerate}
\tightlist
\item
  Initialize \(k \leftarrow 0\)
\item
  \textbf{While} \(k < N_{\max}\) \textbf{do:}

  \begin{itemize}
  \tightlist
  \item
    Compute gradient \(g_k = \nabla f(x_k)\)
  \item
    \textbf{If} \(\|g_k\|_2 < \epsilon\) \textbf{then return} \(x_k\)
    \emph{(converged)}
  \item
    Update: \(x_{k+1} \leftarrow x_k - \alpha \cdot g_k\)
  \item
    \(k \leftarrow k + 1\)
  \end{itemize}
\item
  \textbf{Return} \(x_k\) \emph{(max iterations reached)}
\end{enumerate}
\end{quote}

The algorithm follows the fundamental principle of steepest descent,
moving in the direction of the negative gradient to minimize the
objective function \(f: \mathbb{R}^d \rightarrow \mathbb{R}\)
\cite{cauchy1847methode}.
\end{block}

\begin{block}{Test Problem: Quadratic Minimization}
\protect\phantomsection\label{test-problem-quadratic-minimization}
We use quadratic functions of the form:

\begin{equation}
\label{eq:quadratic_objective}
f(x) = \frac{1}{2} x^T A x - b^T x
\end{equation}

where:

\begin{itemize}
\tightlist
\item
  \(A\) is a positive definite matrix
\item
  \(b\) is the linear term vector
\item
  The gradient is: \(\nabla f(x) = A x - b\)
\end{itemize}

For the simple case \(A = I\) and \(b = 1\), we have:

\begin{equation}
\label{eq:simple_quadratic}
f(x) = \frac{1}{2} x^2 - x
\end{equation}

with gradient:

\begin{equation}
\label{eq:simple_gradient}
\nabla f(x) = x - 1
\end{equation}

The analytical minimum occurs at \(x = 1\) with \(f(1) = -\frac{1}{2}\).
\end{block}
\end{block}

\begin{block}{Convergence Analysis}
\protect\phantomsection\label{convergence-analysis}
\begin{block}{Convergence Rate Theory}
\protect\phantomsection\label{convergence-rate-theory}
The theoretical foundations of convergence analysis for gradient descent
methods are well-established in the optimization literature
\cite{bertsekas1999nonlinear, boyd2004convex}.

For strongly convex functions with condition number
\(\kappa = \frac{\lambda_{\max}}{\lambda_{\min}}\), the convergence rate
of gradient descent satisfies:

\begin{equation}
\label{eq:convergence_rate}
\frac{\|x_{k+1} - x^*\|}{\|x_k - x^*\|} \leq \sqrt{\frac{\kappa - 1}{\kappa + 1}}
\end{equation}

where \(x^*\) denotes the optimal solution. This bound shows linear
convergence with rate
\(\rho = \sqrt{\frac{\kappa - 1}{\kappa + 1}} < 1\).

For quadratic functions \(f(x) = \frac{1}{2}x^T A x - b^T x\) where
\(A\) is positive definite, the convergence factor becomes:

\begin{equation}
\label{eq:convergence_factor}
\rho = \frac{|\lambda_{\max} - \alpha\lambda_{\min}|}{|\lambda_{\min} + \alpha\lambda_{\max}|}
\end{equation}

where \(\alpha\) is the step size. Optimal convergence occurs when
\(\alpha = \frac{2}{\lambda_{\min} + \lambda_{\max}}\), yielding
\(\rho = \frac{\kappa - 1}{\kappa + 1}\).
\end{block}

\begin{block}{Step Size Selection Criteria}
\protect\phantomsection\label{step-size-selection-criteria}
The optimal constant step size for quadratic functions is:

\begin{equation}
\label{eq:optimal_step_size}
\alpha = \frac{2}{\lambda_{\min} + \lambda_{\max}}
\end{equation}

For our test problem with \(\lambda_{\min} = \lambda_{\max} = 1\), this
gives \(\alpha = 1\).
\end{block}

\begin{block}{Complexity Analysis}
\protect\phantomsection\label{complexity-analysis}
The computational complexity per iteration is:

\begin{itemize}
\tightlist
\item
  \textbf{Time complexity}: \(O(n)\) for gradient computation
\item
  \textbf{Space complexity}: \(O(n)\) for storing variables
\end{itemize}

Total complexity for convergence:
\(O\left(n \cdot \log\left(\frac{1}{\epsilon}\right)\right)\)
\end{block}
\end{block}

\begin{block}{Experimental Setup}
\protect\phantomsection\label{experimental-setup}
\begin{block}{Step Size Analysis}
\protect\phantomsection\label{step-size-analysis}
We investigate the effect of different step sizes on convergence:

\begin{itemize}
\tightlist
\item
  \(\alpha = 0.01\) (conservative)
\item
  \(\alpha = 0.05\) (moderate)
\item
  \(\alpha = 0.10\) (aggressive)
\item
  \(\alpha = 0.20\) (very aggressive)
\end{itemize}
\end{block}

\begin{block}{Convergence Criteria}
\protect\phantomsection\label{convergence-criteria}
The algorithm terminates when:

\begin{itemize}
\tightlist
\item
  Gradient norm falls below tolerance: \(||\nabla f(x)|| < \epsilon\)
\item
  Maximum iterations reached: \(k = N\)
\end{itemize}
\end{block}

\begin{block}{Performance Metrics}
\protect\phantomsection\label{performance-metrics}
We track:

\begin{itemize}
\tightlist
\item
  \textbf{Solution accuracy}: Distance to analytical optimum
\item
  \textbf{Convergence speed}: Number of iterations to convergence
\item
  \textbf{Objective value}: Function value at final solution
\end{itemize}
\end{block}
\end{block}

\begin{block}{Implementation Details}
\protect\phantomsection\label{implementation-details}
\begin{block}{Numerical Stability Considerations}
\protect\phantomsection\label{numerical-stability-considerations}
The implementation uses NumPy's vectorized operations for efficient
computation. Numerical stability is ensured through:

\begin{itemize}
\tightlist
\item
  \textbf{Gradient computation}: Analytical gradients computed using
  matrix operations
\item
  \textbf{Convergence checking}: Relative gradient norms to handle
  different scales
\item
  \textbf{Step size validation}: Bounds checking to prevent divergence
\item
  \textbf{Iteration limits}: Maximum iteration caps to prevent infinite
  loops
\end{itemize}
\end{block}

\begin{block}{Error Handling and Robustness}
\protect\phantomsection\label{error-handling-and-robustness}
Input validation ensures algorithmic reliability:

\begin{itemize}
\tightlist
\item
  \textbf{Matrix dimensions}: Compatible shapes for quadratic terms and
  linear coefficients
\item
  \textbf{Step size bounds}: \(\alpha > 0\) with upper bounds to prevent
  oscillation
\item
  \textbf{Tolerance validation}: \(\epsilon > 0\) with machine precision
  considerations
\item
  \textbf{Initial point validation}: Finite, non-NaN starting values
\end{itemize}
\end{block}

\begin{block}{Testing Strategy and Validation}
\protect\phantomsection\label{testing-strategy-and-validation}
The comprehensive test suite covers multiple dimensions:

\begin{itemize}
\tightlist
\item
  \textbf{Functional correctness}: Analytical gradient verification
  against finite differences
\item
  \textbf{Convergence behavior}: Multiple step sizes and tolerance
  levels
\item
  \textbf{Edge cases}: Pre-converged solutions, maximum iteration limits
\item
  \textbf{Numerical accuracy}: Comparison with analytical solutions for
  quadratic functions
\item
  \textbf{Robustness}: Ill-conditioned problems and numerical precision
  limits
\end{itemize}
\end{block}
\end{block}

\begin{block}{LaTeX Customization and Rendering}
\protect\phantomsection\label{latex-customization-and-rendering}
The research template supports advanced LaTeX customization through
optional preamble configuration. An optional \texttt{preamble.md} file
can contain custom LaTeX packages and commands that are automatically
inserted before document compilation. The rendering system ensures
required packages (such as \texttt{graphicx} for figure inclusion) are
loaded automatically, while allowing researchers to add specialized
packages for mathematical notation, bibliography styles, or document
formatting.
\end{block}

\begin{block}{Analysis Pipeline}
\protect\phantomsection\label{analysis-pipeline}
The analysis script automatically:

\begin{enumerate}
\tightlist
\item
  Runs optimization experiments with different parameters
\item
  Collects convergence trajectories
\item
  Generates publication-quality plots
\item
  Saves numerical results to CSV files
\item
  Registers figures for manuscript integration
\end{enumerate}

This automated approach ensures reproducible research and consistent
result generation.
\end{block}
\end{frame}

\end{document}
