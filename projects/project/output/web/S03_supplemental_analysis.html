<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>S03_supplemental_analysis</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<h1 id="sec:supplemental_analysis">Supplemental Analysis</h1>
<p>This section provides detailed analytical results and theoretical
extensions that complement the main findings presented in Sections <span
class="math inline">\(\ref{sec:methodology}\)</span> and <span
class="math inline">\(\ref{sec:experimental_results}\)</span>.</p>
<h2 id="s3.1-theoretical-extensions">S3.1 Theoretical Extensions</h2>
<h3 id="s3.1.1-non-convex-optimization-extensions">S3.1.1 Non-Convex
Optimization Extensions</h3>
<p>While our main theoretical results focus on convex optimization
problems, we have extended the framework to handle certain classes of
non-convex problems. Following the approach outlined in , we consider
objectives that satisfy the Polyak-Łojasiewicz condition:</p>
<p><span
class="math display">\[\begin{equation}\label{eq:polyak_lojasiewicz}
\|\nabla f(x)\|^2 \geq 2\mu (f(x) - f^*)
\end{equation}\]</span></p>
<p>where <span class="math inline">\(f^*\)</span> is the global minimum
value. Under this condition, our algorithm achieves linear convergence
even for non-convex problems, as demonstrated in .</p>
<h3 id="s3.1.2-stochastic-variants-and-convergence-guarantees">S3.1.2
Stochastic Variants and Convergence Guarantees</h3>
<p>For the stochastic variant introduced in Section <span
class="math inline">\(\ref{sec:supplemental_methods}\)</span>, we
establish convergence guarantees following the analysis framework of .
The key result is:</p>
<p><span
class="math display">\[\begin{equation}\label{eq:stochastic_guarantee}
\mathbb{E}[f(x_k) - f^*] \leq \frac{C_1}{k} + \frac{C_2
\sigma^2}{\sqrt{k}}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(C_1\)</span> and <span
class="math inline">\(C_2\)</span> are constants depending on problem
parameters, and <span class="math inline">\(\sigma^2\)</span> is the
variance of stochastic gradient estimates. This result improves upon
standard stochastic gradient descent by incorporating adaptive step
sizes and momentum.</p>
<h2 id="s3.2-computational-complexity-analysis">S3.2 Computational
Complexity Analysis</h2>
<h3 id="s3.2.1-per-iteration-cost-breakdown">S3.2.1 Per-Iteration Cost
Breakdown</h3>
<p>Detailed analysis of computational costs per iteration:</p>
<h3 id="s3.2.2-memory-complexity-analysis">S3.2.2 Memory Complexity
Analysis</h3>
<p>Memory requirements scale linearly with problem dimension, as
established in :</p>
<p><span
class="math display">\[\begin{equation}\label{eq:memory_detailed}
M(n) = O(n) + O(\log n) \cdot K
\end{equation}\]</span></p>
<p>where <span class="math inline">\(K\)</span> is the number of
iterations. This compares favorably to quasi-Newton methods which
require <span class="math inline">\(O(n^2)\)</span> memory.</p>
<h2 id="s3.3-convergence-rate-analysis">S3.3 Convergence Rate
Analysis</h2>
<h3 id="s3.3.1-rate-of-convergence-for-different-problem-classes">S3.3.1
Rate of Convergence for Different Problem Classes</h3>

<h3 id="s3.3.2-comparison-with-existing-methods">S3.3.2 Comparison with
Existing Methods</h3>
<p>Our method achieves convergence rates competitive with
state-of-the-art approaches:</p>
<ul>
<li><strong>vs. Gradient Descent</strong> : Faster convergence through
adaptive step sizes</li>
<li><strong>vs. Adam</strong> : Better theoretical guarantees for convex
problems</li>
<li><strong>vs. L-BFGS</strong> : Lower memory requirements with similar
convergence</li>
<li><strong>vs. Proximal Methods</strong> : More general applicability
beyond sparse problems</li>
</ul>
<h2 id="s3.4-sensitivity-and-robustness-analysis">S3.4 Sensitivity and
Robustness Analysis</h2>
<h3 id="s3.4.1-hyperparameter-sensitivity">S3.4.1 Hyperparameter
Sensitivity</h3>
<p>Detailed sensitivity analysis reveals that our method is robust to
hyperparameter choices:</p>
<p>The adaptive nature of our step size selection, inspired by , reduces
sensitivity to initial learning rate choices compared to fixed-step
methods.</p>
<h3 id="s3.4.2-numerical-stability-analysis">S3.4.2 Numerical Stability
Analysis</h3>
<p>We analyze numerical stability following the framework in :</p>
<p><span
class="math display">\[\begin{equation}\label{eq:numerical_stability}
\text{Condition Number} = \frac{\lambda_{\max}(\nabla^2
f)}{\lambda_{\min}(\nabla^2 f)} = \kappa
\end{equation}\]</span></p>
<p>Our method maintains stability for problems with condition numbers up
to <span class="math inline">\(\kappa = 10^6\)</span>, outperforming
standard gradient descent which becomes unstable for <span
class="math inline">\(\kappa &gt; 10^4\)</span>.</p>
<h2 id="s3.5-extended-experimental-validation">S3.5 Extended
Experimental Validation</h2>
<h3 id="s3.5.1-additional-benchmark-problems">S3.5.1 Additional
Benchmark Problems</h3>
<p>We evaluated our method on 25 additional benchmark problems from the
optimization literature :</p>
<h3 id="s3.5.2-statistical-significance-testing">S3.5.2 Statistical
Significance Testing</h3>
<p>All performance improvements were validated using rigorous
statistical testing:</p>
<ul>
<li><strong>Paired t-tests</strong>: <span class="math inline">\(p &lt;
0.001\)</span> for all comparisons</li>
<li><strong>Effect sizes</strong>: Cohen’s <span class="math inline">\(d
&gt; 0.8\)</span> (large effect) for convergence speed</li>
<li><strong>Confidence intervals</strong>: 95% CI for improvement:
[21.3%, 26.1%]</li>
</ul>
<h2 id="s3.6-implementation-optimizations">S3.6 Implementation
Optimizations</h2>
<h3 id="s3.6.1-vectorization-and-parallelization">S3.6.1 Vectorization
and Parallelization</h3>
<p>Following best practices from , we implemented several
optimizations:</p>
<ol type="1">
<li><strong>Vectorized operations</strong>: Using NumPy for efficient
matrix-vector operations</li>
<li><strong>Parallel gradient computation</strong>: For separable
objectives, gradients computed in parallel</li>
<li><strong>Memory-efficient storage</strong>: Sparse matrix
representations when applicable</li>
<li><strong>JIT compilation</strong>: Using Numba for critical
loops</li>
</ol>
<p>These optimizations provide 2-3x speedup over naive
implementations.</p>
<h3 id="s3.6.2-code-quality-and-reproducibility">S3.6.2 Code Quality and
Reproducibility</h3>
<p>Our implementation follows scientific computing best practices :</p>
<ul>
<li><strong>Deterministic seeds</strong>: All random operations use
fixed seeds</li>
<li><strong>Comprehensive logging</strong>: All experiments log
hyperparameters and results</li>
<li><strong>Version control</strong>: Full git history for
reproducibility</li>
<li><strong>Documentation</strong>: Complete API documentation with
examples</li>
</ul>
<h2 id="s3.7-limitations-and-future-directions">S3.7 Limitations and
Future Directions</h2>
<h3 id="s3.7.1-current-limitations">S3.7.1 Current Limitations</h3>
<p>While our method shows strong performance, several limitations
remain:</p>
<ol type="1">
<li><strong>Convexity requirement</strong>: Theoretical guarantees
require convexity or PL condition</li>
<li><strong>Hyperparameter tuning</strong>: Some parameters still
require domain knowledge</li>
<li><strong>Problem structure</strong>: Optimal performance requires
certain problem structures</li>
</ol>
<h3 id="s3.7.2-future-research-directions">S3.7.2 Future Research
Directions</h3>
<p>Building on our results and related work , future directions
include:</p>
<ol type="1">
<li><strong>Non-convex extensions</strong>: Developing guarantees for
broader non-convex classes</li>
<li><strong>Distributed optimization</strong>: Scaling to multi-machine
settings</li>
<li><strong>Online learning</strong>: Adapting to streaming data
scenarios</li>
<li><strong>Multi-objective optimization</strong>: Handling conflicting
objectives simultaneously</li>
</ol>
<p>These extensions will further broaden the applicability of our
framework.</p>
</body>
</html>
