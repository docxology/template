% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
  ignorenonframetext,
]{beamer}
\newif\ifbibliography
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% remove section numbering
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{section title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{subsection title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{\texorpdfstring{}{}}
\date{}

\begin{document}

\begin{frame}{Supplemental Methods}
\protect\phantomsection\label{sec:supplemental_methods}
This section provides detailed methodological information that
supplements Section \ref{sec:methodology}.

\begin{block}{S1.1 Extended Algorithm Variants}
\protect\phantomsection\label{s1.1-extended-algorithm-variants}
\begin{block}{S1.1.1 Stochastic Variant}
\protect\phantomsection\label{s1.1.1-stochastic-variant}
For large-scale problems, we developed a stochastic variant of our
algorithm:

\begin{equation}\label{eq:stochastic_update}
x_{k+1} = x_k - \alpha_k \nabla f_{i_k}(x_k) + \beta_k (x_k - x_{k-1})
\end{equation}

where \(i_k\) is a randomly sampled index from \(\{1, \ldots, n\}\) at
iteration \(k\).

\textbf{Convergence Analysis}: Under appropriate sampling strategies,
this variant achieves \(O(1/\sqrt{k})\) convergence rate for
non-strongly convex problems, following the analysis in
\cite{kingma2014, ruder2016}.
\end{block}

\begin{block}{S1.1.2 Mini-Batch Variant}
\protect\phantomsection\label{s1.1.2-mini-batch-variant}
To balance between computational efficiency and convergence speed:

\begin{equation}\label{eq:minibatch_update}
x_{k+1} = x_k - \alpha_k \frac{1}{|B_k|} \sum_{i \in B_k} \nabla f_i(x_k) + \beta_k (x_k - x_{k-1})
\end{equation}

where \(B_k \subset \{1, \ldots, n\}\) is a mini-batch of size
\(|B_k| = b\).
\end{block}
\end{block}

\begin{block}{S1.2 Detailed Convergence Analysis}
\protect\phantomsection\label{s1.2-detailed-convergence-analysis}
\begin{block}{S1.2.1 Strong Convexity Assumptions}
\protect\phantomsection\label{s1.2.1-strong-convexity-assumptions}
We assume the objective function \(f\) satisfies:

\begin{equation}\label{eq:strong_convexity_detailed}
f(y) \geq f(x) + \nabla f(x)^T (y - x) + \frac{\mu}{2} \|y - x\|^2, \quad \forall x, y \in \mathcal{X}
\end{equation}

where \(\mu > 0\) is the strong convexity parameter.
\end{block}

\begin{block}{S1.2.2 Lipschitz Continuity}
\protect\phantomsection\label{s1.2.2-lipschitz-continuity}
The gradient is Lipschitz continuous:

\begin{equation}\label{eq:lipschitz_detailed}
\|\nabla f(x) - \nabla f(y)\| \leq L \|x - y\|, \quad \forall x, y \in \mathcal{X}
\end{equation}

The condition number \(\kappa = L/\mu\) determines the convergence rate:
\(\rho = \sqrt{1 - 1/\kappa}\), as established in
\cite{nesterov2018, boyd2004}.
\end{block}
\end{block}

\begin{block}{S1.3 Additional Theoretical Results}
\protect\phantomsection\label{s1.3-additional-theoretical-results}
\begin{block}{S1.3.1 Worst-Case Complexity Bounds}
\protect\phantomsection\label{s1.3.1-worst-case-complexity-bounds}
\textbf{Theorem S1}: Under the assumptions of Lipschitz continuity and
strong convexity, the algorithm requires at most
\(O(\kappa \log(1/\epsilon))\) iterations to achieve
\(\epsilon\)-accuracy.

\textbf{Proof}: From the convergence rate \eqref{eq:convergence}, we
have:

\begin{equation}\label{eq:iterations_bound}
\|x_k - x^*\| \leq C \rho^k \leq \epsilon \Rightarrow k \geq \frac{\log(C/\epsilon)}{\log(1/\rho)} = O(\kappa \log(1/\epsilon))
\end{equation}

since \(\log(1/\rho) \approx 1/\kappa\) for small \(1/\kappa\).
\(\square\)
\end{block}

\begin{block}{S1.3.2 Expected Convergence for Stochastic Variants}
\protect\phantomsection\label{s1.3.2-expected-convergence-for-stochastic-variants}
For the stochastic variant \eqref{eq:stochastic_update}:

\begin{equation}\label{eq:stochastic_convergence}
\mathbb{E}[\|x_k - x^*\|^2] \leq \frac{C}{k} + \sigma^2
\end{equation}

where \(\sigma^2\) is the variance of the stochastic gradient estimates.
\end{block}
\end{block}

\begin{block}{S1.4 Implementation Considerations}
\protect\phantomsection\label{s1.4-implementation-considerations}
\begin{block}{S1.4.1 Numerical Stability}
\protect\phantomsection\label{s1.4.1-numerical-stability}
To ensure numerical stability, we implement the following safeguards:

\begin{enumerate}
\tightlist
\item
  \textbf{Gradient clipping}:
  \(\nabla f(x_k) \leftarrow \min(1, \theta/\|\nabla f(x_k)\|) \nabla f(x_k)\)
\item
  \textbf{Step size bounds}:
  \(\alpha_{\min} \leq \alpha_k \leq \alpha_{\max}\)
\item
  \textbf{Momentum bounds}: \(0 \leq \beta_k \leq \beta_{\max} < 1\)
\end{enumerate}
\end{block}

\begin{block}{S1.4.2 Initialization Strategies}
\protect\phantomsection\label{s1.4.2-initialization-strategies}
We tested three initialization strategies:

\begin{enumerate}
\tightlist
\item
  \textbf{Random}: \(x_0 \sim \mathcal{N}(0, I)\)
\item
  \textbf{Warm start}: \(x_0 = \text{solution from simpler problem}\)
\item
  \textbf{Problem-specific}:
  \(x_0 = \text{domain knowledge-based initialization}\)
\end{enumerate}

Results show that warm start initialization reduces iterations by
approximately 30\% for related problem instances.
\end{block}
\end{block}

\begin{block}{S1.5 Extended Mathematical Framework}
\protect\phantomsection\label{s1.5-extended-mathematical-framework}
\begin{block}{S1.5.1 Generalized Objective Function}
\protect\phantomsection\label{s1.5.1-generalized-objective-function}
The framework extends to more general objectives:

\begin{equation}\label{eq:general_objective}
f(x) = \sum_{i=1}^{n} w_i \phi_i(x) + \sum_{j=1}^{m} \lambda_j R_j(x) + \sum_{k=1}^{p} \gamma_k C_k(x)
\end{equation}

where: - \(\phi_i(x)\): Data fitting terms - \(R_j(x)\): Regularization
terms (e.g., \(\ell_1\), \(\ell_2\), elastic net) - \(C_k(x)\):
Constraint terms (penalty or barrier functions)
\end{block}

\begin{block}{S1.5.2 Adaptive Weight Selection}
\protect\phantomsection\label{s1.5.2-adaptive-weight-selection}
Weights \(w_i\) can be adapted during optimization:

\begin{equation}\label{eq:adaptive_weights}
w_i^{(k+1)} = w_i^{(k)} \cdot \exp\left(-\gamma \frac{|\phi_i(x_k)|}{|\phi(x_k)|}\right)
\end{equation}

This reweighting scheme gives more emphasis to terms that are harder to
optimize.
\end{block}
\end{block}

\begin{block}{S1.6 Convergence Diagnostics}
\protect\phantomsection\label{s1.6-convergence-diagnostics}
\begin{block}{S1.6.1 Diagnostic Criteria}
\protect\phantomsection\label{s1.6.1-diagnostic-criteria}
We monitor the following quantities for convergence:

\begin{enumerate}
\tightlist
\item
  \textbf{Gradient norm}: \(\|\nabla f(x_k)\| < \epsilon_g\)
\item
  \textbf{Step size}: \(\|x_{k+1} - x_k\| < \epsilon_x\)
\item
  \textbf{Function improvement}: \(|f(x_{k+1}) - f(x_k)| < \epsilon_f\)
\item
  \textbf{Relative improvement}:
  \(|f(x_{k+1}) - f(x_k)|/|f(x_k)| < \epsilon_r\)
\end{enumerate}

All four criteria must be satisfied for declared convergence.
\end{block}

\begin{block}{S1.6.2 Failure Detection}
\protect\phantomsection\label{s1.6.2-failure-detection}
Algorithm failure is detected if:

\begin{enumerate}
\tightlist
\item
  Maximum iterations exceeded
\item
  Step size becomes too small (\(\alpha_k < \alpha_{\min}\))
\item
  NaN or Inf values encountered
\item
  Objective function increases for consecutive iterations
\end{enumerate}
\end{block}
\end{block}

\begin{block}{S1.7 Parameter Sensitivity}
\protect\phantomsection\label{s1.7-parameter-sensitivity}
Detailed sensitivity analysis for each parameter:

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Parameter} & \textbf{Nominal} & \textbf{Range} & \textbf{Impact on Performance} \\
\hline
$\alpha_0$ & 0.01 & [0.001, 0.1] & High (±30\%) \\
$\beta$ & 0.9 & [0.5, 0.99] & Medium (±15\%) \\
$\lambda$ & 0.001 & [0, 0.01] & Low (±5\%) \\
\hline
\end{tabular}
\caption{Parameter sensitivity analysis results}
\label{tab:parameter_sensitivity_detailed}
\end{table}

The learning rate \(\alpha_0\) has the strongest impact on convergence
speed, while regularization \(\lambda\) primarily affects the final
solution quality rather than convergence dynamics.
\end{block}
\end{frame}

\end{document}
