# ğŸ¤– AGENTS.md - Complete System Documentation

## ğŸ¯ System Overview

This document provides **comprehensive documentation** for the Research Project Template system, ensuring complete understanding of all functionality, configuration options, and operational procedures.

## ğŸ“‹ Table of Contents

1. [Core Architecture](#core-architecture)
2. [Directory-Level Documentation](#directory-level-documentation)
3. [Configuration System](#configuration-system)
4. [Rendering Pipeline](#rendering-pipeline)
5. [Validation Systems](#validation-systems)
6. [Testing Framework](#testing-framework)
7. [Output Formats](#output-formats)
8. [Advanced Modules](#advanced-modules)
9. [Troubleshooting](#troubleshooting)
10. [Maintenance](#maintenance)

## ğŸ—ï¸ Core Architecture

### Two-Layer Architecture

**Layer 1: Infrastructure (Generic - Reusable)**
- `infrastructure/` - Generic build/validation tools (reusable across projects)
- `scripts/` - Entry point orchestrators (two pipeline options: 6-stage or 9-stage)
- `tests/` - Infrastructure and integration tests

**Layer 2: Project (Project-Specific - Customizable)**
- `project/src/` - Research algorithms and analysis (domain-specific)
- `project/tests/` - Project test suite
- `project/scripts/` - Project analysis scripts (thin orchestrators)
- `project/manuscript/` - Research manuscript

### Thin Orchestrator Pattern

**CRITICAL**: All business logic resides in `project/src/` modules. Scripts are **thin orchestrators** that:

**Root Entry Points (Generic):**
- Coordinate build pipeline stages
- Discover and invoke `project/scripts/`
- Handle I/O, orchestration only
- Work with ANY project structure

**Project Scripts (Project-Specific):**
- Import from `project/src/` for computation
- Import from `infrastructure/` for utilities
- Orchestrate domain-specific workflows
- Handle I/O and visualization

**Violation of this pattern breaks the architecture**.

## ğŸ“š Repository Structure

The template separates **generic infrastructure** from **project-specific code**:

```
template/                           # Generic template repository
â”œâ”€â”€ infrastructure/                 # Generic build/validation tools (Layer 1)
â”‚   â”œâ”€â”€ AGENTS.md
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ *.py                        # build_verifier, figure_manager, etc.
â”œâ”€â”€ scripts/                        # Generic entry point orchestrators
â”‚   â”œâ”€â”€ AGENTS.md                   # Entry points: 00-setup, 01-tests, 02-analysis, 03-pdf, 04-validate, 05-copy
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ 00_setup_environment.py
â”‚   â”œâ”€â”€ 01_run_tests.py
â”‚   â”œâ”€â”€ 02_run_analysis.py          # Discovers & executes project/scripts/
â”‚   â”œâ”€â”€ 03_render_pdf.py
â”‚   â”œâ”€â”€ 04_validate_output.py
â”‚   â”œâ”€â”€ 05_copy_outputs.py          # Copies final deliverables
â”‚   â””â”€â”€ run_all.py                  # Master orchestrator
â”œâ”€â”€ tests/                          # Infrastructure tests
â”‚   â”œâ”€â”€ AGENTS.md
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ test_*.py                   # Tests for infrastructure/ modules
â””â”€â”€ project/                        # Example research project (can be replaced)
    â”œâ”€â”€ src/                        # Project-specific scientific code (Layer 2)
    â”‚   â”œâ”€â”€ AGENTS.md
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ example.py
    â”‚   â”œâ”€â”€ data_generator.py
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ tests/                      # Project tests
    â”‚   â”œâ”€â”€ AGENTS.md
    â”‚   â”œâ”€â”€ README.md
    â”‚   â””â”€â”€ test_*.py
    â”œâ”€â”€ scripts/                    # Project-specific analysis scripts
    â”‚   â”œâ”€â”€ AGENTS.md
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ analysis_pipeline.py
    â”‚   â””â”€â”€ example_figure.py
    â”œâ”€â”€ manuscript/                 # Research manuscript markdown
    â”œâ”€â”€ output/                     # Generated outputs (figures, PDFs)
    â””â”€â”€ pyproject.toml
```

## ğŸ“š Directory-Level Documentation

Each directory contains comprehensive documentation for easy navigation:

### Generic Infrastructure (Reusable)

| Directory | AGENTS.md | README.md | Purpose |
|-----------|-----------|-----------|---------|
| [`infrastructure/`](infrastructure/) | [AGENTS.md](infrastructure/AGENTS.md) | [README.md](infrastructure/README.md) | Generic build/validation tools (Layer 1) |
| [`scripts/`](scripts/) | [AGENTS.md](scripts/AGENTS.md) | [README.md](scripts/README.md) | Generic entry point orchestrators |
| [`tests/`](tests/) | [AGENTS.md](tests/AGENTS.md) | [README.md](tests/README.md) | Infrastructure test suite |

### Project-Specific (Customizable)

| Directory | AGENTS.md | README.md | Purpose |
|-----------|-----------|-----------|---------|
| [`project/src/`](project/src/) | [AGENTS.md](project/src/AGENTS.md) | [README.md](project/src/README.md) | Project-specific scientific code (Layer 2) |
| [`project/tests/`](project/tests/) | [AGENTS.md](project/tests/AGENTS.md) | [README.md](project/tests/README.md) | Project test suite |
| [`project/scripts/`](project/scripts/) | [AGENTS.md](project/scripts/AGENTS.md) | [README.md](project/scripts/README.md) | Project-specific analysis scripts |
| [`project/manuscript/`](project/manuscript/) | [AGENTS.md](project/manuscript/AGENTS.md) | [README.md](project/manuscript/README.md) | Research manuscript sections |

### Documentation Directories

| Directory | AGENTS.md | README.md | Purpose |
|-----------|-----------|-----------|---------|
| [`docs/`](docs/) | [AGENTS.md](docs/AGENTS.md) | [README.md](docs/README.md) | Project documentation hub |

### Documentation Navigation

**For detailed information:**
- Read directory-specific **AGENTS.md** files for comprehensive details
- Each AGENTS.md covers architecture, usage, and best practices

**For quick reference:**
- Check directory-specific **README.md** files for fast answers
- Each README.md provides quick start and essential commands

**Root documentation:**
- This file (root **AGENTS.md**) - Complete system overview
- [README.md](README.md) - Project quick start and introduction

### Directory Structure

```
template/                           # Generic Template
â”œâ”€â”€ infrastructure/                 # Generic build/validation tools (Layer 1)
â”‚   â”œâ”€â”€ AGENTS.md                   # Infrastructure documentation
â”‚   â”œâ”€â”€ README.md                   # Quick reference
â”‚   â”œâ”€â”€ build_verifier.py
â”‚   â”œâ”€â”€ figure_manager.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ scripts/                        # Entry Points (Generic Orchestrators)
â”‚   â”œâ”€â”€ AGENTS.md                   # Entry point documentation
â”‚   â”œâ”€â”€ README.md                   # Quick reference
â”‚   â”œâ”€â”€ 00_setup_environment.py     # Stage 0: Setup
â”‚   â”œâ”€â”€ 01_run_tests.py             # Stage 1: Test
â”‚   â”œâ”€â”€ 02_run_analysis.py          # Stage 2: Analysis (discovers project/scripts/)
â”‚   â”œâ”€â”€ 03_render_pdf.py            # Stage 3: PDF
â”‚   â”œâ”€â”€ 04_validate_output.py       # Stage 4: Validate
â”‚   â”œâ”€â”€ 05_copy_outputs.py          # Stage 5: Copy outputs
â”‚   â””â”€â”€ run_all.py                  # Master orchestrator
â”œâ”€â”€ tests/                          # Infrastructure Tests
â”‚   â”œâ”€â”€ AGENTS.md
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ test_*.py
â”œâ”€â”€ project/                        # Example Research Project (Customizable)
â”‚   â”œâ”€â”€ src/                        # Project scientific code (Layer 2)
â”‚   â”‚   â”œâ”€â”€ AGENTS.md               # Project documentation
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ example.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ tests/                      # Project Tests
â”‚   â”‚   â”œâ”€â”€ AGENTS.md
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â””â”€â”€ test_*.py
â”‚   â”œâ”€â”€ scripts/                    # Project Analysis Scripts
â”‚   â”‚   â”œâ”€â”€ AGENTS.md               # Project scripts documentation
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ analysis_pipeline.py
â”‚   â”‚   â””â”€â”€ example_figure.py
â”‚   â”œâ”€â”€ manuscript/                 # Research Manuscript
â”‚   â”œâ”€â”€ output/                     # Generated Files (disposable)
â”‚   â””â”€â”€ pyproject.toml              # Project configuration
â”œâ”€â”€ docs/                           # Documentation
â”‚   â”œâ”€â”€ AGENTS.md
â”‚   â””â”€â”€ README.md
â””â”€â”€ pyproject.toml                  # Root configuration
```

**Documentation in each directory:**
- **AGENTS.md** - Detailed directory-specific documentation
- **README.md** - Quick reference and navigation

**Note on src/ directory:**
- Root `src/` no longer exists (was empty shells)
- All code is in `infrastructure/` (generic) or `project/src/` (project-specific)
- This separation enables reusability across projects

## âš™ï¸ Configuration System

### Configuration File (Recommended)

The system supports configuration through a YAML file in `manuscript/config.yaml`, providing a centralized, version-controllable way to manage all paper metadata.

**Location**: `manuscript/config.yaml`  
**Template**: `manuscript/config.yaml.example`

**Example configuration**:
```yaml
paper:
  title: "Novel Optimization Framework"
  subtitle: ""  # Optional
  version: "1.0"

authors:
  - name: "Dr. Jane Smith"
    orcid: "0000-0000-0000-1234"
    email: "jane.smith@university.edu"
    affiliation: "University of Example"
    corresponding: true

publication:
  doi: "10.5281/zenodo.12345678"  # Optional
  journal: ""  # Optional
  volume: ""  # Optional
  pages: ""  # Optional

keywords:
  - "optimization"
  - "machine learning"

metadata:
  license: "Apache-2.0"
  language: "en"

# LLM Review Settings (optional)
llm:
  translations:
    enabled: true  # Set to false to disable translation generation
    languages:
      - zh  # Chinese (Simplified)
      - hi  # Hindi
      - ru  # Russian
```

**Benefits**:
- âœ… Version controllable (can be committed to git)
- âœ… Single file for all metadata
- âœ… Supports multiple authors with affiliations
- âœ… Structured format (YAML)
- âœ… Easy to edit and maintain

### Environment Variables (Backward Compatible)

For backward compatibility, environment variables still work and take precedence over config file values:

| Variable | Default | Description |
|----------|---------|-------------|
| `AUTHOR_NAME` | `"Project Author"` | Primary author name |
| `AUTHOR_ORCID` | `"0000-0000-0000-0000"` | Author ORCID identifier |
| `AUTHOR_EMAIL` | `"author@example.com"` | Author contact email |
| `DOI` | `""` | Digital Object Identifier (optional) |
| `PROJECT_TITLE` | `"Project Title"` | Project/research title |
| `LOG_LEVEL` | `1` | Logging verbosity (0=DEBUG, 1=INFO, 2=WARN, 3=ERROR) |

**Priority order**:
1. Environment variables (highest priority - override config file)
2. Config file (`manuscript/config.yaml`)
3. Default values (lowest priority)

### Configuration Examples

#### Using Configuration File (Recommended)
```bash
# Edit manuscript/config.yaml with your information
vim manuscript/config.yaml

# Build with config file values
python3 scripts/03_render_pdf.py
```

#### Using Environment Variables
```bash
export AUTHOR_NAME="Dr. Jane Smith"
export PROJECT_TITLE="Novel Optimization Framework"
export AUTHOR_EMAIL="jane.smith@university.edu"
export AUTHOR_ORCID="0000-0000-0000-1234"
export DOI="10.5281/zenodo.12345678"  # Optional

python3 scripts/03_render_pdf.py
```

#### Verbose Logging
```bash
export LOG_LEVEL=0  # Show all debug messages
python3 scripts/03_render_pdf.py
```

### Runtime Configuration

Configuration is read at runtime by `render_pdf.sh` and applied to:
- PDF metadata (title, author, date)
- LaTeX document properties
- Generated file headers
- Cross-reference systems
- Title page generation

## ğŸš€ Rendering Pipeline

### Complete Pipeline Execution

The template provides **two entry points** for pipeline execution:

**Interactive Menu (Recommended)**
```bash
# Full interactive menu with all options
./run.sh
```

**Non-Interactive Pipelines**
```bash
# Extended pipeline (9 stages) with optional LLM review
./run.sh --pipeline

# Core pipeline (6 stages) - Python orchestrator
python3 scripts/run_all.py
```

**Entry Point Comparison**
- **`./run.sh --pipeline`**: 9 stages (0-8), includes optional LLM review stages
- **`python3 scripts/run_all.py`**: 6 stages (00-05), core pipeline only

### Pipeline Stages

**Core Pipeline Stages** (both entry points):

1. **Environment Setup** - Verify system requirements and dependencies
2. **Test Validation** (coverage: 70% project, 49% infra) - Run test suites
3. **Project Analysis** - Execute `project/scripts/` analysis workflows
4. **PDF Rendering** - Generate manuscript PDFs and figures
5. **Output Validation** - Validate all generated outputs
6. **Copy Outputs** - Copy final deliverables to root `output/` directory

**Extended Pipeline Stages** (`./run.sh --pipeline` only):

7. **LLM Scientific Review** - AI-powered manuscript analysis (optional)
8. **LLM Translations** - Multi-language technical abstract generation (optional)

**Stage Numbering:**
- `run_all.py`: Stages 00-05 (zero-padded, Python convention)
- `run.sh`: Stages 0-8 (zero-indexed, displayed as [1/9] to [8/9])

### Manual Execution Options

**Individual Stage Execution:**
```bash
# Environment setup
python3 scripts/00_setup_environment.py

# Test execution (combined infra + project)
python3 scripts/01_run_tests.py

# Project analysis scripts
python3 scripts/02_run_analysis.py

# PDF rendering
python3 scripts/03_render_pdf.py

# Output validation
python3 scripts/04_validate_output.py

# Copy outputs
python3 scripts/05_copy_outputs.py

# LLM manuscript review (optional, requires Ollama)
python3 scripts/06_llm_review.py
```

**Validation Tools:**
```bash
# Validate markdown files
python3 -m infrastructure.validation.cli markdown manuscript/

# Validate PDF outputs
python3 -m infrastructure.validation.cli pdf output/pdf/
```

## âœ… Validation Systems

### PDF Validation

```bash
# Validate generated PDF for issues
python3 -m infrastructure.validation.cli pdf output/pdf/

# With verbose output
python3 -m infrastructure.validation.cli pdf output/pdf/ --verbose

# Specific PDF file
python3 -m infrastructure.validation.cli pdf output/pdf/01_abstract.pdf
```

**Validation Checks**:
- Unresolved references (`??`)
- Missing citations (`[?]`)
- LaTeX warnings and errors
- Document structure integrity
- Word count and content preview

### Markdown Validation

```bash
# Validate all markdown files
python3 -m infrastructure.validation.cli markdown manuscript/

# Strict mode (fail on any issues)
python3 -m infrastructure.validation.cli markdown manuscript/ --strict
```

**Validation Checks**:
- Image reference resolution
- Cross-reference integrity
- Equation label validation
- Link formatting
- Mathematical notation

### Test Coverage

```bash
# Run both infrastructure and project tests via orchestrator
python3 scripts/01_run_tests.py

# Or run manually with coverage reports
python3 -m pytest tests/infrastructure/ --cov=infrastructure --cov-report=html
python3 -m pytest project/tests/ --cov=project/src --cov-report=html
```

**Coverage Requirements**:
- 70% minimum for project/src/ (currently achieving 99.88%)
- 49% minimum for infrastructure/ (currently achieving 70.09%)
- All tests must pass before PDF generation
- No mock methods (real data analysis only)

**Coverage Gap Analysis**:
- See [`docs/COVERAGE_GAPS.md`](docs/COVERAGE_GAPS.md) for detailed analysis
- Low-coverage modules identified and improvement plans documented
- New test files created for checkpoint, progress, retry, CLI, LLM operations, and paper selector

## ğŸ§ª Testing Framework

### Test Structure

Tests follow the **thin orchestrator pattern** principles:
- Import real methods from `project/src/` or `infrastructure/` modules  
- Use real data and computation
- Validate actual behavior (no mocks)
- Ensure reproducible, deterministic results

### Test Categories

1. **Unit Tests** (`test_*.py`) - Individual function validation
2. **Integration Tests** - Script and pipeline integration
3. **Validation Tests** - PDF and markdown quality checks

### Running Tests

```bash
# All tests via orchestrator (recommended)
python3 scripts/01_run_tests.py

# Specific test file
python3 -m pytest project/tests/test_example.py -v

# Infrastructure tests with coverage
python3 -m pytest tests/infrastructure/ --cov=infrastructure --cov-report=html

# Project tests with coverage
python3 -m pytest project/tests/ --cov=project/src --cov-report=html
```

## ğŸ“¤ Output Formats

### Generated Files Structure

```
output/
â”œâ”€â”€ pdf/                    # PDF documents
â”‚   â”œâ”€â”€ 01_abstract.pdf     # Individual section PDFs
â”‚   â”œâ”€â”€ 02_introduction.pdf
â”‚   â”œâ”€â”€ project_combined.pdf # Complete manuscript
â”‚   â””â”€â”€ *.pdf               # Additional formats
â”œâ”€â”€ tex/                    # LaTeX source files
â”‚   â”œâ”€â”€ 01_abstract.tex
â”‚   â”œâ”€â”€ project_combined.tex
â”‚   â””â”€â”€ *.tex
â”œâ”€â”€ figures/                # Generated figures
â”‚   â”œâ”€â”€ example_figure.png
â”‚   â”œâ”€â”€ convergence_plot.png
â”‚   â””â”€â”€ *.png
â”œâ”€â”€ data/                   # Generated datasets
â”‚   â”œâ”€â”€ example_data.csv
â”‚   â”œâ”€â”€ convergence_data.npz
â”‚   â””â”€â”€ *.csv
â””â”€â”€ project_combined.html   # HTML version for IDE
```

### PDF Versions

1. **Standard PDF** (`project_combined.pdf`)
   - Professional printing format
   - Optimized for LaTeX rendering
   - Complete cross-references and citations

2. **IDE-Friendly PDF** (`project_combined_ide_friendly.pdf`)
   - Enhanced for text editor viewing
   - Better font rendering in IDEs
   - Simplified layout for screen reading

3. **HTML Version** (`project_combined.html`)
   - Web browser compatible
   - IDE integration
   - Interactive features (when available)

## ğŸ§ª **Advanced Modules**

The template includes advanced modules for comprehensive scientific package development:

### ğŸ“Š **Quality Analysis** (`infrastructure/quality_checker.py`)
**Advanced document quality analysis and metrics**

**Key Features:**
- **Readability Analysis**: Flesch score, Gunning Fog index
- **Academic Standards**: Compliance with research writing standards
- **Structural Integrity**: Document organization and completeness
- **Formatting Quality**: Consistent styling and presentation
- **Comprehensive Reporting**: Detailed quality assessment reports

**Usage:**
```python
from quality_checker import analyze_document_quality, generate_quality_report

metrics = analyze_document_quality(pdf_path)
report = generate_quality_report(metrics)
print(report)
```

### ğŸ”„ **Reproducibility Tools** (`infrastructure/reproducibility.py`)
**Build reproducibility and environment tracking**

**Key Features:**
- **Environment Capture**: Platform, Python version, dependencies
- **Dependency Tracking**: Version management and consistency
- **Build Manifests**: Comprehensive build artifact tracking
- **Snapshot Comparison**: Version control and change detection
- **Reproducible Builds**: Deterministic environment setup

**Usage:**
```python
from reproducibility import generate_reproducibility_report, save_reproducibility_report

report = generate_reproducibility_report(output_dir)
save_reproducibility_report(report, "reproducibility.json")
```

### ğŸ” **Integrity Verification** (`infrastructure/integrity.py`)
**File integrity and cross-reference validation**

**Key Features:**
- **File Integrity**: Hash-based verification of output files
- **Cross-Reference Validation**: LaTeX reference integrity checking
- **Data Consistency**: Format and structure validation
- **Academic Standards**: Compliance with writing standards
- **Build Artifact Verification**: Complete output validation

**Usage:**
```python
from integrity import verify_output_integrity, generate_integrity_report

report = verify_output_integrity(output_dir)
print(generate_integrity_report(report))
```

### ğŸ“š **Publishing Tools** (`infrastructure/publishing.py`)
**Academic publishing workflow assistance**

**Key Features:**
- **DOI Validation**: Format and checksum verification
- **Citation Generation**: BibTeX, APA, MLA formats
- **Publication Metadata**: Comprehensive metadata extraction
- **Submission Preparation**: Checklist and package creation
- **Academic Profile**: ORCID and repository integration
- **Platform Integration**: Zenodo, arXiv, GitHub releases (NEW)

**Usage:**
```python
from publishing import extract_publication_metadata, generate_citation_bibtex

metadata = extract_publication_metadata(markdown_files)
bibtex = generate_citation_bibtex(metadata)
print(bibtex)
```

### ğŸ”¬ **Scientific Development** (`infrastructure/scientific_dev.py`)
**Scientific computing best practices and tools**

**Key Features:**
- **Numerical Stability**: Algorithm stability testing
- **Performance Benchmarking**: Execution time and memory analysis
- **Scientific Documentation**: API documentation generation
- **Best Practices Validation**: Code quality assessment
- **Research Workflow Templates**: Reproducible experiment templates

**Usage:**
```python
from scientific_dev import check_numerical_stability, benchmark_function

stability = check_numerical_stability(your_function, test_inputs)
benchmark = benchmark_function(your_function, test_inputs)
```

### ğŸ—ï¸ **Build Verification** (`infrastructure/build_verifier.py`)
**Comprehensive build process validation**

**Key Features:**
- **Build Process Validation**: Command execution and error handling
- **Artifact Verification**: Expected output file checking
- **Reproducibility Testing**: Multiple build run comparison
- **Environment Validation**: Dependency and tool availability
- **Build Reporting**: Comprehensive validation reports

**Usage:**
```python
from build_verifier import verify_build_artifacts, verify_build_reproducibility

verification = verify_build_artifacts(output_dir, expected_files)
reproducibility = verify_build_reproducibility(build_command, expected_outputs)
```

### ğŸ“– **Literature Search** (`infrastructure/literature/`) **NEW**
**Academic literature search and reference management**

**Key Features:**
- **Multi-Source Search**: arXiv, Semantic Scholar, CrossRef, PubMed
- **PDF Download**: Automatic paper retrieval with retry logic
- **Citation Extraction**: Extract citations from papers
- **BibTeX Management**: Generate and manage bibliography files
- **Reference Deduplication**: Merge results from multiple sources
- **Library Management**: Organize research papers

**Usage:**
```python
from infrastructure.literature import LiteratureSearch

searcher = LiteratureSearch()
papers = searcher.search("machine learning", limit=10)
searcher.add_to_library(papers[0])
searcher.export_bibtex("references.bib")
```

### ğŸ¤– **LLM Integration** (`infrastructure/llm/`) **NEW**
**Local LLM assistance for research workflows**

**Key Features:**
- **Ollama Integration**: Local model support (privacy-first)
- **Template System**: Pre-built prompts for common research tasks
- **Context Management**: Multi-turn conversation handling
- **Streaming Support**: Real-time response generation
- **Model Fallback**: Automatic fallback to alternative models
- **Token Counting**: Track usage and costs

**Research Templates:**
- Abstract summarization
- Literature review generation
- Code documentation
- Data interpretation
- Section drafting assistance
- Citation formatting
- Technical abstract translation (Chinese, Hindi, Russian)

**Usage:**
```python
from infrastructure.llm import LLMClient

client = LLMClient()
summary = client.apply_template("summarize_abstract", text=abstract)
response = client.query("What are the key findings?")
```

### ğŸ¨ **Rendering System** (`infrastructure/rendering/`) **NEW**
**Multi-format output generation from single source**

**Key Features:**
- **PDF Rendering**: Professional LaTeX-based PDFs
- **Presentation Slides**: Beamer (PDF) and reveal.js (HTML) slides
- **Web Output**: Interactive HTML with MathJax
- **Scientific Posters**: Large-format poster generation
- **Format-Agnostic**: Single source, multiple outputs
- **Quality Validation**: Automated output checking

**Usage:**
```python
from infrastructure.rendering import RenderManager

manager = RenderManager()
pdf = manager.render_pdf("manuscript.tex")
slides = manager.render_slides("presentation.md", format="revealjs")
html = manager.render_web("manuscript.md")
all_outputs = manager.render_all("manuscript.md")
```

### ğŸš€ **Publishing API** (`infrastructure/publishing_api.py`) **NEW**
**Automated publishing to academic platforms**

**Key Features:**
- **Zenodo Integration**: Upload with DOI minting
- **arXiv Preparation**: Submission package creation
- **GitHub Releases**: Automated release management
- **Metrics Tracking**: Download and citation tracking
- **Distribution Packages**: Complete publication bundles

**Usage:**
```python
from infrastructure import publishing

# Publish to Zenodo
doi = publishing.publish_to_zenodo(metadata, files, token)

# Create GitHub release
release = publishing.create_github_release(metadata, files, token)

# Prepare arXiv submission
package = publishing.publish_to_arxiv(metadata, files)
```

### **Module Integration**

All advanced modules follow the **thin orchestrator pattern**:
- **Business logic** in `infrastructure/` modules with comprehensive test coverage
- **Orchestration** in separate utility scripts
- **Integration** with existing build pipeline
- **Comprehensive testing** ensuring reliability
- **Documentation** for each module's functionality

**Testing Coverage:**
- âœ… **Quality Checker**: Comprehensive tests (26 tests)
- âœ… **Reproducibility**: Comprehensive tests (18 tests)
- âœ… **Integrity**: Comprehensive tests (16 tests)
- âœ… **Publishing**: Comprehensive tests (14 tests)
- âœ… **Scientific Dev**: Comprehensive tests (12 tests)
- âœ… **Build Verifier**: Comprehensive tests (10 tests)
- âœ… **Literature Search**: 91% coverage (8 tests) **NEW**
- âœ… **LLM Integration**: 91% coverage (11 tests) **NEW**
- âœ… **Rendering System**: 91% coverage (10 tests) **NEW**
- âœ… **Publishing API**: Integrated with existing suite **NEW**

### Accessing Outputs

```bash
# Open combined PDF
open output/pdf/project_combined.pdf

# Open HTML version in browser
open output/project_combined.html

# List all generated files
ls -la output/

# Check PDF validation
python3 -m infrastructure.validation.cli pdf output/pdf/
```

## ğŸ”§ Troubleshooting

### Quick Reference

- **General Troubleshooting**: [`docs/TROUBLESHOOTING_GUIDE.md`](docs/TROUBLESHOOTING_GUIDE.md)
- **LLM Review Issues**: [`docs/LLM_REVIEW_TROUBLESHOOTING.md`](docs/LLM_REVIEW_TROUBLESHOOTING.md)
- **Checkpoint/Resume**: [`docs/CHECKPOINT_RESUME.md`](docs/CHECKPOINT_RESUME.md)
- **Performance Issues**: [`docs/PERFORMANCE_OPTIMIZATION.md`](docs/PERFORMANCE_OPTIMIZATION.md)

### Common Issues

#### Tests Failing
```bash
# Ensure coverage requirements met for both suites
python3 scripts/01_run_tests.py

# Or run individually with coverage reports
python3 -m pytest tests/infrastructure/ --cov=infrastructure --cov-fail-under=49
python3 -m pytest project/tests/ --cov=project/src --cov-fail-under=70
```

#### Scripts Failing
```bash
# Run scripts individually to debug
python3 scripts/example_figure.py

# Check import errors
python3 -c "import project.src.example; print('Import successful')"
```

#### PDF Generation Issues
```bash
# Check LaTeX installation
which xelatex

# Validate markdown first
python3 -m infrastructure.validation.cli markdown manuscript/

# Check compilation logs
ls output/pdf/*_compile.log
```

#### Missing Dependencies
```bash
# Install system dependencies
# Ubuntu/Debian:
sudo apt-get install -y pandoc texlive-xetex texlive-fonts-recommended fonts-dejavu

# macOS:
brew install pandoc
brew install --cask mactex
```

### Debug Mode

```bash
# Enable verbose logging
export LOG_LEVEL=0
python3 scripts/03_render_pdf.py

# Run with debug output
python3 -m infrastructure.validation.cli pdf output/pdf/ --verbose
```

### Log Files

Key log files for debugging:
- `output/pdf/*_compile.log` - LaTeX compilation logs
- `output/project_combined.md` - Combined markdown source
- Test output from pytest runs

## ğŸ› ï¸ Maintenance

### System Updates

1. **Update Dependencies**
   ```bash
   # Update Python packages
   uv sync

   # Update system packages
   sudo apt-get update && sudo apt-get upgrade
   ```

2. **Version Control**
   ```bash
   # Check current status
   git status

   # Stage changes
   git add .

   # Commit with descriptive message
   git commit -m "feat: add new validation feature"
   ```

3. **Backup Strategy**
   ```bash
   # Clean outputs before backup
   python3 scripts/run_all.py --clean

   # Backup source files only
   tar -czf project_backup.tar.gz project/src/ project/tests/ project/scripts/ project/manuscript/ docs/
   ```

### Adding New Features

1. **Business Logic** â†’ Add to `project/src/`
2. **Tests** â†’ Add to `project/tests/`
3. **Scripts** â†’ Add to `project/scripts/` (use `project/src/` methods)
4. **Documentation** â†’ Update relevant `.md` files
5. **Validation** â†’ Ensure coverage requirements met

### Performance Optimization

- **Parallel Testing**: Use `pytest-xdist` for faster test runs
- **Caching**: Enable pytest caching for repeated runs
- **Incremental Builds**: Only rebuild changed components
- **Performance Monitoring**: Automatic bottleneck detection in pipeline summary
- **Resource Tracking**: Memory and CPU usage reporting (when enabled)

See [`docs/PERFORMANCE_OPTIMIZATION.md`](docs/PERFORMANCE_OPTIMIZATION.md) for comprehensive optimization guide.

### Checkpoint and Resume

The pipeline includes automatic checkpointing for resume capability:

```bash
# Resume from last checkpoint
python3 scripts/run_all.py --resume
./run.sh --pipeline --resume

# Start fresh (clears checkpoint on success)
python3 scripts/run_all.py
./run.sh --pipeline
```

**Features**:
- Automatic checkpoint after each successful stage
- Checkpoint validation before resume
- Graceful handling of corrupted checkpoints
- Preserves pipeline start time and stage durations

See [`docs/CHECKPOINT_RESUME.md`](docs/CHECKPOINT_RESUME.md) for complete documentation.

## ğŸ“š References

### Internal Documentation
- [`README.md`](README.md) - Project overview and quick start
- [`docs/HOW_TO_USE.md`](docs/HOW_TO_USE.md) - Complete usage guide
- [`docs/ARCHITECTURE.md`](docs/ARCHITECTURE.md) - System design details
- [`docs/WORKFLOW.md`](docs/WORKFLOW.md) - Development workflow

### External Resources
- [Pandoc Manual](https://pandoc.org/MANUAL.html) - Document conversion
- [LaTeX Wikibook](https://en.wikibooks.org/wiki/LaTeX) - LaTeX documentation
- [Python Testing](https://docs.pytest.org/) - Testing framework

## ğŸ¯ Best Practices

### Development Workflow
1. Write tests first (TDD)
2. Ensure coverage requirements met
3. Follow thin orchestrator pattern
4. Validate all outputs
5. Update documentation
6. Commit with clear messages

### Code Quality
- **Type Hints**: All public APIs must have type annotations
- **Documentation**: Clear docstrings for all functions
- **Error Handling**: Graceful failure with informative messages
- **Consistency**: Follow established patterns and conventions

### System Reliability
- **Deterministic Outputs**: All generation must be reproducible
- **Comprehensive Validation**: Check all aspects of output quality
- **Error Recovery**: Handle failures gracefully with clear messages
- **Performance Monitoring**: Track execution time and resource usage

---

## âœ… System Status: FULLY OPERATIONAL (v2.0)

**All systems confirmed functional:**
- âœ… Test suite (878 tests passing: 558 infrastructure + 320 project)
- âœ… Package API testing (test_package_imports.py validates __init__.py)
- âœ… Script execution (thin orchestrator pattern fully compliant)
- âœ… Markdown validation (all references resolved, no warnings)
- âœ… PDF generation (individual + combined, all formats)
- âœ… Cross-reference system (citations, equations, figures resolved)
- âœ… Configuration system (YAML config + environment variables)
- âœ… Output validation (no rendering issues, all PDFs valid)
- âœ… Documentation (comprehensive guides, complete .cursorrules standards)

**New Modules (v2.0):**
- âœ… Literature Search (91% coverage, 8 tests) - Multi-source academic search
- âœ… LLM Integration (91% coverage, 11 tests) - Local Ollama support
- âœ… Rendering System (91% coverage, 10 tests) - Multi-format output
- âœ… Publishing API (integrated) - Zenodo, arXiv, GitHub automation

**Comprehensive Audit Status:**
- âœ… Comprehensive code coverage achieved (55.89% infra, exceeds 49% minimum)
- âœ… Zero mock methods - all tests use real data
- âœ… All .cursorrules standards fully implemented
- âœ… Complete compliance with thin orchestrator pattern
- âœ… Production-ready build pipeline (5-stage execution)
- âœ… Reproducible outputs (deterministic with fixed seeds)
- âœ… 40 new tests (100% passing) for new modules
- âœ… 85%+ coverage on new infrastructure modules